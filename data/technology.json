{
  "updated_at": "2026-02-24T11:35:24.193Z",
  "clusters": [
    {
      "id": "cluster_6",
      "coverage": 3,
      "updated_at": "Tue, 24 Feb 2026 11:00:49 +0000",
      "title": "The US military will reportedly use Elon Musk's Grok AI in its classified systems",
      "neutral_headline": "US AI giant accuses Chinese rivals of mass data theft",
      "bullet_summary": [
        "The US Department of Defense has reportedly reached a deal to use Elon Musk's Grok in its classified systems, according to Axios",
        "Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife",
        "Anthropic reportedly refused to offer its tech for those things, even with a \"safety stack\" built into that model",
        "The Pentagon is reportedly also negotiating deals with OpenAI and Gemini, both of which it considers to be on par with Anthropic"
      ],
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/the-us-military-will-reportedly-use-elon-musks-grok-ai-in-its-classified-systems-110049021.html",
          "published_at": "Tue, 24 Feb 2026 11:00:49 +0000",
          "title": "The US military will reportedly use Elon Musk's Grok AI in its classified systems",
          "standfirst": "The US Department of Defense has reportedly reached a deal to use Elon Musk's Grok in its classified systems, according to Axios. That follows news that the Pentagon is currently in a dispute with another AI company, Anthropic, over limits on its technology for things like mass surveillance. Last year, the White ordered Grok, along with ChatGPT, Gemini and Anthropic's Claude to be approved for government use. Up until now, though, only Anthropic's model has been allowed for the military's most sensitive tasks in intelligence, weapons development and battlefield operations. Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife. However, the Pentagon demanded that Anthropic make Claude available for \"all lawful purposes\" including mass surveillance and the development of fully autonomous weapons. Anthropic reportedly refused to offer its tech for those things, even with a \"safety stack\" built into that model. xAI, by contrast, agreed to a standard that would allow the DoD to employ its AI for any purpose it deems \"lawful.\" However, the xAI model is not considered by officials to be as cutting-edge or reliable as Anthropic's Claude, and they admit that replacing Claude with Grok would be a challenge. The Pentagon is reportedly also negotiating deals with OpenAI and Gemini, both of which it considers to be on par with Anthropic. xAI had announced a version of Grok for US government agencies in July 2025. Shortly before that, though, the chatbot started spouting fascist propaganda and antisemitic rhetoric while dubbing itself \"MechaHitler.\" All of that followed a public spat between Musk and Trump over the president's spending bill, after which GSA approval of Grok seemed to stall. Earlier this week, Anthropic accused three Chinese AI labs of abusing Claude's AI with \"distillation attacks\" to improve their own models. This article originally appeared on Engadget at https://www.engadget.com/ai/the-us-military-will-reportedly-use-elon-musks-grok-ai-in-its-classified-systems-110049021.html?src=rss",
          "content": "The US Department of Defense has reportedly reached a deal to use Elon Musk's Grok in its classified systems, according to Axios. That follows news that the Pentagon is currently in a dispute with another AI company, Anthropic, over limits on its technology for things like mass surveillance. Last year, the White ordered Grok, along with ChatGPT, Gemini and Anthropic's Claude to be approved for government use. Up until now, though, only Anthropic's model has been allowed for the military's most sensitive tasks in intelligence, weapons development and battlefield operations. Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife. However, the Pentagon demanded that Anthropic make Claude available for \"all lawful purposes\" including mass surveillance and the development of fully autonomous weapons. Anthropic reportedly refused to offer its tech for those things, even with a \"safety stack\" built into that model. xAI, by contrast, agreed to a standard that would allow the DoD to employ its AI for any purpose it deems \"lawful.\" However, the xAI model is not considered by officials to be as cutting-edge or reliable as Anthropic's Claude, and they admit that replacing Claude with Grok would be a challenge. The Pentagon is reportedly also negotiating deals with OpenAI and Gemini, both of which it considers to be on par with Anthropic. xAI had announced a version of Grok for US government agencies in July 2025. Shortly before that, though, the chatbot started spouting fascist propaganda and antisemitic rhetoric while dubbing itself \"MechaHitler.\" All of that followed a public spat between Musk and Trump over the president's spending bill, after which GSA approval of Grok seemed to stall. Earlier this week, Anthropic accused three Chinese AI labs of abusing Claude's AI with \"distillation attacks\" to improve their own models. This article originally appeared on Engadget at https://www.engadget.com/ai/the-us-military-will-reportedly-use-elon-musks-grok-ai-in-its-classified-systems-110049021.html?src=rss",
          "feed_position": 1
        },
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/technology/2026/feb/23/us-ai-anthropic-china",
          "published_at": "Mon, 23 Feb 2026 23:15:50 GMT",
          "title": "US AI giant accuses Chinese rivals of mass data theft",
          "standfirst": "Anthropic says three Chinese firms used ‘distillation’ technique to extract information from its Claude chatbotUS artificial intelligence company Anthropic said on Monday it had uncovered campaigns by three Chinese AI firms to illicitly extract capabilities from its Claude chatbot, in what it described as industrial-scale intellectual property theft. OpenAI leveled similar charges last month.Anthropic said DeepSeek, Moonshot AI and MiniMax used a technique known as “distillation” – using outputs from a more powerful AI system to rapidly boost the performance of a less capable one. Continue reading...",
          "content": "Anthropic says three Chinese firms used ‘distillation’ technique to extract information from its Claude chatbotUS artificial intelligence company Anthropic said on Monday it had uncovered campaigns by three Chinese AI firms to illicitly extract capabilities from its Claude chatbot, in what it described as industrial-scale intellectual property theft. OpenAI leveled similar charges last month.Anthropic said DeepSeek, Moonshot AI and MiniMax used a technique known as “distillation” – using outputs from a more powerful AI system to rapidly boost the performance of a less capable one. Continue reading...",
          "feed_position": 1
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/anthropic-accuses-three-chinese-ai-labs-of-abusing-claude-to-improve-their-own-models-205210613.html",
          "published_at": "Mon, 23 Feb 2026 20:52:10 +0000",
          "title": "Anthropic accuses three Chinese AI labs of abusing Claude to improve their own models",
          "standfirst": "Anthropic is issuing a call to action against AI \"distillation attacks,\" after accusing three AI companies of misusing its Claude chatbot. On its website, Anthropic claimed that DeepSeek, Moonshot and MiniMax have been conducting \"industrial-scale campaigns…to illicitly extract Claude’s capabilities to improve their own models.\" Distillation in the AI world refers to when less capable models lean on the responses of more powerful ones to train themselves. While distillation isn't a bad thing across the board, Anthropic said that these types of attacks can be used in a more nefarious way. According to Anthropic, these three Chinese AI firms were responsible for more than \"16 million exchanges with Claude through approximately 24,000 fraudulent accounts.\" From Anthropic's perspective, these competing companies were using Claude as a shortcut to develop more advanced AI models, which could also lead to circumventing certain safeguards. Anthropic said in its post that it was able to link each of these distilling attack campaigns to the specific companies with \"high confidence\" thanks to IP address correlation, metadata requests and infrastructure indicators, along with corroborating with others in the AI industry who have noticed similar behaviors. Early last year, OpenAI made similar claims of rival firms distilling its models and banned suspected accounts in response. As for Anthropic, the company behind Claude said it would upgrade its system to make distillation attacks harder to do and easier to identify. While Anthropic is pointing fingers at these other firms, it's also facing a lawsuit from music publishers who accused the AI company of using illegal copies of songs to train its Claude chatbot.This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-accuses-three-chinese-ai-labs-of-abusing-claude-to-improve-their-own-models-205210613.html?src=rss",
          "content": "Anthropic is issuing a call to action against AI \"distillation attacks,\" after accusing three AI companies of misusing its Claude chatbot. On its website, Anthropic claimed that DeepSeek, Moonshot and MiniMax have been conducting \"industrial-scale campaigns…to illicitly extract Claude’s capabilities to improve their own models.\" Distillation in the AI world refers to when less capable models lean on the responses of more powerful ones to train themselves. While distillation isn't a bad thing across the board, Anthropic said that these types of attacks can be used in a more nefarious way. According to Anthropic, these three Chinese AI firms were responsible for more than \"16 million exchanges with Claude through approximately 24,000 fraudulent accounts.\" From Anthropic's perspective, these competing companies were using Claude as a shortcut to develop more advanced AI models, which could also lead to circumventing certain safeguards. Anthropic said in its post that it was able to link each of these distilling attack campaigns to the specific companies with \"high confidence\" thanks to IP address correlation, metadata requests and infrastructure indicators, along with corroborating with others in the AI industry who have noticed similar behaviors. Early last year, OpenAI made similar claims of rival firms distilling its models and banned suspected accounts in response. As for Anthropic, the company behind Claude said it would upgrade its system to make distillation attacks harder to do and easier to identify. While Anthropic is pointing fingers at these other firms, it's also facing a lawsuit from music publishers who accused the AI company of using illegal copies of songs to train its Claude chatbot.This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-accuses-three-chinese-ai-labs-of-abusing-claude-to-improve-their-own-models-205210613.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/ai-artificial-intelligence/883243/anthropic-claude-deepseek-china-ai-distillation",
          "published_at": "2026-02-23T15:22:55-05:00",
          "title": "Anthropic accuses DeepSeek and other Chinese firms of using Claude to train their AI",
          "standfirst": "Anthropic claims DeepSeek and two other Chinese AI companies misused its Claude AI model in an attempt to improve their own products. In an announcement on Monday, Anthropic says the \"industrial-scale campaigns\" involved the creation of around 24,000 fraudulent accounts and more than 16 million exchanges with Claude, as reported earlier by The Wall Street [&#8230;]",
          "content": "Anthropic claims DeepSeek and two other Chinese AI companies misused its Claude AI model in an attempt to improve their own products. In an announcement on Monday, Anthropic says the \"industrial-scale campaigns\" involved the creation of around 24,000 fraudulent accounts and more than 16 million exchanges with Claude, as reported earlier by The Wall Street Journal. The three companies - DeepSeek, MiniMax, and Moonshot - are accused of \"distilling\" Claude, or training a smaller AI model based on a more advanced one. Though Anthropic says that distillation is a \"legitimate training method,\" it adds that it can \"also be used for illicit purpose … Read the full story at The Verge.",
          "feed_position": 4
        }
      ],
      "popularity_score": 3019.4235575
    },
    {
      "id": "cluster_4",
      "coverage": 2,
      "updated_at": "2026-02-24T06:15:37-05:00",
      "title": "Apple will soon make (some) Mac Minis in the US",
      "neutral_headline": "Apple will soon make (some) Mac Minis in the US",
      "bullet_summary": [
        "Reported by The Verge, TechMeme"
      ],
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/tech/883581/apple-mac-mini-production-texas-houston",
          "published_at": "2026-02-24T06:15:37-05:00",
          "title": "Apple will soon make (some) Mac Minis in the US",
          "standfirst": "Apple is preparing to move some of its Mac Mini production to the US as part of the company's ongoing efforts to appease the Trump administration's push for domestic investment. Manufacturing is set to begin later this year in north Houston, Texas, at a Foxconn facility that currently assembles Apple's AI servers. \"Apple is deeply [&#8230;]",
          "content": "Apple will still be producing Mac Mini computers in Asia too. | Photo by Chris Welch / The Verge Apple is preparing to move some of its Mac Mini production to the US as part of the company's ongoing efforts to appease the Trump administration's push for domestic investment. Manufacturing is set to begin later this year in north Houston, Texas, at a Foxconn facility that currently assembles Apple's AI servers. \"Apple is deeply committed to the future of American manufacturing, and we're proud to significantly expand our footprint in Houston with the production of Mac Mini starting later this year,\" Apple CEO Tim Cook said in the announcement. \"We began shipping advanced AI servers from Houston ahead of schedule, and we're excited to ac … Read the full story at The Verge.",
          "feed_position": 0
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260223/p36#a260223p36",
          "published_at": "Mon, 23 Feb 2026 22:35:00 -0500",
          "title": "A look at Apple's efforts to reshore the US chip supply chain, as the company commits to buying 100M+ chips from TSMC Arizona in 2026, amid US-China tensions (Rolfe Winkler/Wall Street Journal)",
          "standfirst": "Rolfe Winkler / Wall Street Journal: A look at Apple's efforts to reshore the US chip supply chain, as the company commits to buying 100M+ chips from TSMC Arizona in 2026, amid US-China tensions &mdash; The iPhone maker wants more supply based in U.S., which remains years behind Asia &mdash; PHOENIX&mdash;On a desolate stretch of land dotted &hellip;",
          "content": "Rolfe Winkler / Wall Street Journal: A look at Apple's efforts to reshore the US chip supply chain, as the company commits to buying 100M+ chips from TSMC Arizona in 2026, amid US-China tensions &mdash; The iPhone maker wants more supply based in U.S., which remains years behind Asia &mdash; PHOENIX&mdash;On a desolate stretch of land dotted &hellip;",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/260223/i36.jpg"
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260223/p35#a260223p35",
          "published_at": "Mon, 23 Feb 2026 22:00:01 -0500",
          "title": "Apple says it plans to move some Mac mini production to Houston from Asia later in 2026, as part of its efforts to invest $600B in the US over four years (Rolfe Winkler/Wall Street Journal)",
          "standfirst": "Rolfe Winkler / Wall Street Journal: Apple says it plans to move some Mac mini production to Houston from Asia later in 2026, as part of its efforts to invest $600B in the US over four years &mdash; The company will move some production of the desktop computer to Foxconn facility in Texas &mdash; Apple will move some production &hellip;",
          "content": "Rolfe Winkler / Wall Street Journal: Apple says it plans to move some Mac mini production to Houston from Asia later in 2026, as part of its efforts to invest $600B in the US over four years &mdash; The company will move some production of the desktop computer to Foxconn facility in Texas &mdash; Apple will move some production &hellip;",
          "feed_position": 10,
          "image_url": "http://www.techmeme.com/260223/i35.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260223/i36.jpg",
      "popularity_score": 2019.6702241666667
    },
    {
      "id": "cluster_14",
      "coverage": 2,
      "updated_at": "Tue, 24 Feb 2026 05:35:02 -0500",
      "title": "Discord says it \"ran a limited test of Persona in the UK\" to verify user ages and \"that test has since concluded\", distancing itself from Persona amid backlash (Emma Roth/The Verge)",
      "neutral_headline": "Discord distances itself from Persona age verification after user backlash",
      "bullet_summary": [
        "Reported by TechMeme, The Verge"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260224/p4#a260224p4",
          "published_at": "Tue, 24 Feb 2026 05:35:02 -0500",
          "title": "Discord says it \"ran a limited test of Persona in the UK\" to verify user ages and \"that test has since concluded\", distancing itself from Persona amid backlash (Emma Roth/The Verge)",
          "standfirst": "Emma Roth / The Verge: Discord says it &ldquo;ran a limited test of Persona in the UK&rdquo; to verify user ages and &ldquo;that test has since concluded&rdquo;, distancing itself from Persona amid backlash &mdash; The messaging platform says its &lsquo;limited test&rsquo; of Persona in the UK has come to an end.",
          "content": "Emma Roth / The Verge: Discord says it &ldquo;ran a limited test of Persona in the UK&rdquo; to verify user ages and &ldquo;that test has since concluded&rdquo;, distancing itself from Persona amid backlash &mdash; The messaging platform says its &lsquo;limited test&rsquo; of Persona in the UK has come to an end.",
          "feed_position": 5,
          "image_url": "http://www.techmeme.com/260224/i4.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/tech/878369/discord-persona-age-verification",
          "published_at": "2026-02-23T11:22:20-05:00",
          "title": "Discord distances itself from Persona age verification after user backlash",
          "standfirst": "Discord is attempting to distance itself from the age verification provider Persona following a steady stream of user backlash. In an emailed statement to The Verge, Discord's head of product policy, Savannah Badalich, confirms the company \"ran a limited test of Persona in the UK where age assurance had previously launched and that test has [&#8230;]",
          "content": "Discord is attempting to distance itself from the age verification provider Persona following a steady stream of user backlash. In an emailed statement to The Verge, Discord's head of product policy, Savannah Badalich, confirms the company \"ran a limited test of Persona in the UK where age assurance had previously launched and that test has since concluded.\" After Discord announced plans to implement age verification globally starting next month, users across social media accused Discord of \"lying\" about how it plans on handling face scans and ID uploads. Much of the criticism was directed toward Discord's partnership with Persona, an age v … Read the full story at The Verge.",
          "feed_position": 7
        }
      ],
      "featured_image": "http://www.techmeme.com/260224/i4.jpg",
      "popularity_score": 2018.9938352777779
    },
    {
      "id": "cluster_16",
      "coverage": 2,
      "updated_at": "Tue, 24 Feb 2026 10:00:36 +0000",
      "title": "The best webcams for 2026",
      "neutral_headline": "The best webcams for 2026",
      "bullet_summary": [
        "Just one week ago, on February 15, OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI to lead its “next generation of personal agents",
        "” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival",
        "Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, said in an X post that the company noticed “malicious usage” that led to service degradation",
        "Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result"
      ],
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-webcams-123047068.html",
          "published_at": "Tue, 24 Feb 2026 10:00:36 +0000",
          "title": "The best webcams for 2026",
          "standfirst": "Whether you’re on back-to-back video meetings, live streaming or just trying to look presentable on a family call, your webcam matters more than most might expect. The cameras built into laptops are fine in a pinch, but they rarely deliver consistent image quality, especially in less-than-ideal lighting. A dedicated webcam can noticeably improve sharpness, color accuracy and overall reliability. There’s no single “best” webcam for everyone, though. Some models are built around higher resolutions, while others focus on smoother video, better low-light performance or stronger onboard microphones. We’ve tested a wide range of options to see which ones are actually worth using day to day. Best webcams for 2026 Factors to consider before buying a webcam Resolution and field of view While some newer computers have 1080p webcams, most built-in cameras have a resolution of 720p, so you’ll want to look for an external webcam that has a higher resolution. FHD webcams will give you better video quality; ideally, you’re looking for something that can handle 1080p at 60fps or 30fps. If you’re considering a cheap 720p webcam, make sure to get one that supports at least 30fps (most will) or, even better, 60fps. However, if your primary concern is better picture quality during video calls, 1080p is the way to go. Some webcams can shoot in 4K, but that’s overkill for most people. Not to mention most video conferencing services like Zoom, Google Meet and Skype don’t even support 4K video. When it comes to streaming, Twitch maxes out at 1080p video, but YouTube added 4K live streaming back in 2016. Ultimately, with 4K webcam shots having such limited use, most people can get by with a solid 1080p camera. Field of view (FOV) controls how much can fit in the frame when you’re recording. Most webcams I tested had a default field of view of around 78 degrees, which captured me and enough of my background to prove that I really need to organize my home office. On cheaper webcams you’ll usually see narrower fields of view (around 60 degrees), and those aren’t necessarily bad. They won’t show as much of your background, but that also means you won’t be able to squeeze as many friends or family members into frame when you’re having Zoom birthday parties. On the flip side, more expensive webcams may let you adjust the field of view to be even wider than average, and some even offer features like digital zoom. Autofocus and other “auto” features Webcams with autofocus will keep the image quality sharp without much work on your part. You should be able to move around, step back and forth, and remain in focus the whole time. Some standalone webcam models let you manually adjust focus, too, if you have specific needs. Devices with fixed focus are less convenient, but they tend to be more affordable. In the same vein is auto framing, a feature that some high-end webcams now offer. Similarly to Apple’s Center Stage feature, the camera automatically adjusts to keep you in the center of the frame even as you move around. This used to be a feature only available on the most premium webcams, but now you can find it on sub-$200 devices. You’ll also see other “auto” features listed in webcam specs, most notably auto light correction. This will adjust the camera’s settings to make up for a dimly lit room. If you don’t have bright lights, or often take calls in places where you can’t control the lighting, this feature will be valuable. Alternatively, you might consider using your mirrorless camera as a high-quality webcam solution, taking all of the benefits and features with you (albeit in a cumbersome package). Microphones Most webcams have built-in microphones that, depending on your setup, might end up being closer to you than your computer’s own mics. Check to see if the model you’re considering has mono or stereo mics, as the latter is better. Some even use noise-reduction technology to keep your voice loud and clear. While audiophiles and streamers will want to invest in a standalone microphone, most others can get by using a webcam’s built-in mic. Design There aren’t a ton of fascinating breakthroughs when it comes to external webcam design. Most are round or rectangular devices that clip onto a monitor or your laptop screen. Some have the ability to swivel or screw onto a tripod stand and others can simply sit on your desk beside your computer. But unless you really like having people stare up your nose, the latter isn’t ideal. We recommend clipping your webcam to your monitor and ensuring that it’s at or slightly above eye level. A few webcams go above and beyond by adding hardware extras like built-in lights and lens covers, too. The former can help you stand out in a dark room, while the latter makes it so hackers can’t view you through your webcam without your knowledge. Price Most external webcams that are just good enough to be a step up from your computer’s built-in camera cost between $60 and $150. If the webcam has the same resolution as the internal one on your laptop, you should look out for other specs like auto light correction, a wider field of view or an extra-long connecting cable that can provide a step-up in quality or ease of use. Spending $150 or more means you might get advanced features that tend to be present in a pro webcam like 4K resolution, vertical and horizontal recording options, stereo mics, customizable video settings and more. But unless you’re spending hours on video calls each day or streaming multiple times each week, you can settle on a budget webcam and safely skip most of those high-end options. How we test webcams We primarily test webcams by putting them through as much real-world use as possible. We examine their design, how flexible they are and how easy they are to reposition, and make note of how heavy they are and if that affects their ability to stay put while sitting on top of a screen. We use each webcam for at least a week straight as our primary camera for all video chats, and we make sure to use the device in different lighting environments to test low-light performance. We also use any built-in microphones as our primary audio inputs on video calls as well. Finally, although most of these webcams are plug-and-play, we test out any proprietary software that’s intended to work with each webcam, tweaking things like field of view, video resolution and effects, and using any special features like Show Mode on Logitech webcams. Others webcams we tested Logitech C920s Pro HD Our previous top pick, the Logitech C920s Pro HD webcam remains a solid option for those with less than $100 to spend and really only need a basic 1080p camera to upgrade their setup, or something affordable to make them look better on those inevitable Zoom calls. It has a 78-degree field of view, decent microphones and handy privacy shutter built in. The Brio 500 took the top spot away from this model thanks to its advanced light correction, auto-framing and Show Mode. Webcam FAQs Should I get a 4K or 1080p webcam? It depends on how you plan to use it. A 1080p webcam is more than enough for most video calls, online classes and casual streaming. The picture looks clear, loads quickly and works well even on slower internet connections. A 4K webcam makes sense if you want sharper detail, especially for content creation, professional streaming or recordings you plan to upload. The extra resolution also helps if you crop or zoom in during a call without losing much quality. Keep in mind that 4K requires more bandwidth and not every platform supports it, so think about whether your setup and audience will benefit before spending more. Georgie Peru contributed to this report.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-webcams-123047068.html?src=rss",
          "content": "Whether you’re on back-to-back video meetings, live streaming or just trying to look presentable on a family call, your webcam matters more than most might expect. The cameras built into laptops are fine in a pinch, but they rarely deliver consistent image quality, especially in less-than-ideal lighting. A dedicated webcam can noticeably improve sharpness, color accuracy and overall reliability. There’s no single “best” webcam for everyone, though. Some models are built around higher resolutions, while others focus on smoother video, better low-light performance or stronger onboard microphones. We’ve tested a wide range of options to see which ones are actually worth using day to day. Best webcams for 2026 Factors to consider before buying a webcam Resolution and field of view While some newer computers have 1080p webcams, most built-in cameras have a resolution of 720p, so you’ll want to look for an external webcam that has a higher resolution. FHD webcams will give you better video quality; ideally, you’re looking for something that can handle 1080p at 60fps or 30fps. If you’re considering a cheap 720p webcam, make sure to get one that supports at least 30fps (most will) or, even better, 60fps. However, if your primary concern is better picture quality during video calls, 1080p is the way to go. Some webcams can shoot in 4K, but that’s overkill for most people. Not to mention most video conferencing services like Zoom, Google Meet and Skype don’t even support 4K video. When it comes to streaming, Twitch maxes out at 1080p video, but YouTube added 4K live streaming back in 2016. Ultimately, with 4K webcam shots having such limited use, most people can get by with a solid 1080p camera. Field of view (FOV) controls how much can fit in the frame when you’re recording. Most webcams I tested had a default field of view of around 78 degrees, which captured me and enough of my background to prove that I really need to organize my home office. On cheaper webcams you’ll usually see narrower fields of view (around 60 degrees), and those aren’t necessarily bad. They won’t show as much of your background, but that also means you won’t be able to squeeze as many friends or family members into frame when you’re having Zoom birthday parties. On the flip side, more expensive webcams may let you adjust the field of view to be even wider than average, and some even offer features like digital zoom. Autofocus and other “auto” features Webcams with autofocus will keep the image quality sharp without much work on your part. You should be able to move around, step back and forth, and remain in focus the whole time. Some standalone webcam models let you manually adjust focus, too, if you have specific needs. Devices with fixed focus are less convenient, but they tend to be more affordable. In the same vein is auto framing, a feature that some high-end webcams now offer. Similarly to Apple’s Center Stage feature, the camera automatically adjusts to keep you in the center of the frame even as you move around. This used to be a feature only available on the most premium webcams, but now you can find it on sub-$200 devices. You’ll also see other “auto” features listed in webcam specs, most notably auto light correction. This will adjust the camera’s settings to make up for a dimly lit room. If you don’t have bright lights, or often take calls in places where you can’t control the lighting, this feature will be valuable. Alternatively, you might consider using your mirrorless camera as a high-quality webcam solution, taking all of the benefits and features with you (albeit in a cumbersome package). Microphones Most webcams have built-in microphones that, depending on your setup, might end up being closer to you than your computer’s own mics. Check to see if the model you’re considering has mono or stereo mics, as the latter is better. Some even use noise-reduction technology to keep your voice loud and clear. While audiophiles and streamers will want to invest in a standalone microphone, most others can get by using a webcam’s built-in mic. Design There aren’t a ton of fascinating breakthroughs when it comes to external webcam design. Most are round or rectangular devices that clip onto a monitor or your laptop screen. Some have the ability to swivel or screw onto a tripod stand and others can simply sit on your desk beside your computer. But unless you really like having people stare up your nose, the latter isn’t ideal. We recommend clipping your webcam to your monitor and ensuring that it’s at or slightly above eye level. A few webcams go above and beyond by adding hardware extras like built-in lights and lens covers, too. The former can help you stand out in a dark room, while the latter makes it so hackers can’t view you through your webcam without your knowledge. Price Most external webcams that are just good enough to be a step up from your computer’s built-in camera cost between $60 and $150. If the webcam has the same resolution as the internal one on your laptop, you should look out for other specs like auto light correction, a wider field of view or an extra-long connecting cable that can provide a step-up in quality or ease of use. Spending $150 or more means you might get advanced features that tend to be present in a pro webcam like 4K resolution, vertical and horizontal recording options, stereo mics, customizable video settings and more. But unless you’re spending hours on video calls each day or streaming multiple times each week, you can settle on a budget webcam and safely skip most of those high-end options. How we test webcams We primarily test webcams by putting them through as much real-world use as possible. We examine their design, how flexible they are and how easy they are to reposition, and make note of how heavy they are and if that affects their ability to stay put while sitting on top of a screen. We use each webcam for at least a week straight as our primary camera for all video chats, and we make sure to use the device in different lighting environments to test low-light performance. We also use any built-in microphones as our primary audio inputs on video calls as well. Finally, although most of these webcams are plug-and-play, we test out any proprietary software that’s intended to work with each webcam, tweaking things like field of view, video resolution and effects, and using any special features like Show Mode on Logitech webcams. Others webcams we tested Logitech C920s Pro HD Our previous top pick, the Logitech C920s Pro HD webcam remains a solid option for those with less than $100 to spend and really only need a basic 1080p camera to upgrade their setup, or something affordable to make them look better on those inevitable Zoom calls. It has a 78-degree field of view, decent microphones and handy privacy shutter built in. The Brio 500 took the top spot away from this model thanks to its advanced light correction, auto-framing and Show Mode. Webcam FAQs Should I get a 4K or 1080p webcam? It depends on how you plan to use it. A 1080p webcam is more than enough for most video calls, online classes and casual streaming. The picture looks clear, loads quickly and works well even on slower internet connections. A 4K webcam makes sense if you want sharper detail, especially for content creation, professional streaming or recordings you plan to upload. The extra resolution also helps if you crop or zoom in during a call without losing much quality. Keep in mind that 4K requires more bandwidth and not every platform supports it, so think about whether your setup and audience will benefit before spending more. Georgie Peru contributed to this report.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-webcams-123047068.html?src=rss",
          "feed_position": 3,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2021-04/02284fb0-a11a-11eb-aafb-70e6f3b5aa36"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/google-clamps-down-on-antigravity-malicious-usage-cutting-off-openclaw-users",
          "published_at": "Mon, 23 Feb 2026 23:01:00 GMT",
          "title": "Google clamps down on Antigravity 'malicious usage', cutting off OpenClaw users in sweeping ToS enforcement move",
          "standfirst": "Google caused controversy among some developers this weekend and today, Monday, February 23rd, after restricting their usage of its new Antigravity \"vibe coding\" platform, alleging \"maliciously usage.\" Some users who had been using the open source autonomous AI agent OpenClaw in conjunction with agents built on Antigravity, as well as those who had connected OpenClaw agents to their Gmails, claimed on social media that they lost access to their Google accounts. According to Google, said users had been using Antigravity to access a larger number of Gemini tokens via third-party platforms like OpenClaw, which overwhelmed the system for other Antigravity customers. This move has cut off several users, underscoring the architectural and trust issues that can arise with OpenClaw. The timing of Google’s crackdown is particularly pointed. Just one week ago, on February 15, OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI to lead its “next generation of personal agents.” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival. By cutting off OpenClaw’s access to Antigravity, Google isn’t just protecting its server load; it is effectively severing a pipeline that allows an OpenAI-adjacent tool to leverage Google’s most advanced Gemini models.Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, said in an X post that the company noticed “malicious usage” that led to service degradation.“We’ve been seeing a massive increase in malicious usage of the Antigravity backend that has tremendously degraded the quality of service for our users. We needed to find a path to quickly shut off access to these users that are not using the product as intended. We understand that a subset of these users were not aware that this was against our ToS [Terms of Service] and will get a path for them to come back on but we have limited capacity and want to be fair to our actual users,” the post said. A Google DeepMind spokesperson told VentureBeat that the move is not to permanently ban the use of Antigravity to access third-party platforms, but to align its use with the platform’s terms of service. Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result. Infrastructure and connection uncertaintyOpenClaw emerged as a way for individual users to run shell commands and access local files, fulfilling a major promise of AI agents: efficiently running workflows for users. But, as VentureBeat has frequently pointed out, it can often run into security and guardrail issues. There are companies building ways for enterprise customers to access OpenClaw securely and with a governance layer, though OpenClaw is so new that we should expect more announcements soon.However, Google’s move was not framed as a security issue but rather as one of access and runtime, further showing that there is still significant uncertainty when users want to bring in something like OpenClaw into their workflow. This is not the first time developers and power users of agentic AI found their access curtailed. Last year, Anthropic throttled access to Claude Code after the company claimed some users were abusing the system by running it 24/7. What this does highlight is the disconnect between companies like Google and OpenClaw users. OpenClaw offered many interesting possibilities for creating workflows with agents. However, because it is continually evolving, users may inadvertently run afoul of ToS or rate limits. Mohan said Google is working to bring the banned users back, but whether this means the company will amend its ToS or figure out a secure connection between OpenClaw agents and Antigravity models remains to be seen. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Affected usersSeveral users said on both the Y Combinator chat boards and X that they no longer had access to their Google accounts after running OpenClaw instances for certain Google products. Google’s move mirrors a broader industry shift toward \"walled garden\" agent ecosystems. Earlier this year, Anthropic introduced \"client fingerprinting\" to ensure that its Claude Code environment remains the exclusive interface for its models, effectively locking out third-party wrappers like OpenClaw. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Some have said they will no longer use Google or Gemini for their projects. Right now, people who still want to keep using Antigravity will need to wait until Google figures out a way for them to use OpenClaw and access Gemini tokens in a manner Google deems “fair.” Google DeepMind reiterated that it had only cut access to Antigravity, not to other Google applications. Conclusion: the enterprise takeawayFor enterprise technical decision-makers, the \"Antigravity Ban\" serves as a definitive case study in the risks of agentic dependency. As the industry moves from chatbots to autonomous agents, the following realities must now dictate strategy:Platform fragility is the new normal: The sudden lockout of $250/month \"Ultra\" users proves that even high-paying enterprise customers have little leverage when a provider decides to change its \"fair use\" definitions. Relying on OAuth-based third-party wrappers for core business logic is now a high-risk gamble.The rise of local-first governance: With OpenClaw moving toward an OpenAI-backed foundation and Google/Anthropic tightening their clouds, enterprises should prioritize agent frameworks that can run \"local-first\" or within VPCs. The \"token loophole\" that OpenClaw exploited is being closed; future agentic scale will require direct, high-cost API contracts rather than subsidized consumer seats.Account portability as a requirement: The fact that users \"lost access to their Google accounts\" underscores the danger of bundling development environments with primary identity providers. Decision-makers should decouple AI development from core corporate identity (SSO) where possible to avoid a single ToS violation paralyzing an entire team&#x27;s communications.Ultimately, the Antigravity incident marks the end of the \"Wild West\" for AI agents. As Google and OpenAI stake their claims, the enterprise must choose between the stability of the walled garden or the complexity (and cost) of truly independent, self-hosted infrastructure.",
          "content": "Google caused controversy among some developers this weekend and today, Monday, February 23rd, after restricting their usage of its new Antigravity \"vibe coding\" platform, alleging \"maliciously usage.\" Some users who had been using the open source autonomous AI agent OpenClaw in conjunction with agents built on Antigravity, as well as those who had connected OpenClaw agents to their Gmails, claimed on social media that they lost access to their Google accounts. According to Google, said users had been using Antigravity to access a larger number of Gemini tokens via third-party platforms like OpenClaw, which overwhelmed the system for other Antigravity customers. This move has cut off several users, underscoring the architectural and trust issues that can arise with OpenClaw. The timing of Google’s crackdown is particularly pointed. Just one week ago, on February 15, OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI to lead its “next generation of personal agents.” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival. By cutting off OpenClaw’s access to Antigravity, Google isn’t just protecting its server load; it is effectively severing a pipeline that allows an OpenAI-adjacent tool to leverage Google’s most advanced Gemini models.Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, said in an X post that the company noticed “malicious usage” that led to service degradation.“We’ve been seeing a massive increase in malicious usage of the Antigravity backend that has tremendously degraded the quality of service for our users. We needed to find a path to quickly shut off access to these users that are not using the product as intended. We understand that a subset of these users were not aware that this was against our ToS [Terms of Service] and will get a path for them to come back on but we have limited capacity and want to be fair to our actual users,” the post said. A Google DeepMind spokesperson told VentureBeat that the move is not to permanently ban the use of Antigravity to access third-party platforms, but to align its use with the platform’s terms of service. Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result. Infrastructure and connection uncertaintyOpenClaw emerged as a way for individual users to run shell commands and access local files, fulfilling a major promise of AI agents: efficiently running workflows for users. But, as VentureBeat has frequently pointed out, it can often run into security and guardrail issues. There are companies building ways for enterprise customers to access OpenClaw securely and with a governance layer, though OpenClaw is so new that we should expect more announcements soon.However, Google’s move was not framed as a security issue but rather as one of access and runtime, further showing that there is still significant uncertainty when users want to bring in something like OpenClaw into their workflow. This is not the first time developers and power users of agentic AI found their access curtailed. Last year, Anthropic throttled access to Claude Code after the company claimed some users were abusing the system by running it 24/7. What this does highlight is the disconnect between companies like Google and OpenClaw users. OpenClaw offered many interesting possibilities for creating workflows with agents. However, because it is continually evolving, users may inadvertently run afoul of ToS or rate limits. Mohan said Google is working to bring the banned users back, but whether this means the company will amend its ToS or figure out a secure connection between OpenClaw agents and Antigravity models remains to be seen. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Affected usersSeveral users said on both the Y Combinator chat boards and X that they no longer had access to their Google accounts after running OpenClaw instances for certain Google products. Google’s move mirrors a broader industry shift toward \"walled garden\" agent ecosystems. Earlier this year, Anthropic introduced \"client fingerprinting\" to ensure that its Claude Code environment remains the exclusive interface for its models, effectively locking out third-party wrappers like OpenClaw. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Some have said they will no longer use Google or Gemini for their projects. Right now, people who still want to keep using Antigravity will need to wait until Google figures out a way for them to use OpenClaw and access Gemini tokens in a manner Google deems “fair.” Google DeepMind reiterated that it had only cut access to Antigravity, not to other Google applications. Conclusion: the enterprise takeawayFor enterprise technical decision-makers, the \"Antigravity Ban\" serves as a definitive case study in the risks of agentic dependency. As the industry moves from chatbots to autonomous agents, the following realities must now dictate strategy:Platform fragility is the new normal: The sudden lockout of $250/month \"Ultra\" users proves that even high-paying enterprise customers have little leverage when a provider decides to change its \"fair use\" definitions. Relying on OAuth-based third-party wrappers for core business logic is now a high-risk gamble.The rise of local-first governance: With OpenClaw moving toward an OpenAI-backed foundation and Google/Anthropic tightening their clouds, enterprises should prioritize agent frameworks that can run \"local-first\" or within VPCs. The \"token loophole\" that OpenClaw exploited is being closed; future agentic scale will require direct, high-cost API contracts rather than subsidized consumer seats.Account portability as a requirement: The fact that users \"lost access to their Google accounts\" underscores the danger of bundling development environments with primary identity providers. Decision-makers should decouple AI development from core corporate identity (SSO) where possible to avoid a single ToS violation paralyzing an entire team&#x27;s communications.Ultimately, the Antigravity incident marks the end of the \"Wild West\" for AI agents. As Google and OpenAI stake their claims, the enterprise must choose between the stability of the walled garden or the complexity (and cost) of truly independent, self-hosted infrastructure.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/vwGThxYtgeCfHxjn466tk/5630229b22b3017cc1edff8e5021d66a/crimedy7_illustration_of_a_lobster_thats_in_a_cage_--ar_169_-_2081b356-9d2f-480b-9bef-99c11d44bb10_1.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/one-engineer-made-a-production-saas-product-in-an-hour-heres-the-governance",
          "published_at": "Mon, 23 Feb 2026 22:50:00 GMT",
          "title": "One engineer made a production SaaS product in an hour: here's the governance system that made it possible",
          "standfirst": "Every engineering leader watching the agentic coding wave is eventually going to face the same question: if AI can generate production-quality code faster than any team, what does governance look like when the human isn&#x27;t writing the code anymore?Most teams don&#x27;t have a good answer yet. Treasure Data, a SoftBank-backed customer data platform serving more than 450 global brands, now has one, though they learned parts of it the hard way.The company today officially announced Treasure Code, a new AI-native command-line interface that lets data engineers and platform teams operate its full CDP through natural language, with Claude Code handling creation and iteration underneath. It was built by a single engineer. The company says the coding itself took roughly 60 minutes. But that number is almost beside the point. The more important story is what had to be true before those 60 minutes were possible, and what broke after.\"From a planning standpoint, we still have to plan to derisk the business, and that did take a couple of weeks,\" Rafa Flores, Chief Product Officer at Treasure Data, told VentureBeat. \"From an ideation and execution standpoint, that&#x27;s where you kind of just blend the two and you just go, go, go. And it&#x27;s not just prototyping, it&#x27;s rolling things out in production in a safe way.\"Build the governance layer firstBefore even a single line of code was written, Treasure Data had to answer a harder question: what does the system need to be prohibited from doing, and how do you enforce that at the platform level rather than hoping the code respects it?The guardrails Treasure Data built live upstream of the code itself. When any user connects to the CDP through Treasure Code, access control and permission management are inherited directly from the platform. Users can only reach resources they already have permission for. PII cannot be exposed. API keys cannot be surfaced. The system cannot speak disparagingly about a brand or competitor.\"We had to get CISOs involved. I was involved. Our CTO, heads of engineering, just to make sure that this thing didn&#x27;t just go rogue,\" Flores said.This foundation made the next step possible: letting AI generate 100% of the codebase, with a three-tier quality pipeline enforcing production standards throughout.The three-tier pipeline for AI code generation The first tier is an AI-based code reviewer also using Claude Code. The code reviewer sits at the pull request stage and runs a structured review checklist against every proposed merge, checking for architectural alignment, security compliance, proper error handling, test coverage and documentation quality. When all criteria are satisfied it can merge automatically. When they aren&#x27;t, it flags for human intervention.The fact that Treasure Data built the code reviewer in Claude Code is not incidental. It means the tool validating AI-generated code was itself AI-generated, a proof point that the workflow is self-reinforcing rather than dependent on a separate human-written quality layer.The second tier is a standard CI/CD pipeline running automated unit, integration and end-to-end tests, static analysis, linting and security checks against every change. The third is human review, required wherever automated systems flag risk or enterprise policy demands sign-off.The internal principle Treasure Data operates under: AI writes code, but AI does not ship code.Why this isn&#x27;t just Cursor pointed at a databaseThe obvious question for any engineering team is why not just point an existing tool like Cursor at your data platform, or expose it as an MCP server and let Claude Code query it directly.Flores argued the difference is governance depth. A generic connection gives you natural language access to data but inherits none of the platform&#x27;s existing permission structures, meaning every query runs with whatever access the API key allows. Treasure Code inherits Treasure Data&#x27;s full access control and permissioning layer, so what a user can do through natural language is bounded by what they&#x27;re already authorized to do in the platform. The second distinction is orchestration. Because Treasure Code connects directly to Treasure Data&#x27;s AI Agent Foundry, it can coordinate sub-agents and skills across the platform rather than executing single tasks in isolation: the difference between telling an AI to run an analysis and having it orchestrate that analysis across omni-channel activation, segmentation and reporting simultaneously.What broke anywayEven with the governance architecture in place, the launch didn&#x27;t go cleanly, and Flores was candid about it.Treasure Data initially made Treasure Code available to customers without a go-to-market plan. The assumption was that it would stay quiet while the team figured out next steps. Customers found it anyway. More than 100 customers and close to 1,000 users adopted it within two weeks, entirely through organic discovery.\"We didn&#x27;t put any go-to-market motions behind it. We didn&#x27;t think people were going to find it. Well, they did,\" Flores said. \"We were left scrambling with, how do we actually do the go-to-market motions? Do we even do a beta, since technically it&#x27;s live?\"The unplanned adoption also created a compliance gap. Treasure Data is still in the process of formally certifying Treasure Code under its Trust AI compliance program, a certification it had not completed before the product reached customers.A second problem emerged when Treasure Data opened skill development to non-engineering teams. CSMs and account directors began building and submitting skills without understanding what would get approved and merged, creating significant wasted effort and a backlog of submissions that couldn&#x27;t clear the repository&#x27;s access policies.Enterprise validation and what&#x27;s still missingThomson Reuters is among the early adopters. Flores said that the company had been attempting to build an in-house AI agent platform and struggling to move fast enough. It connected with Treasure Data&#x27;s AI Agent Foundry to accelerate audience segmentation work, then extended into Treasure Code to customize and iterate more rapidly.The feedback, Flores said, has centered on extensibility and flexibility, and the fact that procurement was already done, removing a significant enterprise barrier to adoption.The gap Thomson Reuters has flagged, and that Flores acknowledges the product doesn&#x27;t yet address, is guidance on AI maturity. Treasure Code doesn&#x27;t tell users who should use it, what to tackle first, or how to structure access across different skill levels within an organization.\"AI that allows you to be leveraged, but also tells you how to leverage it, I think that&#x27;s very differentiated,\" Flores said. He sees it as the next meaningful layer to build.What engineering leaders should take from thisFlores has had time to reflect on what the experience actually taught him, and he was direct about what he&#x27;d change. Next time, he said, the release would stay internal first.\"We will release it internally only. I will not release it to anyone outside of the organization,\" he said. \"It will be more of a controlled release so we can actually learn what we&#x27;re actually being exposed to at lower risk.\"On skill development, the lesson was to establish clear criteria for what gets approved and merged before opening the process to teams outside engineering, not after.The common thread in both lessons is the same one that shaped the governance architecture and the three-tier pipeline: speed is only an advantage if the structure around it holds. For engineering leaders evaluating whether agentic coding is ready for production, the Treasure Data experience translates into three practical conclusions.Governance infrastructure has to precede the code, not follow it. The platform-level access controls and permission inheritance were what made it safe to let AI generate freely. Without that foundation, the speed advantage disappears because every output requires exhaustive manual review.A quality gate that doesn&#x27;t depend entirely on humans is not optional at scale. Build a quality gate that doesn&#x27;t depend entirely on humans. AI can review every pull request consistently, without fatigue, and check policy compliance systematically across the entire codebase. Human review remains essential, but as a final check rather than the primary quality mechanism.Plan for organic adoption. If the product works, people will find it before you&#x27;re ready. The compliance and go-to-market gaps Treasure Data is still closing are a direct result of underestimating that.\"Yes, vibe coding can work if done in a safe way and proper guardrails are in place,\" Flores said. \"Embrace it in a way to find means of not replacing the good work you do, but the tedious work that you can probably automate.\"",
          "content": "Every engineering leader watching the agentic coding wave is eventually going to face the same question: if AI can generate production-quality code faster than any team, what does governance look like when the human isn&#x27;t writing the code anymore?Most teams don&#x27;t have a good answer yet. Treasure Data, a SoftBank-backed customer data platform serving more than 450 global brands, now has one, though they learned parts of it the hard way.The company today officially announced Treasure Code, a new AI-native command-line interface that lets data engineers and platform teams operate its full CDP through natural language, with Claude Code handling creation and iteration underneath. It was built by a single engineer. The company says the coding itself took roughly 60 minutes. But that number is almost beside the point. The more important story is what had to be true before those 60 minutes were possible, and what broke after.\"From a planning standpoint, we still have to plan to derisk the business, and that did take a couple of weeks,\" Rafa Flores, Chief Product Officer at Treasure Data, told VentureBeat. \"From an ideation and execution standpoint, that&#x27;s where you kind of just blend the two and you just go, go, go. And it&#x27;s not just prototyping, it&#x27;s rolling things out in production in a safe way.\"Build the governance layer firstBefore even a single line of code was written, Treasure Data had to answer a harder question: what does the system need to be prohibited from doing, and how do you enforce that at the platform level rather than hoping the code respects it?The guardrails Treasure Data built live upstream of the code itself. When any user connects to the CDP through Treasure Code, access control and permission management are inherited directly from the platform. Users can only reach resources they already have permission for. PII cannot be exposed. API keys cannot be surfaced. The system cannot speak disparagingly about a brand or competitor.\"We had to get CISOs involved. I was involved. Our CTO, heads of engineering, just to make sure that this thing didn&#x27;t just go rogue,\" Flores said.This foundation made the next step possible: letting AI generate 100% of the codebase, with a three-tier quality pipeline enforcing production standards throughout.The three-tier pipeline for AI code generation The first tier is an AI-based code reviewer also using Claude Code. The code reviewer sits at the pull request stage and runs a structured review checklist against every proposed merge, checking for architectural alignment, security compliance, proper error handling, test coverage and documentation quality. When all criteria are satisfied it can merge automatically. When they aren&#x27;t, it flags for human intervention.The fact that Treasure Data built the code reviewer in Claude Code is not incidental. It means the tool validating AI-generated code was itself AI-generated, a proof point that the workflow is self-reinforcing rather than dependent on a separate human-written quality layer.The second tier is a standard CI/CD pipeline running automated unit, integration and end-to-end tests, static analysis, linting and security checks against every change. The third is human review, required wherever automated systems flag risk or enterprise policy demands sign-off.The internal principle Treasure Data operates under: AI writes code, but AI does not ship code.Why this isn&#x27;t just Cursor pointed at a databaseThe obvious question for any engineering team is why not just point an existing tool like Cursor at your data platform, or expose it as an MCP server and let Claude Code query it directly.Flores argued the difference is governance depth. A generic connection gives you natural language access to data but inherits none of the platform&#x27;s existing permission structures, meaning every query runs with whatever access the API key allows. Treasure Code inherits Treasure Data&#x27;s full access control and permissioning layer, so what a user can do through natural language is bounded by what they&#x27;re already authorized to do in the platform. The second distinction is orchestration. Because Treasure Code connects directly to Treasure Data&#x27;s AI Agent Foundry, it can coordinate sub-agents and skills across the platform rather than executing single tasks in isolation: the difference between telling an AI to run an analysis and having it orchestrate that analysis across omni-channel activation, segmentation and reporting simultaneously.What broke anywayEven with the governance architecture in place, the launch didn&#x27;t go cleanly, and Flores was candid about it.Treasure Data initially made Treasure Code available to customers without a go-to-market plan. The assumption was that it would stay quiet while the team figured out next steps. Customers found it anyway. More than 100 customers and close to 1,000 users adopted it within two weeks, entirely through organic discovery.\"We didn&#x27;t put any go-to-market motions behind it. We didn&#x27;t think people were going to find it. Well, they did,\" Flores said. \"We were left scrambling with, how do we actually do the go-to-market motions? Do we even do a beta, since technically it&#x27;s live?\"The unplanned adoption also created a compliance gap. Treasure Data is still in the process of formally certifying Treasure Code under its Trust AI compliance program, a certification it had not completed before the product reached customers.A second problem emerged when Treasure Data opened skill development to non-engineering teams. CSMs and account directors began building and submitting skills without understanding what would get approved and merged, creating significant wasted effort and a backlog of submissions that couldn&#x27;t clear the repository&#x27;s access policies.Enterprise validation and what&#x27;s still missingThomson Reuters is among the early adopters. Flores said that the company had been attempting to build an in-house AI agent platform and struggling to move fast enough. It connected with Treasure Data&#x27;s AI Agent Foundry to accelerate audience segmentation work, then extended into Treasure Code to customize and iterate more rapidly.The feedback, Flores said, has centered on extensibility and flexibility, and the fact that procurement was already done, removing a significant enterprise barrier to adoption.The gap Thomson Reuters has flagged, and that Flores acknowledges the product doesn&#x27;t yet address, is guidance on AI maturity. Treasure Code doesn&#x27;t tell users who should use it, what to tackle first, or how to structure access across different skill levels within an organization.\"AI that allows you to be leveraged, but also tells you how to leverage it, I think that&#x27;s very differentiated,\" Flores said. He sees it as the next meaningful layer to build.What engineering leaders should take from thisFlores has had time to reflect on what the experience actually taught him, and he was direct about what he&#x27;d change. Next time, he said, the release would stay internal first.\"We will release it internally only. I will not release it to anyone outside of the organization,\" he said. \"It will be more of a controlled release so we can actually learn what we&#x27;re actually being exposed to at lower risk.\"On skill development, the lesson was to establish clear criteria for what gets approved and merged before opening the process to teams outside engineering, not after.The common thread in both lessons is the same one that shaped the governance architecture and the three-tier pipeline: speed is only an advantage if the structure around it holds. For engineering leaders evaluating whether agentic coding is ready for production, the Treasure Data experience translates into three practical conclusions.Governance infrastructure has to precede the code, not follow it. The platform-level access controls and permission inheritance were what made it safe to let AI generate freely. Without that foundation, the speed advantage disappears because every output requires exhaustive manual review.A quality gate that doesn&#x27;t depend entirely on humans is not optional at scale. Build a quality gate that doesn&#x27;t depend entirely on humans. AI can review every pull request consistently, without fatigue, and check policy compliance systematically across the entire codebase. Human review remains essential, but as a final check rather than the primary quality mechanism.Plan for organic adoption. If the product works, people will find it before you&#x27;re ready. The compliance and go-to-market gaps Treasure Data is still closing are a direct result of underestimating that.\"Yes, vibe coding can work if done in a safe way and proper guardrails are in place,\" Flores said. \"Embrace it in a way to find means of not replacing the good work you do, but the tedious work that you can probably automate.\"",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2LXjt32cwUg6zhRZwofUc7/6771abe5244e9fe9fff86de9ba0cf984/treasure-data-vibe-coding-smk1.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/anthropic-says-deepseek-moonshot-and-minimax-used-24-000-fake-accounts-to",
          "published_at": "Mon, 23 Feb 2026 22:20:00 GMT",
          "title": "Anthropic says DeepSeek, Moonshot, and MiniMax used 24,000 fake accounts to rip off Claude",
          "standfirst": "Anthropic dropped a bombshell on the artificial intelligence industry Monday, publicly accusing three prominent Chinese AI laboratories — DeepSeek, Moonshot AI, and MiniMax — of orchestrating coordinated, industrial-scale campaigns to siphon capabilities from its Claude models using tens of thousands of fraudulent accounts.The San Francisco-based company said the three labs collectively generated more than 16 million exchanges with Claude through approximately 24,000 fake accounts, all in violation of Anthropic&#x27;s terms of service and regional access restrictions. The campaigns, Anthropic said, are the most concrete and detailed public evidence to date of a practice that has haunted Silicon Valley for months: foreign competitors systematically using a technique called distillation to leapfrog years of research and billions of dollars in investment.\"These campaigns are growing in intensity and sophistication,\" Anthropic wrote in a technical blog post published Monday. \"The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.\"The disclosure marks a dramatic escalation in the simmering tensions between American and Chinese AI developers — and it arrives at a moment when Washington is actively debating whether to tighten or loosen export controls on the advanced chips that power AI training. Anthropic, led by CEO Dario Amodei, has been among the most vocal advocates for restricting chip sales to China, and the company explicitly connected Monday&#x27;s revelations to that policy fight.How AI distillation went from obscure research technique to geopolitical flashpointTo understand what Anthropic alleges, it helps to understand what distillation actually is — and how it evolved from an academic curiosity into the most contentious issue in the global AI race.At its core, distillation is a process of extracting knowledge from a larger, more powerful AI model — the \"teacher\" — to create a smaller, more efficient one — the \"student.\" The student model learns not from raw data, but from the teacher&#x27;s outputs: its answers, reasoning patterns, and behaviors. Done correctly, the student can achieve performance remarkably close to the teacher&#x27;s while requiring a fraction of the compute to train.As Anthropic itself acknowledged, distillation is \"a widely used and legitimate training method.\" Frontier AI labs, including Anthropic, routinely distill their own models to create smaller, cheaper versions for customers. But the same technique can be weaponized. A competitor can pose as a legitimate customer, bombard a frontier model with carefully crafted prompts, collect the outputs, and use those outputs to train a rival system — capturing capabilities that took years and hundreds of millions of dollars to develop.The technique burst into public consciousness in January 2025 when DeepSeek released its R1 reasoning model, which appeared to match or approach the performance of leading American models at dramatically lower cost. Databricks CEO Ali Ghodsi captured the industry&#x27;s anxiety at the time, telling CNBC: \"This distillation technique is just so extremely powerful and so extremely cheap, and it&#x27;s just available to anyone.\" He predicted the technique would usher in an era of intense competition for large language models.That prediction proved prescient. In the weeks following DeepSeek&#x27;s release, researchers at UC Berkeley said they recreated OpenAI&#x27;s reasoning model for just $450 in 19 hours. Researchers at Stanford and the University of Washington followed with their own version built in 26 minutes for under $50 in compute credits. The startup Hugging Face replicated OpenAI&#x27;s Deep Research feature as a 24-hour coding challenge. DeepSeek itself openly released a family of distilled models on Hugging Face — including versions built on top of Qwen and Llama architectures — under the permissive MIT license, with the model card explicitly stating that the DeepSeek-R1 series supports commercial use and allows for any modifications and derivative works, \"including, but not limited to, distillation for training other LLMs.\"But what Anthropic described Monday goes far beyond academic replication or open-source experimentation. The company detailed what it characterized as deliberate, covert, and large-scale intellectual property extraction by well-resourced commercial laboratories operating under the jurisdiction of the Chinese government.Anthropic traces 16 million fraudulent exchanges to researchers at DeepSeek, Moonshot, and MiniMaxAnthropic attributed each campaign \"with high confidence\" through IP address correlation, request metadata, infrastructure indicators, and corroboration from unnamed industry partners who observed the same actors on their own platforms. Each campaign specifically targeted what Anthropic described as Claude&#x27;s most differentiated capabilities: agentic reasoning, tool use, and coding.DeepSeek, the company that ignited the distillation debate, conducted what Anthropic described as the most technically sophisticated of the three operations, generating over 150,000 exchanges with Claude. Anthropic said DeepSeek&#x27;s prompts targeted reasoning capabilities, rubric-based grading tasks designed to make Claude function as a reward model for reinforcement learning, and — in a detail likely to draw particular political attention — the creation of \"censorship-safe alternatives to policy sensitive queries.\"Anthropic alleged that DeepSeek \"generated synchronized traffic across accounts\" with \"identical patterns, shared payment methods, and coordinated timing\" that suggested load balancing to maximize throughput while evading detection. In one particularly notable technique, Anthropic said DeepSeek&#x27;s prompts \"asked Claude to imagine and articulate the internal reasoning behind a completed response and write it out step by step — effectively generating chain-of-thought training data at scale.\" The company also alleged it observed tasks in which Claude was used to generate alternatives to politically sensitive queries about \"dissidents, party leaders, or authoritarianism,\" likely to train DeepSeek&#x27;s own models to steer conversations away from censored topics. Anthropic said it was able to trace these accounts to specific researchers at the lab.Moonshot AI, the Beijing-based creator of the Kimi models, ran the second-largest operation by volume at over 3.4 million exchanges. Anthropic said Moonshot targeted agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. The company employed \"hundreds of fraudulent accounts spanning multiple access pathways,\" making the campaign harder to detect as a coordinated operation. Anthropic attributed the campaign through request metadata that \"matched the public profiles of senior Moonshot staff.\" In a later phase, Anthropic said, Moonshot adopted a more targeted approach, \"attempting to extract and reconstruct Claude&#x27;s reasoning traces.\"MiniMax, the least publicly known of the three but the most prolific by volume, generated over 13 million exchanges — more than three-quarters of the total. Anthropic said MiniMax&#x27;s campaign focused on agentic coding, tool use, and orchestration. The company said it detected MiniMax&#x27;s campaign while it was still active, \"before MiniMax released the model it was training,\" giving Anthropic \"unprecedented visibility into the life cycle of distillation attacks, from data generation through to model launch.\" In a detail that underscores the urgency and opportunism Anthropic alleges, the company said that when it released a new model during MiniMax&#x27;s active campaign, MiniMax \"pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system.\"How proxy networks and &#x27;hydra cluster&#x27; architectures helped Chinese labs bypass Anthropic&#x27;s China banAnthropic does not currently offer commercial access to Claude in China, a policy it maintains for national security reasons. So how did these labs access the models at all?The answer, Anthropic said, lies in commercial proxy services that resell access to Claude and other frontier AI models at scale. Anthropic described these services as running what it calls \"hydra cluster\" architectures — sprawling networks of fraudulent accounts that distribute traffic across Anthropic&#x27;s API and third-party cloud platforms. \"The breadth of these networks means that there are no single points of failure,\" Anthropic wrote. \"When one account is banned, a new one takes its place.\" In one case, Anthropic said, a single proxy network managed more than 20,000 fraudulent accounts simultaneously, mixing distillation traffic with unrelated customer requests to make detection harder.The description suggests a mature and well-resourced infrastructure ecosystem dedicated to circumventing access controls — one that may serve many more clients than just the three labs Anthropic named.Why Anthropic framed distillation as a national security crisis, not just an IP disputeAnthropic did not treat this as a mere terms-of-service violation. The company embedded its technical disclosure within an explicit national security argument, warning that \"illicitly distilled models lack necessary safeguards, creating significant national security risks.\"The company argued that models built through illicit distillation are \"unlikely to retain\" the safety guardrails that American companies build into their systems — protections designed to prevent AI from being used to develop bioweapons, carry out cyberattacks, or enable mass surveillance. \"Foreign labs that distill American models can then feed these unprotected capabilities into military, intelligence, and surveillance systems,\" Anthropic wrote, \"enabling authoritarian governments to deploy frontier AI for offensive cyber operations, disinformation campaigns, and mass surveillance.\"This framing directly connects to the chip export control debate that Amodei has made a centerpiece of his public advocacy. In a detailed essay published in January 2025, Amodei argued that export controls are \"the most important determinant of whether we end up in a unipolar or bipolar world\" — a world where either only the U.S. and its allies possess the most powerful AI, or one where China achieves parity. He specifically noted at the time that he was \"not taking any position on reports of distillation from Western models\" and would \"just take DeepSeek at their word that they trained it the way they said in the paper.\"Monday&#x27;s disclosure is a sharp departure from that earlier restraint. Anthropic now argues that distillation attacks \"undermine\" export controls \"by allowing foreign labs, including those subject to the control of the Chinese Communist Party, to close the competitive advantage that export controls are designed to preserve through other means.\" The company went further, asserting that \"without visibility into these attacks, the apparently rapid advancements made by these labs are incorrectly taken as evidence that export controls are ineffective.\" In other words, Anthropic is arguing that what some observers interpreted as proof that Chinese labs can innovate around chip restrictions was actually, in significant part, the result of stealing American capabilities.The murky legal landscape around AI distillation may explain Anthropic&#x27;s political strategyAnthropic&#x27;s decision to frame this as a national security issue rather than a legal dispute may reflect the difficult reality that intellectual property law offers limited recourse against distillation.As a March 2025 analysis by the law firm Winston & Strawn noted, \"the legal landscape surrounding AI distillation is unclear and evolving.\" The firm&#x27;s attorneys observed that proving a copyright claim in this context would be challenging, since it remains unclear whether the outputs of AI models qualify as copyrightable creative expression. The U.S. Copyright Office affirmed in January 2025 that copyright protection requires human authorship, and that \"mere provision of prompts does not render the outputs copyrightable.\"The legal picture is further complicated by the way frontier labs structure output ownership. OpenAI&#x27;s terms of use, for instance, assign ownership of model outputs to the user — meaning that even if a company can prove extraction occurred, it may not hold copyrights over the extracted data. Winston & Strawn noted that this dynamic means \"even if OpenAI can present enough evidence to show that DeepSeek extracted data from its models, OpenAI likely does not have copyrights over the data.\" The same logic would almost certainly apply to Anthropic&#x27;s outputs.Contract law may offer a more promising avenue. Anthropic&#x27;s terms of service prohibit the kind of systematic extraction the company describes, and violation of those terms is a more straightforward legal claim than copyright infringement. But enforcing contractual terms against entities operating through proxy services and fraudulent accounts in a foreign jurisdiction presents its own formidable challenges.This may explain why Anthropic chose the national security frame over a purely legal one. By positioning distillation attacks as threats to export control regimes and democratic security rather than as intellectual property disputes, Anthropic appeals to policymakers and regulators who have tools — sanctions, entity list designations, enhanced export restrictions — that go far beyond what civil litigation could achieve.What Anthropic&#x27;s distillation crackdown means for every company running a frontier AI modelAnthropic outlined a multipronged defensive response. The company said it has built classifiers and behavioral fingerprinting systems designed to identify distillation attack patterns in API traffic, including detection of chain-of-thought elicitation used to construct reasoning training data. It is sharing technical indicators with other AI labs, cloud providers, and relevant authorities to build what it described as a more holistic picture of the distillation landscape. The company has also strengthened verification for educational accounts, security research programs, and startup organizations — the pathways most commonly exploited for setting up fraudulent accounts — and is developing model-level safeguards designed to reduce the usefulness of outputs for illicit distillation without degrading the experience for legitimate customers.But the company acknowledged that \"no company can solve this alone,\" calling for coordinated action across the industry, cloud providers, and policymakers.The disclosure is likely to reverberate through multiple ongoing policy debates. In Congress, the bipartisan No DeepSeek on Government Devices Act has already been introduced. Federal agencies including NASA have banned DeepSeek from employee devices. And the broader question of chip export controls — which the Trump administration has been weighing amid competing pressures from Nvidia and national security hawks — now has a new and vivid data point.For the AI industry&#x27;s technical decision-makers, the implications are immediate and practical. If Anthropic&#x27;s account is accurate, the proxy infrastructure enabling these attacks is vast, sophisticated, and adaptable — and it is not limited to targeting a single company. Every frontier AI lab with an API is a potential target. The era of treating model access as a simple commercial transaction may be coming to an end, replaced by one in which API security is as strategically important as the model weights themselves.Anthropic has now put names, numbers, and forensic detail behind accusations that the industry had only whispered about for months. Whether that evidence galvanizes the coordinated response the company is calling for — or simply accelerates an arms race between distillers and defenders — may depend on a question no classifier can answer: whether Washington sees this as an act of espionage or just the cost of doing business in an era when intelligence itself has become a commodity.",
          "content": "Anthropic dropped a bombshell on the artificial intelligence industry Monday, publicly accusing three prominent Chinese AI laboratories — DeepSeek, Moonshot AI, and MiniMax — of orchestrating coordinated, industrial-scale campaigns to siphon capabilities from its Claude models using tens of thousands of fraudulent accounts.The San Francisco-based company said the three labs collectively generated more than 16 million exchanges with Claude through approximately 24,000 fake accounts, all in violation of Anthropic&#x27;s terms of service and regional access restrictions. The campaigns, Anthropic said, are the most concrete and detailed public evidence to date of a practice that has haunted Silicon Valley for months: foreign competitors systematically using a technique called distillation to leapfrog years of research and billions of dollars in investment.\"These campaigns are growing in intensity and sophistication,\" Anthropic wrote in a technical blog post published Monday. \"The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.\"The disclosure marks a dramatic escalation in the simmering tensions between American and Chinese AI developers — and it arrives at a moment when Washington is actively debating whether to tighten or loosen export controls on the advanced chips that power AI training. Anthropic, led by CEO Dario Amodei, has been among the most vocal advocates for restricting chip sales to China, and the company explicitly connected Monday&#x27;s revelations to that policy fight.How AI distillation went from obscure research technique to geopolitical flashpointTo understand what Anthropic alleges, it helps to understand what distillation actually is — and how it evolved from an academic curiosity into the most contentious issue in the global AI race.At its core, distillation is a process of extracting knowledge from a larger, more powerful AI model — the \"teacher\" — to create a smaller, more efficient one — the \"student.\" The student model learns not from raw data, but from the teacher&#x27;s outputs: its answers, reasoning patterns, and behaviors. Done correctly, the student can achieve performance remarkably close to the teacher&#x27;s while requiring a fraction of the compute to train.As Anthropic itself acknowledged, distillation is \"a widely used and legitimate training method.\" Frontier AI labs, including Anthropic, routinely distill their own models to create smaller, cheaper versions for customers. But the same technique can be weaponized. A competitor can pose as a legitimate customer, bombard a frontier model with carefully crafted prompts, collect the outputs, and use those outputs to train a rival system — capturing capabilities that took years and hundreds of millions of dollars to develop.The technique burst into public consciousness in January 2025 when DeepSeek released its R1 reasoning model, which appeared to match or approach the performance of leading American models at dramatically lower cost. Databricks CEO Ali Ghodsi captured the industry&#x27;s anxiety at the time, telling CNBC: \"This distillation technique is just so extremely powerful and so extremely cheap, and it&#x27;s just available to anyone.\" He predicted the technique would usher in an era of intense competition for large language models.That prediction proved prescient. In the weeks following DeepSeek&#x27;s release, researchers at UC Berkeley said they recreated OpenAI&#x27;s reasoning model for just $450 in 19 hours. Researchers at Stanford and the University of Washington followed with their own version built in 26 minutes for under $50 in compute credits. The startup Hugging Face replicated OpenAI&#x27;s Deep Research feature as a 24-hour coding challenge. DeepSeek itself openly released a family of distilled models on Hugging Face — including versions built on top of Qwen and Llama architectures — under the permissive MIT license, with the model card explicitly stating that the DeepSeek-R1 series supports commercial use and allows for any modifications and derivative works, \"including, but not limited to, distillation for training other LLMs.\"But what Anthropic described Monday goes far beyond academic replication or open-source experimentation. The company detailed what it characterized as deliberate, covert, and large-scale intellectual property extraction by well-resourced commercial laboratories operating under the jurisdiction of the Chinese government.Anthropic traces 16 million fraudulent exchanges to researchers at DeepSeek, Moonshot, and MiniMaxAnthropic attributed each campaign \"with high confidence\" through IP address correlation, request metadata, infrastructure indicators, and corroboration from unnamed industry partners who observed the same actors on their own platforms. Each campaign specifically targeted what Anthropic described as Claude&#x27;s most differentiated capabilities: agentic reasoning, tool use, and coding.DeepSeek, the company that ignited the distillation debate, conducted what Anthropic described as the most technically sophisticated of the three operations, generating over 150,000 exchanges with Claude. Anthropic said DeepSeek&#x27;s prompts targeted reasoning capabilities, rubric-based grading tasks designed to make Claude function as a reward model for reinforcement learning, and — in a detail likely to draw particular political attention — the creation of \"censorship-safe alternatives to policy sensitive queries.\"Anthropic alleged that DeepSeek \"generated synchronized traffic across accounts\" with \"identical patterns, shared payment methods, and coordinated timing\" that suggested load balancing to maximize throughput while evading detection. In one particularly notable technique, Anthropic said DeepSeek&#x27;s prompts \"asked Claude to imagine and articulate the internal reasoning behind a completed response and write it out step by step — effectively generating chain-of-thought training data at scale.\" The company also alleged it observed tasks in which Claude was used to generate alternatives to politically sensitive queries about \"dissidents, party leaders, or authoritarianism,\" likely to train DeepSeek&#x27;s own models to steer conversations away from censored topics. Anthropic said it was able to trace these accounts to specific researchers at the lab.Moonshot AI, the Beijing-based creator of the Kimi models, ran the second-largest operation by volume at over 3.4 million exchanges. Anthropic said Moonshot targeted agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. The company employed \"hundreds of fraudulent accounts spanning multiple access pathways,\" making the campaign harder to detect as a coordinated operation. Anthropic attributed the campaign through request metadata that \"matched the public profiles of senior Moonshot staff.\" In a later phase, Anthropic said, Moonshot adopted a more targeted approach, \"attempting to extract and reconstruct Claude&#x27;s reasoning traces.\"MiniMax, the least publicly known of the three but the most prolific by volume, generated over 13 million exchanges — more than three-quarters of the total. Anthropic said MiniMax&#x27;s campaign focused on agentic coding, tool use, and orchestration. The company said it detected MiniMax&#x27;s campaign while it was still active, \"before MiniMax released the model it was training,\" giving Anthropic \"unprecedented visibility into the life cycle of distillation attacks, from data generation through to model launch.\" In a detail that underscores the urgency and opportunism Anthropic alleges, the company said that when it released a new model during MiniMax&#x27;s active campaign, MiniMax \"pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system.\"How proxy networks and &#x27;hydra cluster&#x27; architectures helped Chinese labs bypass Anthropic&#x27;s China banAnthropic does not currently offer commercial access to Claude in China, a policy it maintains for national security reasons. So how did these labs access the models at all?The answer, Anthropic said, lies in commercial proxy services that resell access to Claude and other frontier AI models at scale. Anthropic described these services as running what it calls \"hydra cluster\" architectures — sprawling networks of fraudulent accounts that distribute traffic across Anthropic&#x27;s API and third-party cloud platforms. \"The breadth of these networks means that there are no single points of failure,\" Anthropic wrote. \"When one account is banned, a new one takes its place.\" In one case, Anthropic said, a single proxy network managed more than 20,000 fraudulent accounts simultaneously, mixing distillation traffic with unrelated customer requests to make detection harder.The description suggests a mature and well-resourced infrastructure ecosystem dedicated to circumventing access controls — one that may serve many more clients than just the three labs Anthropic named.Why Anthropic framed distillation as a national security crisis, not just an IP disputeAnthropic did not treat this as a mere terms-of-service violation. The company embedded its technical disclosure within an explicit national security argument, warning that \"illicitly distilled models lack necessary safeguards, creating significant national security risks.\"The company argued that models built through illicit distillation are \"unlikely to retain\" the safety guardrails that American companies build into their systems — protections designed to prevent AI from being used to develop bioweapons, carry out cyberattacks, or enable mass surveillance. \"Foreign labs that distill American models can then feed these unprotected capabilities into military, intelligence, and surveillance systems,\" Anthropic wrote, \"enabling authoritarian governments to deploy frontier AI for offensive cyber operations, disinformation campaigns, and mass surveillance.\"This framing directly connects to the chip export control debate that Amodei has made a centerpiece of his public advocacy. In a detailed essay published in January 2025, Amodei argued that export controls are \"the most important determinant of whether we end up in a unipolar or bipolar world\" — a world where either only the U.S. and its allies possess the most powerful AI, or one where China achieves parity. He specifically noted at the time that he was \"not taking any position on reports of distillation from Western models\" and would \"just take DeepSeek at their word that they trained it the way they said in the paper.\"Monday&#x27;s disclosure is a sharp departure from that earlier restraint. Anthropic now argues that distillation attacks \"undermine\" export controls \"by allowing foreign labs, including those subject to the control of the Chinese Communist Party, to close the competitive advantage that export controls are designed to preserve through other means.\" The company went further, asserting that \"without visibility into these attacks, the apparently rapid advancements made by these labs are incorrectly taken as evidence that export controls are ineffective.\" In other words, Anthropic is arguing that what some observers interpreted as proof that Chinese labs can innovate around chip restrictions was actually, in significant part, the result of stealing American capabilities.The murky legal landscape around AI distillation may explain Anthropic&#x27;s political strategyAnthropic&#x27;s decision to frame this as a national security issue rather than a legal dispute may reflect the difficult reality that intellectual property law offers limited recourse against distillation.As a March 2025 analysis by the law firm Winston & Strawn noted, \"the legal landscape surrounding AI distillation is unclear and evolving.\" The firm&#x27;s attorneys observed that proving a copyright claim in this context would be challenging, since it remains unclear whether the outputs of AI models qualify as copyrightable creative expression. The U.S. Copyright Office affirmed in January 2025 that copyright protection requires human authorship, and that \"mere provision of prompts does not render the outputs copyrightable.\"The legal picture is further complicated by the way frontier labs structure output ownership. OpenAI&#x27;s terms of use, for instance, assign ownership of model outputs to the user — meaning that even if a company can prove extraction occurred, it may not hold copyrights over the extracted data. Winston & Strawn noted that this dynamic means \"even if OpenAI can present enough evidence to show that DeepSeek extracted data from its models, OpenAI likely does not have copyrights over the data.\" The same logic would almost certainly apply to Anthropic&#x27;s outputs.Contract law may offer a more promising avenue. Anthropic&#x27;s terms of service prohibit the kind of systematic extraction the company describes, and violation of those terms is a more straightforward legal claim than copyright infringement. But enforcing contractual terms against entities operating through proxy services and fraudulent accounts in a foreign jurisdiction presents its own formidable challenges.This may explain why Anthropic chose the national security frame over a purely legal one. By positioning distillation attacks as threats to export control regimes and democratic security rather than as intellectual property disputes, Anthropic appeals to policymakers and regulators who have tools — sanctions, entity list designations, enhanced export restrictions — that go far beyond what civil litigation could achieve.What Anthropic&#x27;s distillation crackdown means for every company running a frontier AI modelAnthropic outlined a multipronged defensive response. The company said it has built classifiers and behavioral fingerprinting systems designed to identify distillation attack patterns in API traffic, including detection of chain-of-thought elicitation used to construct reasoning training data. It is sharing technical indicators with other AI labs, cloud providers, and relevant authorities to build what it described as a more holistic picture of the distillation landscape. The company has also strengthened verification for educational accounts, security research programs, and startup organizations — the pathways most commonly exploited for setting up fraudulent accounts — and is developing model-level safeguards designed to reduce the usefulness of outputs for illicit distillation without degrading the experience for legitimate customers.But the company acknowledged that \"no company can solve this alone,\" calling for coordinated action across the industry, cloud providers, and policymakers.The disclosure is likely to reverberate through multiple ongoing policy debates. In Congress, the bipartisan No DeepSeek on Government Devices Act has already been introduced. Federal agencies including NASA have banned DeepSeek from employee devices. And the broader question of chip export controls — which the Trump administration has been weighing amid competing pressures from Nvidia and national security hawks — now has a new and vivid data point.For the AI industry&#x27;s technical decision-makers, the implications are immediate and practical. If Anthropic&#x27;s account is accurate, the proxy infrastructure enabling these attacks is vast, sophisticated, and adaptable — and it is not limited to targeting a single company. Every frontier AI lab with an API is a potential target. The era of treating model access as a simple commercial transaction may be coming to an end, replaced by one in which API security is as strategically important as the model weights themselves.Anthropic has now put names, numbers, and forensic detail behind accusations that the industry had only whispered about for months. Whether that evidence galvanizes the coordinated response the company is calling for — or simply accelerates an arms race between distillers and defenders — may depend on a question no classifier can answer: whether Washington sees this as an act of espionage or just the cost of doing business in an era when intelligence itself has become a commodity.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/vgR8Wyc97pPTG90zTzQGX/9988675ce7156041adcf19bf19895fde/nuneybits_Vector_art_of_giant_syringe_siphoning_circuit-lines_i_60db17a8-17a3-4f0a-bece-ed1cfb79d116.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/falcon-northwest-fragbox-review-a-compact-gaming-rig-that-does-everything-right-130000837.html",
          "published_at": "Mon, 23 Feb 2026 18:51:26 +0000",
          "title": "Falcon Northwest FragBox review: A compact gaming rig that does everything right",
          "standfirst": "Mafia: The Old Country demands to be played on an enormous screen. As much as I love my 32-inch Alienware OLED gaming monitor, it doesn't do justice to Mafia's cinematic vistas of Sicily. But, I also wanted to play that game in its full 4K glory, with none of the compromises of today's game consoles. So why not just shove a tiny gaming desktop under my home theater? Enter the Fragbox, Falcon Northwest's revamped small form factor gaming PC. While it's very expensive, starting at $3,997, it's incredibly powerful and gives you the freedom to easily upgrade the hardware down the line. I know what you're thinking: \"A $4,000 desktop, in this economy?\" That pricing also doesn't include upgrading from the stock NVIDIA's RTX 5070 GPU, as well as adding more RAM and larger SSDs, all of which could drive the price up thousands more. I initially planned to review the FragBox back in early December 2025, before the AI-induced RAMaggedon made memory, storage and other components dramatically more expensive. Falcon Northwest is mainly known as a boutique and high-end system builder, so its wealthier clientele can likely weather the pricing storm. If you're looking for a deal, though, you won't find it here. So what, exactly, is a FragBox? Imagine a typical mid-tower desktop squashed down to a system that's only 10.2-inches tall, 10.5-inches wide and 15.9-inches deep. When Falcon initially debuted the FragBox in 2003, it was notable for being a genuinely small PC that used full-sized parts. That's still a main selling point today: It can still fit in large NVIDIA GPUs, including the beefy RTX 5090, as well as either Intel's latest Core Ultra chips or AMD's Ryzen 9000 CPUs. A huge 280mm radiator sits at the top pulling out hot air, and it also serves as an All-in-One (AIO) liquid cooler for the CPU. At 25 pounds, the FragBox isn't exactly light, but its sturdy metal handle makes it easy to move around. Most mid-tower desktops usually weigh between 20 and 35 pounds, depending on their case material. But they're also much larger and harder to squeeze into tight spaces. The FragBox's relatively squat size makes it easy to shove into a home entertainment center, or just sit on the corner of your desk. If you need a bit more height clearance, you can also remove the handle from the top panel. Just be sure there’s enough room for some airflow — all of that heat has to go somewhere, right? Falcon Northwest FragBox Devindra Hardawar for Engadget Despite its density, the FragBox's elegant design makes it a cinch to access to all of the system's components. Just unscrew the side and top panels and you can easily remove the GPU, RAM, storage and other major components. There are three slots of M.2 SSDs, as well as two locations for 2.5-inch drives and a spot for a large 3.5-inch HDD. The system is bundled with a 1,200W power supply, which should be more than enough to handle future GPUs and CPUs. Ports are plentiful as well: There are two USB-A and one USB-C connections right up front, alongside a headphone jack. On the rear, you've got your typical assortment of mid-tower connections, including four USB-A 2.0 connections, seven USB-A 3 ports, one 20G USB-C 3.2 port, 2.5G Ethernet, HDMI and DisplayPort. Our RTX 5090 review unit also included three DisplayPort jacks and one HDMI connection (which you'll see on most GPUs). Wi-Fi 6E was also built into our unit, but Falcon says that Wi-Fi 7 is now standard with new builds. Falcon Northwest FragBox Devindra Hardawar for Engadget The FragBox, thankfully, lacks the garish LEDs and cheesy thermal glass you find on more ostentatious gaming rigs. Falcon Northwest's aluminum case looks and feels stately, like an old-school luxury car. If you want something flashier, you can shell out an additional $400 for a custom UV printed case or $149 for a UV-printed front panel. Our review unit was equipped with AMD's Ryzen 9950X3D CPU, NVIDIA's RTX 5090, 96GB of DDR5 RAM and a 2TB SSD, which adds up to a whopping $7,995. Five months ago, it would have cost $7,047 —- you can thank the RAM shortage for the price jump. Even before benchmarking or running any games, I expected it to be a beast. In PCMark 10, the FragBox scored a whopping 13,810, which is around 500 points higher than my mid-tower system with the same CPU and GPU. It also scored the highest 3DMark Speedway and Port Royal ray tracing scores I've ever seen. Even more impressive, the FragBox's fans were barely audible under load, and the CPU and GPU sat at a chill 52C and 65C, respectively CPU GeekBench 6 CPU GeekBench 6 GPU Cinebench 2024 Falcon Northwest FragBox 3,445/22,787 390,148 N/A Desktop with AMD Ryzen 9 9950X3D, RTX 5090 3,366/18,950 381,400 134/2,124 Desktop with AMD Ryzen 9 7900X, RTX 5090 2,822/14,216 358,253 113/1,103 Apple Mac Studio M4 Max 4,090/26,394 116,028 190/2066 To get back to my initial point, it ran Mafia: The Old Country in 4K flawlessly, with every graphics setting cranked all the way up. While playing on my 120-inch projector home theater setup, the game reached 62 fps natively, and flipping on DLSS upscaling and frame generation bumped that up to 120 fps. Not that you need a super higher framerate for a slow-paced, mostly cinematic action game. I was just happy to be playing without any compromises — even the PS5 Pro can't reach the same level of graphical fidelity as the monstrously powerful RTX 5090. Falcon Northwest FragBox Devindra Hardawar for Engadget I'm no stranger to big-screen PC gaming, but previously I've had to run a laughably long HDMI cable from my desktop to make it work. I'm just too old for that mess now. And it also doesn't work consistently, especially at higher framerates, thanks to the massive bandwidth required to pump out 4K at high refresh rates. In-home game streaming is also an option, but that's not great when you're blowing games up to an enormous TV or projector screen. It's just too hard to ignore the imperfections of streaming compression. (Admittedly, I need to test newer high-bandwidth options, especially after I was impressed by NVIDIA's GeForce Now upgrade last year.) The FragBox also made it easy to jump into all of my recent Steam titles, including Mewgeneics and Arc Raiders on a big screen. Unfortunately, Windows itself remains a key stumbling block for home theater PC gaming. You'll still need to keep a keyboard and PC around to deal with the initial OS configuration. And even once I enabled Steam's Big Picture mode, which offers excellent controller options, I still occasionally had to deal with Windows Updates and other annoyances. Falcon Northwest FragBox Devindra Hardawar for Engadget Microsoft is currently trying to optimize Windows for gaming handhelds, and it's reportedly doing even more to make a future PC-powered Xbox feel more console-like. For now, though, using a Windows PC in your home theater doesn't feel much different than it did a decade ago. Steam is your savior, Windows is your enemy. Or you could just save thousands of dollars and buy a $500 PlayStation 5 or $700 PS5 Pro, instead. The latter will still get you smooth framerates and a healthy dose of ray tracing, without the annoyance of Windows, keyboards and mice. But if you just want a compact and insanely powerful gaming desktop, and you don't mind spending a premium, it's hard to deny that the FragBox gets everything right. Update 2/23, 1:48PM: Added updated information about Wi-Fi 7, handle removability and pricing.This article originally appeared on Engadget at https://www.engadget.com/computing/falcon-northwest-fragbox-review-a-compact-gaming-rig-that-does-everything-right-130000837.html?src=rss",
          "content": "Mafia: The Old Country demands to be played on an enormous screen. As much as I love my 32-inch Alienware OLED gaming monitor, it doesn't do justice to Mafia's cinematic vistas of Sicily. But, I also wanted to play that game in its full 4K glory, with none of the compromises of today's game consoles. So why not just shove a tiny gaming desktop under my home theater? Enter the Fragbox, Falcon Northwest's revamped small form factor gaming PC. While it's very expensive, starting at $3,997, it's incredibly powerful and gives you the freedom to easily upgrade the hardware down the line. I know what you're thinking: \"A $4,000 desktop, in this economy?\" That pricing also doesn't include upgrading from the stock NVIDIA's RTX 5070 GPU, as well as adding more RAM and larger SSDs, all of which could drive the price up thousands more. I initially planned to review the FragBox back in early December 2025, before the AI-induced RAMaggedon made memory, storage and other components dramatically more expensive. Falcon Northwest is mainly known as a boutique and high-end system builder, so its wealthier clientele can likely weather the pricing storm. If you're looking for a deal, though, you won't find it here. So what, exactly, is a FragBox? Imagine a typical mid-tower desktop squashed down to a system that's only 10.2-inches tall, 10.5-inches wide and 15.9-inches deep. When Falcon initially debuted the FragBox in 2003, it was notable for being a genuinely small PC that used full-sized parts. That's still a main selling point today: It can still fit in large NVIDIA GPUs, including the beefy RTX 5090, as well as either Intel's latest Core Ultra chips or AMD's Ryzen 9000 CPUs. A huge 280mm radiator sits at the top pulling out hot air, and it also serves as an All-in-One (AIO) liquid cooler for the CPU. At 25 pounds, the FragBox isn't exactly light, but its sturdy metal handle makes it easy to move around. Most mid-tower desktops usually weigh between 20 and 35 pounds, depending on their case material. But they're also much larger and harder to squeeze into tight spaces. The FragBox's relatively squat size makes it easy to shove into a home entertainment center, or just sit on the corner of your desk. If you need a bit more height clearance, you can also remove the handle from the top panel. Just be sure there’s enough room for some airflow — all of that heat has to go somewhere, right? Falcon Northwest FragBox Devindra Hardawar for Engadget Despite its density, the FragBox's elegant design makes it a cinch to access to all of the system's components. Just unscrew the side and top panels and you can easily remove the GPU, RAM, storage and other major components. There are three slots of M.2 SSDs, as well as two locations for 2.5-inch drives and a spot for a large 3.5-inch HDD. The system is bundled with a 1,200W power supply, which should be more than enough to handle future GPUs and CPUs. Ports are plentiful as well: There are two USB-A and one USB-C connections right up front, alongside a headphone jack. On the rear, you've got your typical assortment of mid-tower connections, including four USB-A 2.0 connections, seven USB-A 3 ports, one 20G USB-C 3.2 port, 2.5G Ethernet, HDMI and DisplayPort. Our RTX 5090 review unit also included three DisplayPort jacks and one HDMI connection (which you'll see on most GPUs). Wi-Fi 6E was also built into our unit, but Falcon says that Wi-Fi 7 is now standard with new builds. Falcon Northwest FragBox Devindra Hardawar for Engadget The FragBox, thankfully, lacks the garish LEDs and cheesy thermal glass you find on more ostentatious gaming rigs. Falcon Northwest's aluminum case looks and feels stately, like an old-school luxury car. If you want something flashier, you can shell out an additional $400 for a custom UV printed case or $149 for a UV-printed front panel. Our review unit was equipped with AMD's Ryzen 9950X3D CPU, NVIDIA's RTX 5090, 96GB of DDR5 RAM and a 2TB SSD, which adds up to a whopping $7,995. Five months ago, it would have cost $7,047 —- you can thank the RAM shortage for the price jump. Even before benchmarking or running any games, I expected it to be a beast. In PCMark 10, the FragBox scored a whopping 13,810, which is around 500 points higher than my mid-tower system with the same CPU and GPU. It also scored the highest 3DMark Speedway and Port Royal ray tracing scores I've ever seen. Even more impressive, the FragBox's fans were barely audible under load, and the CPU and GPU sat at a chill 52C and 65C, respectively CPU GeekBench 6 CPU GeekBench 6 GPU Cinebench 2024 Falcon Northwest FragBox 3,445/22,787 390,148 N/A Desktop with AMD Ryzen 9 9950X3D, RTX 5090 3,366/18,950 381,400 134/2,124 Desktop with AMD Ryzen 9 7900X, RTX 5090 2,822/14,216 358,253 113/1,103 Apple Mac Studio M4 Max 4,090/26,394 116,028 190/2066 To get back to my initial point, it ran Mafia: The Old Country in 4K flawlessly, with every graphics setting cranked all the way up. While playing on my 120-inch projector home theater setup, the game reached 62 fps natively, and flipping on DLSS upscaling and frame generation bumped that up to 120 fps. Not that you need a super higher framerate for a slow-paced, mostly cinematic action game. I was just happy to be playing without any compromises — even the PS5 Pro can't reach the same level of graphical fidelity as the monstrously powerful RTX 5090. Falcon Northwest FragBox Devindra Hardawar for Engadget I'm no stranger to big-screen PC gaming, but previously I've had to run a laughably long HDMI cable from my desktop to make it work. I'm just too old for that mess now. And it also doesn't work consistently, especially at higher framerates, thanks to the massive bandwidth required to pump out 4K at high refresh rates. In-home game streaming is also an option, but that's not great when you're blowing games up to an enormous TV or projector screen. It's just too hard to ignore the imperfections of streaming compression. (Admittedly, I need to test newer high-bandwidth options, especially after I was impressed by NVIDIA's GeForce Now upgrade last year.) The FragBox also made it easy to jump into all of my recent Steam titles, including Mewgeneics and Arc Raiders on a big screen. Unfortunately, Windows itself remains a key stumbling block for home theater PC gaming. You'll still need to keep a keyboard and PC around to deal with the initial OS configuration. And even once I enabled Steam's Big Picture mode, which offers excellent controller options, I still occasionally had to deal with Windows Updates and other annoyances. Falcon Northwest FragBox Devindra Hardawar for Engadget Microsoft is currently trying to optimize Windows for gaming handhelds, and it's reportedly doing even more to make a future PC-powered Xbox feel more console-like. For now, though, using a Windows PC in your home theater doesn't feel much different than it did a decade ago. Steam is your savior, Windows is your enemy. Or you could just save thousands of dollars and buy a $500 PlayStation 5 or $700 PS5 Pro, instead. The latter will still get you smooth framerates and a healthy dose of ray tracing, without the annoyance of Windows, keyboards and mice. But if you just want a compact and insanely powerful gaming desktop, and you don't mind spending a premium, it's hard to deny that the FragBox gets everything right. Update 2/23, 1:48PM: Added updated information about Wi-Fi 7, handle removability and pricing.This article originally appeared on Engadget at https://www.engadget.com/computing/falcon-northwest-fragbox-review-a-compact-gaming-rig-that-does-everything-right-130000837.html?src=rss",
          "feed_position": 8,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/falcon_northwest_fragbox_2.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/researchers-baked-3x-inference-speedups-directly-into-llm-weights-without",
          "published_at": "Mon, 23 Feb 2026 17:00:00 GMT",
          "title": "Researchers baked 3x inference speedups directly into LLM weights — without speculative decoding",
          "standfirst": "As agentic AI workflows multiply the cost and latency of long reasoning chains, a team from the University of Maryland, Lawrence Livermore National Labs, Columbia University and TogetherAI has found a way to bake 3x throughput gains directly into a model&#x27;s weights.Unlike speculative decoding, which requires a separate drafting model, this approach requires no additional infrastructure — just a single special token added to the model&#x27;s existing architecture.The limits of next-token predictionNext-token prediction — generating text one token per forward pass — creates a throughput ceiling that becomes painfully expensive when models need to produce thousands of tokens. This bottleneck is especially problematic in reasoning models, which frequently generate thousands of “chain of thought” tokens before producing the final response, leading to a slow and expensive user experience.Multi-token prediction (MTP) offers an alternative training paradigm that allows a language model to produce multiple tokens simultaneously in a single forward pass. For example, the model can be trained to predict a block of tokens all at once instead of just the immediate next token.John Kirchenbauer, a doctorate candidate in computer science at the University of Maryland and co-author of the paper, told VentureBeat that as we move toward agentic workflows, the focus is shifting from overall throughput to single-user speed. \"Today, with ultra-long thinking traces being the norm and agentic outer loops multiplying out those costs even further, latency is becoming as equally important a dimension of overall serving efficiency as gross tokens per second per hardware unit (tps/GPU),\" Kirchenbauer said. He said that while standard batched next-token prediction is already optimal for overall throughput, the new approach \"strive[s] to saturate the GPU with just a single user&#x27;s query to decrease latency for that single user.\"Other methods exist, but they come with drawbacks. \"It&#x27;s worth noting that speculative decoding, and diffusion LLMs as an efficiency focused alternative to next token prediction (NTP), are both latency focused acceleration techniques,\" Kirchenbauer said. But speculative decoding requires deploying and managing an auxiliary \"drafting\" model, which spends more absolute compute to draft and verify. MTP, on the other hand, \"leverages a similar sort of tradeoff, it&#x27;s just simpler to serve and scientifically interesting in its own right.\"Current MTP paradigms have limitations, however. The standard objective for training a language model for MTP involves comparing its predictions against ground-truth text from a dataset. The pitfall is that this standard training teaches the model to predict the probability of a token at a specific position independently, rather than caring about the joint relationship between a sequence of tokens.If a model tries to predict multiple tokens at once using this standard method, two major problems occur. The first is grammatical mismatch. For example, if a model predicts two words following the prefix \"The zookeeper fed the,\" it might sample independently and produce a mismatched phrase like \"panda meat\" or \"lion bamboo\" instead of \"panda bamboo\" and “lion meat.”The second issue is degenerate repetition. Because typical text is unpredictable, a model trying to predict a token 100 positions into the future against a standard dataset will just predict \"the,\" since it is the most common word in English. This results in the model outputting nonsense like \"...the the the...\" for far-future positions.Multi-token prediction via self-distillationTo solve the issues of generating multiple tokens, the researchers propose a novel training technique that uses a student-teacher scheme. A student model, which is the model learning to predict multiple tokens, generates a deterministic multi-token block. A teacher model, acting as a strong standard next-token prediction language model, evaluates that block. The teacher acts as a critic, calculating how likely and coherent the student&#x27;s proposed sequence is. If the student proposes a mismatched phrase like \"lion bamboo,\" the teacher assigns it a high loss, teaching the student to avoid that construction.The paradigm is inspired by on-policy reinforcement learning because the student model is not simply memorizing static text. It generates a full rollout (sequence of actions in RL parlance) instantly in parallel on a single forward pass and receives a reward based on how good the teacher thinks it is. Unlike static supervised methods where training pairs are fixed in advance, the feedback here is dynamic, generated from the student&#x27;s own outputs in real time. The strong teacher also verifies the coherence of the tokens, which prevents the student model from learning degenerate outputs like repeated words.For developers, the beauty of this approach lies in its simplicity. \"There are truly no modifications to the architecture except for the addition of a special token,\" Kirchenbauer said. By co-opting an unused slot in a model&#x27;s existing embedding matrix to act as an <MTP> mask token, the technique converts sequential operations into parallel ones. \"Any standard next token prediction language model can be adapted in this way... the internal implementation — MoE, windowed attention, SSM layers, etc. — are left untouched and present no barrier to adaptation.\"For engineering teams, this means the adaptation can be applied to models already in production without rebuilding pipelines. Generating multiple tokens at the same time can still hurt the accuracy of the response at inference time. To maximize generation speed without sacrificing the quality of the output, the authors introduce an adaptive decoding strategy called ConfAdapt.ConfAdapt evaluates a confidence threshold, such as 90%, at each step. The model generates a block of tokens, but it only keeps the tokens that meet or exceed this high-confidence threshold. When the upcoming text is highly predictable or structural, the model&#x27;s confidence is very high. It will accept and output a large chunk of tokens all at once, saving significant computational time on easy tokens. It then focuses its costly single-token passes on harder tokens that require more computational effort.Putting multi-token prediction to the testTo see how the training paradigm performed in practice, the researchers applied their method to popular open-weight instruction-tuned models. They tested the strong general-purpose model Llama-3.1-8B-Magpie and the smaller, efficient Qwen3-4B-Instruct-2507, which is often chosen for cost-sensitive enterprise deployments. Both models were tuned on MetaMathQA, a dataset of synthetic grade school math problems that rely heavily on reasoning traces.The experiments revealed a clear sweet spot between speed and accuracy. Using the ConfAdapt strategy, the Llama-3.1-8B model achieved a 3x speedup with less than a 3% drop in accuracy on math benchmarks. The Qwen3-4B model achieved the same 3x speedup with a slightly higher 7% drop in accuracy. More aggressive settings could hit 5x speedups, though they came with steeper accuracy penalties.How this translates to real-world tasks depends on predictability. \"As the ConfAdapt approach naturally tailors the acceleration to the inherent entropy in the domain, when the model &#x27;knows&#x27; exactly what comes next it can emit it in a single pass,\" he noted, leading to massive acceleration on predictable tasks, while using more steps for uncertain outputs.The speedups also transferred across domains that were not included in the multi-token prediction training phase. This included tasks within the same domain as the training data, like math and reasoning, as well as open-ended tasks such as creative writing and summarization.Despite this transfer learning, enterprises deploying these models for specialized tasks shouldn&#x27;t rely on it entirely. \"Our recommendation would be to tune/adapt the model for MTP using samples from the special industrial domain,\" Kirchenbauer said. \"The best performance is likely achieved if the MTP adaptation is performed using prompts from the deployment domain.\"Serving compatibility and the road aheadThe research team released their trained models on Hugging Face and will soon release the code for their MTP framework. Infrastructure teams integrating these models into vLLM or SGLang will need to account for changes in how batching and KV caching are handled — but that&#x27;s a one-time engineering investment, not an ongoing burden. However, Kirchenbauer sees \"no clear barriers to integration\" and confirmed the team is \"working with some systems experts to identify the shortest path to integration.\"Kirchenbauer&#x27;s advice for teams wanting to test the released models: start with toy prompts like counting or repeating a phrase to see ConfAdapt&#x27;s gains in action, then adapt the model using samples from your specific deployment domain for best results. \"Overall we do expect that a production-ready implementation of our approach could simplify the lifecycle of building and deploying low-latency agentic models,\" Kirchenbauer concluded. \"While existing acceleration techniques for NTP models focus almost solely on inference harnesses and logic, our approach just bakes some of the complexity into the model itself making it largely complementary to existing work.\"",
          "content": "As agentic AI workflows multiply the cost and latency of long reasoning chains, a team from the University of Maryland, Lawrence Livermore National Labs, Columbia University and TogetherAI has found a way to bake 3x throughput gains directly into a model&#x27;s weights.Unlike speculative decoding, which requires a separate drafting model, this approach requires no additional infrastructure — just a single special token added to the model&#x27;s existing architecture.The limits of next-token predictionNext-token prediction — generating text one token per forward pass — creates a throughput ceiling that becomes painfully expensive when models need to produce thousands of tokens. This bottleneck is especially problematic in reasoning models, which frequently generate thousands of “chain of thought” tokens before producing the final response, leading to a slow and expensive user experience.Multi-token prediction (MTP) offers an alternative training paradigm that allows a language model to produce multiple tokens simultaneously in a single forward pass. For example, the model can be trained to predict a block of tokens all at once instead of just the immediate next token.John Kirchenbauer, a doctorate candidate in computer science at the University of Maryland and co-author of the paper, told VentureBeat that as we move toward agentic workflows, the focus is shifting from overall throughput to single-user speed. \"Today, with ultra-long thinking traces being the norm and agentic outer loops multiplying out those costs even further, latency is becoming as equally important a dimension of overall serving efficiency as gross tokens per second per hardware unit (tps/GPU),\" Kirchenbauer said. He said that while standard batched next-token prediction is already optimal for overall throughput, the new approach \"strive[s] to saturate the GPU with just a single user&#x27;s query to decrease latency for that single user.\"Other methods exist, but they come with drawbacks. \"It&#x27;s worth noting that speculative decoding, and diffusion LLMs as an efficiency focused alternative to next token prediction (NTP), are both latency focused acceleration techniques,\" Kirchenbauer said. But speculative decoding requires deploying and managing an auxiliary \"drafting\" model, which spends more absolute compute to draft and verify. MTP, on the other hand, \"leverages a similar sort of tradeoff, it&#x27;s just simpler to serve and scientifically interesting in its own right.\"Current MTP paradigms have limitations, however. The standard objective for training a language model for MTP involves comparing its predictions against ground-truth text from a dataset. The pitfall is that this standard training teaches the model to predict the probability of a token at a specific position independently, rather than caring about the joint relationship between a sequence of tokens.If a model tries to predict multiple tokens at once using this standard method, two major problems occur. The first is grammatical mismatch. For example, if a model predicts two words following the prefix \"The zookeeper fed the,\" it might sample independently and produce a mismatched phrase like \"panda meat\" or \"lion bamboo\" instead of \"panda bamboo\" and “lion meat.”The second issue is degenerate repetition. Because typical text is unpredictable, a model trying to predict a token 100 positions into the future against a standard dataset will just predict \"the,\" since it is the most common word in English. This results in the model outputting nonsense like \"...the the the...\" for far-future positions.Multi-token prediction via self-distillationTo solve the issues of generating multiple tokens, the researchers propose a novel training technique that uses a student-teacher scheme. A student model, which is the model learning to predict multiple tokens, generates a deterministic multi-token block. A teacher model, acting as a strong standard next-token prediction language model, evaluates that block. The teacher acts as a critic, calculating how likely and coherent the student&#x27;s proposed sequence is. If the student proposes a mismatched phrase like \"lion bamboo,\" the teacher assigns it a high loss, teaching the student to avoid that construction.The paradigm is inspired by on-policy reinforcement learning because the student model is not simply memorizing static text. It generates a full rollout (sequence of actions in RL parlance) instantly in parallel on a single forward pass and receives a reward based on how good the teacher thinks it is. Unlike static supervised methods where training pairs are fixed in advance, the feedback here is dynamic, generated from the student&#x27;s own outputs in real time. The strong teacher also verifies the coherence of the tokens, which prevents the student model from learning degenerate outputs like repeated words.For developers, the beauty of this approach lies in its simplicity. \"There are truly no modifications to the architecture except for the addition of a special token,\" Kirchenbauer said. By co-opting an unused slot in a model&#x27;s existing embedding matrix to act as an <MTP> mask token, the technique converts sequential operations into parallel ones. \"Any standard next token prediction language model can be adapted in this way... the internal implementation — MoE, windowed attention, SSM layers, etc. — are left untouched and present no barrier to adaptation.\"For engineering teams, this means the adaptation can be applied to models already in production without rebuilding pipelines. Generating multiple tokens at the same time can still hurt the accuracy of the response at inference time. To maximize generation speed without sacrificing the quality of the output, the authors introduce an adaptive decoding strategy called ConfAdapt.ConfAdapt evaluates a confidence threshold, such as 90%, at each step. The model generates a block of tokens, but it only keeps the tokens that meet or exceed this high-confidence threshold. When the upcoming text is highly predictable or structural, the model&#x27;s confidence is very high. It will accept and output a large chunk of tokens all at once, saving significant computational time on easy tokens. It then focuses its costly single-token passes on harder tokens that require more computational effort.Putting multi-token prediction to the testTo see how the training paradigm performed in practice, the researchers applied their method to popular open-weight instruction-tuned models. They tested the strong general-purpose model Llama-3.1-8B-Magpie and the smaller, efficient Qwen3-4B-Instruct-2507, which is often chosen for cost-sensitive enterprise deployments. Both models were tuned on MetaMathQA, a dataset of synthetic grade school math problems that rely heavily on reasoning traces.The experiments revealed a clear sweet spot between speed and accuracy. Using the ConfAdapt strategy, the Llama-3.1-8B model achieved a 3x speedup with less than a 3% drop in accuracy on math benchmarks. The Qwen3-4B model achieved the same 3x speedup with a slightly higher 7% drop in accuracy. More aggressive settings could hit 5x speedups, though they came with steeper accuracy penalties.How this translates to real-world tasks depends on predictability. \"As the ConfAdapt approach naturally tailors the acceleration to the inherent entropy in the domain, when the model &#x27;knows&#x27; exactly what comes next it can emit it in a single pass,\" he noted, leading to massive acceleration on predictable tasks, while using more steps for uncertain outputs.The speedups also transferred across domains that were not included in the multi-token prediction training phase. This included tasks within the same domain as the training data, like math and reasoning, as well as open-ended tasks such as creative writing and summarization.Despite this transfer learning, enterprises deploying these models for specialized tasks shouldn&#x27;t rely on it entirely. \"Our recommendation would be to tune/adapt the model for MTP using samples from the special industrial domain,\" Kirchenbauer said. \"The best performance is likely achieved if the MTP adaptation is performed using prompts from the deployment domain.\"Serving compatibility and the road aheadThe research team released their trained models on Hugging Face and will soon release the code for their MTP framework. Infrastructure teams integrating these models into vLLM or SGLang will need to account for changes in how batching and KV caching are handled — but that&#x27;s a one-time engineering investment, not an ongoing burden. However, Kirchenbauer sees \"no clear barriers to integration\" and confirmed the team is \"working with some systems experts to identify the shortest path to integration.\"Kirchenbauer&#x27;s advice for teams wanting to test the released models: start with toy prompts like counting or repeating a phrase to see ConfAdapt&#x27;s gains in action, then adapt the model using samples from your specific deployment domain for best results. \"Overall we do expect that a production-ready implementation of our approach could simplify the lifecycle of building and deploying low-latency agentic models,\" Kirchenbauer concluded. \"While existing acceleration techniques for NTP models focus almost solely on inference harnesses and logic, our approach just bakes some of the complexity into the model itself making it largely complementary to existing work.\"",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7lB8UaMsO4LhTkCf5Tf7Yf/40524ebbaf93e89ba37322f9753fdd03/multi-token_prediction.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/anthropic-claude-code-security-reasoning-vulnerability-hunting",
          "published_at": "Mon, 23 Feb 2026 15:44:00 GMT",
          "title": "Anthropic's Claude Code Security is available now after finding 500+ vulnerabilities: how security leaders should respond",
          "standfirst": "Anthropic pointed its most advanced AI model, Claude Opus 4.6, at production open-source codebases and found a plethora of security holes: more than 500 high-severity vulnerabilities that had survived decades of expert review and millions of hours of fuzzing, with each candidate vetted through internal and external security review before disclosure. Fifteen days later, the company productized the capability and launched Claude Code Security.Security directors responsible for seven-figure vulnerability management stacks should expect a common question from their boards in the next review cycle. VentureBeat anticipates the emails and conversations will start with, \"How do we add reasoning-based scanning before attackers get there first?\", because as Anthropic&#x27;s review found, simply pointing an AI model at exposed code can be enough to identify — and in the case of malicious actors, exploit — security lapses in production code. The answer matters more than the number, and it is primarily structural: how your tooling and processes allocate work between pattern-based scanners and reasoning-based analysis. CodeQL and the tools built on it match code against known patterns. Claude Code Security, which Anthropic launched February 20 as a limited research preview, reasons about code the way a human security researcher would. It follows how data moves through an application and catches flaws in business logic and access control that no rule set covers.The board conversation security leaders need to have this weekFive hundred newly discovered zero-days is less a scare statistic than a standing budget justification for rethinking how you fund code security. The reasoning capability Claude Code Security represents, and its inevitable competitors, need to drive the procurement conversation. Static application security testing (SAST) catches known vulnerability classes. Reasoning-based scanners find what pattern-matching was never designed to detect. Both have a role.Anthropic published the zero-day research on February 5. Fifteen days later, they shipped the product. While it&#x27;s the same model and capabilities, it is now available to Enterprise and Team customers.What Claude does that CodeQL couldn&#x27;tGitHub has offered CodeQL-based scanning through Advanced Security for years, and added Copilot Autofix in August 2024 to generate LLM-suggested fixes for alerts. Security teams rely on it. But the detection boundary is the CodeQL rule set, and everything outside that boundary stays invisible.Claude Code Security extends that boundary by generating and testing its own hypotheses about how data and control flow through an application, including cases where no existing rule set describes. CodeQL solves the problem it was built to solve: data-flow analysis within predefined queries. It tells you whether tainted input reaches a dangerous function.CodeQL is not designed to autonomously read a project&#x27;s commit history, infer an incomplete patch, trace that logic into another file, and then assemble a working proof-of-concept exploit end to end. Claude did exactly that on GhostScript, OpenSC, and CGIF, each time using a different reasoning strategy.\"The real shift is from pattern-matching to hypothesis generation,\" said Merritt Baer, CSO at Enkrypt AI, advisor to Andesite and AppOmni, and former Deputy CISO at AWS, in an exclusive interview with VentureBeat. \"That&#x27;s a step-function increase in discovery power, and it demands equally strong human and technical controls.\"Three proof points from Anthropic&#x27;s published methodology show where pattern-matching ends and hypothesis generation begins.Commit history analysis across files. GhostScript is a widely deployed utility for processing PostScript and PDF files. Fuzzing turned up nothing, and neither did manual analysis. Then Claude pulled the Git commit history, found a patch that added stack bounds checking for font handling in gstype1.c, and reversed the logic: if the fix was needed there, every other call to that function without the fix was still vulnerable. In gdevpsfx.c, a completely different file, the call to the same function lacked the bounds checking patched elsewhere. Claude built a working proof-of-concept crash. No CodeQL rule describes that bug today. The maintainers have since patched it.Reasoning about preconditions that fuzzers can&#x27;t reach. OpenSC processes smart card data. Standard approaches failed here, too, so Claude searched the repository for function calls that are frequently vulnerable and found a location where multiple strcat operations ran in succession without length checking on the output buffer. Fuzzers rarely reached that code path because too many preconditions stood in the way. Claude reasoned about which code fragments looked interesting, constructed a buffer overflow, and proved the vulnerability.Algorithm-level edge cases that no coverage metric catches. CGIF is a library for processing GIF files. This vulnerability required understanding how LZW compression builds a dictionary of tokens. CGIF assumed compressed output would always be smaller than uncompressed input, which is almost always true. Claude recognized that if the LZW dictionary filled up and triggered resets, the compressed output could exceed the uncompressed size, overflowing the buffer. Even 100% branch coverage wouldn&#x27;t catch this. The flaw demands a particular sequence of operations that exercises an edge case in the compression algorithm itself. Random input generation almost never produces it. Claude did.Baer sees something broader in that progression. \"The challenge with reasoning isn&#x27;t accuracy, it&#x27;s agency,\" she told VentureBeat. \"Once a system can form hypotheses and pursue them, you&#x27;ve shifted from a lookup tool to something that can explore your environment in ways that are harder to predict and constrain.\"How Anthropic validated 500+ findingsAnthropic placed Claude inside a sandboxed virtual machine with standard utilities and vulnerability analysis tools. The red team didn&#x27;t provide any specialized instructions, custom harnesses, or task-specific prompting. Just the model and the code.The red team focused on memory corruption vulnerabilities because they&#x27;re the easiest to confirm objectively. Crash monitoring and address sanitizers don&#x27;t leave room for debate. Claude filtered its own output, deduplicating and reprioritizing before human researchers touched anything. When the confirmed count kept climbing, Anthropic brought in external security professionals to validate findings and write patches.Every target was an open-source project underpinning enterprise systems and critical infrastructure. Small teams maintain many of them, staffed by volunteers, not security professionals. When a vulnerability sits in one of these projects for a decade, every product that pulls from it inherits the risk.Anthropic didn&#x27;t start with the product launch. The defensive research spans more than a year. The company entered Claude in competitive Capture-the-Flag events where it ranked in the top 3% of PicoCTF globally, solved 19 of 20 challenges in the HackTheBox AI vs Human CTF, and placed 6th out of 9 teams defending live networks against human red team attacks at Western Regional CCDC. Anthropic also partnered with Pacific Northwest National Laboratory to test Claude against a simulated water treatment plant. PNNL&#x27;s researchers estimated that the model completed adversary emulation in three hours. The traditional process takes multiple weeks.The dual-use question security leaders can&#x27;t avoidThe same reasoning that finds a vulnerability can help an attacker exploit one. Frontier Red Team leader Logan Graham acknowledged this directly to Fortune&#x27;s Sharon Goldman. He told Fortune the models can now explore codebases autonomously and follow investigative leads faster than a junior security researcher.Gabby Curtis, Anthropic&#x27;s communications lead, told VentureBeat in an exclusive interview the company built Claude Code Security to make defensive capabilities more widely available, \"tipping the scales towards defenders.\" She was equally direct about the tension: \"The same reasoning that helps Claude find and fix a vulnerability could help an attacker exploit it, so we&#x27;re being deliberate about how we release this.\"In interviews with more than 40 CISOs across industries, VentureBeat found that formal governance frameworks for reasoning-based scanning tools are the exception, not the norm. The most common responses are that the area was considered so nascent that many CISOs didn&#x27;t think this capability would arrive so early in 2026.The question every security director has to answer before deploying this: if I give my team a tool that finds zero-days through reasoning, have I unintentionally expanded my internal threat surface?\"You didn&#x27;t weaponize your internal surface, you revealed it,\" Baer told VentureBeat. \"These tools can be helpful, but they also may surface latent risk faster and more scalably. The same tool that finds zero-days for defense can expose gaps in your threat model. Keep in mind that most intrusions don&#x27;t come from zero-days, they come from misconfigurations.\"\"In addition to the access and attack path risk, there is IP risk,\" she said. \"Not just exfiltration, but transformation. Reasoning models can internalize and re-express proprietary insights in ways that blur the line between use and leakage.\"The release is deliberately constrained. Enterprise and Team customers only, through a limited research preview. Open-source maintainers apply for free expedited access. Findings go through multi-stage self-verification before reaching an analyst, with severity ratings and confidence scores attached. Every patch requires human approval.Anthropic also built detection into the model itself. In a blog post detailing the safeguards, the company described deploying probes that measure activations within the model as it generates responses, with new cyber-specific probes designed to track potential misuse. On the enforcement side, Anthropic is expanding its response capabilities to include real-time intervention, including blocking traffic it detects as malicious.Graham was direct with Axios: the models are extremely good at finding vulnerabilities, and he expects them to get much better still. VentureBeat asked Anthropic for the false-positive rate before and after self-verification, the number of disclosed vulnerabilities with patches landed versus still in triage, and the specific safeguards that distinguish attacker use from defender use. The lead researcher on the 500-vulnerability project was unavailable, and the company declined to share specific attacker-detection mechanisms to avoid tipping off threat actors.\"Offense and defense are converging in capability,\" Baer said. \"The differentiator is oversight. If you can&#x27;t audit and bound how the tool is used, you&#x27;ve created another risk.\"That speed advantage doesn&#x27;t favor defenders by default. It favors whoever adopts it first. Security directors who move early set the terms.Anthropic isn&#x27;t alone. The pattern is repeating.Security researcher Sean Heelan used OpenAI&#x27;s o3 model with no custom tooling and no agentic framework to discover CVE-2025-37899, a previously unknown use-after-free vulnerability in the Linux kernel&#x27;s SMB implementation. The model analyzed over 12,000 lines of code and identified a race condition that traditional static analysis tools consistently missed because detecting it requires understanding concurrent thread interactions across connections.Separately, AI security startup AISLE discovered all 12 zero-day vulnerabilities announced in OpenSSL&#x27;s January 2026 security patch, including a rare high-severity finding (CVE-2025-15467, a stack buffer overflow in CMS message parsing that is potentially remotely exploitable without valid key material). AISLE co-founder and chief scientist Stanislav Fort reported that his team&#x27;s AI system accounted for 13 of the 14 total OpenSSL CVEs assigned in 2025. OpenSSL is among the most scrutinized cryptographic libraries on the planet. Fuzzers have run against it for years. The AI found what they were not designed to find.The window is already openThose 500 vulnerabilities live in open-source projects that enterprise applications depend on. Anthropic is disclosing and patching, but the window between discovery and adoption of those patches is where attackers operate today.The same model improvements behind Claude Code Security are available to anyone with API access.If your team is evaluating these capabilities, the limited research preview is the right place to start, with clearly defined data handling rules, audit logging, and success criteria agreed up front.",
          "content": "Anthropic pointed its most advanced AI model, Claude Opus 4.6, at production open-source codebases and found a plethora of security holes: more than 500 high-severity vulnerabilities that had survived decades of expert review and millions of hours of fuzzing, with each candidate vetted through internal and external security review before disclosure. Fifteen days later, the company productized the capability and launched Claude Code Security.Security directors responsible for seven-figure vulnerability management stacks should expect a common question from their boards in the next review cycle. VentureBeat anticipates the emails and conversations will start with, \"How do we add reasoning-based scanning before attackers get there first?\", because as Anthropic&#x27;s review found, simply pointing an AI model at exposed code can be enough to identify — and in the case of malicious actors, exploit — security lapses in production code. The answer matters more than the number, and it is primarily structural: how your tooling and processes allocate work between pattern-based scanners and reasoning-based analysis. CodeQL and the tools built on it match code against known patterns. Claude Code Security, which Anthropic launched February 20 as a limited research preview, reasons about code the way a human security researcher would. It follows how data moves through an application and catches flaws in business logic and access control that no rule set covers.The board conversation security leaders need to have this weekFive hundred newly discovered zero-days is less a scare statistic than a standing budget justification for rethinking how you fund code security. The reasoning capability Claude Code Security represents, and its inevitable competitors, need to drive the procurement conversation. Static application security testing (SAST) catches known vulnerability classes. Reasoning-based scanners find what pattern-matching was never designed to detect. Both have a role.Anthropic published the zero-day research on February 5. Fifteen days later, they shipped the product. While it&#x27;s the same model and capabilities, it is now available to Enterprise and Team customers.What Claude does that CodeQL couldn&#x27;tGitHub has offered CodeQL-based scanning through Advanced Security for years, and added Copilot Autofix in August 2024 to generate LLM-suggested fixes for alerts. Security teams rely on it. But the detection boundary is the CodeQL rule set, and everything outside that boundary stays invisible.Claude Code Security extends that boundary by generating and testing its own hypotheses about how data and control flow through an application, including cases where no existing rule set describes. CodeQL solves the problem it was built to solve: data-flow analysis within predefined queries. It tells you whether tainted input reaches a dangerous function.CodeQL is not designed to autonomously read a project&#x27;s commit history, infer an incomplete patch, trace that logic into another file, and then assemble a working proof-of-concept exploit end to end. Claude did exactly that on GhostScript, OpenSC, and CGIF, each time using a different reasoning strategy.\"The real shift is from pattern-matching to hypothesis generation,\" said Merritt Baer, CSO at Enkrypt AI, advisor to Andesite and AppOmni, and former Deputy CISO at AWS, in an exclusive interview with VentureBeat. \"That&#x27;s a step-function increase in discovery power, and it demands equally strong human and technical controls.\"Three proof points from Anthropic&#x27;s published methodology show where pattern-matching ends and hypothesis generation begins.Commit history analysis across files. GhostScript is a widely deployed utility for processing PostScript and PDF files. Fuzzing turned up nothing, and neither did manual analysis. Then Claude pulled the Git commit history, found a patch that added stack bounds checking for font handling in gstype1.c, and reversed the logic: if the fix was needed there, every other call to that function without the fix was still vulnerable. In gdevpsfx.c, a completely different file, the call to the same function lacked the bounds checking patched elsewhere. Claude built a working proof-of-concept crash. No CodeQL rule describes that bug today. The maintainers have since patched it.Reasoning about preconditions that fuzzers can&#x27;t reach. OpenSC processes smart card data. Standard approaches failed here, too, so Claude searched the repository for function calls that are frequently vulnerable and found a location where multiple strcat operations ran in succession without length checking on the output buffer. Fuzzers rarely reached that code path because too many preconditions stood in the way. Claude reasoned about which code fragments looked interesting, constructed a buffer overflow, and proved the vulnerability.Algorithm-level edge cases that no coverage metric catches. CGIF is a library for processing GIF files. This vulnerability required understanding how LZW compression builds a dictionary of tokens. CGIF assumed compressed output would always be smaller than uncompressed input, which is almost always true. Claude recognized that if the LZW dictionary filled up and triggered resets, the compressed output could exceed the uncompressed size, overflowing the buffer. Even 100% branch coverage wouldn&#x27;t catch this. The flaw demands a particular sequence of operations that exercises an edge case in the compression algorithm itself. Random input generation almost never produces it. Claude did.Baer sees something broader in that progression. \"The challenge with reasoning isn&#x27;t accuracy, it&#x27;s agency,\" she told VentureBeat. \"Once a system can form hypotheses and pursue them, you&#x27;ve shifted from a lookup tool to something that can explore your environment in ways that are harder to predict and constrain.\"How Anthropic validated 500+ findingsAnthropic placed Claude inside a sandboxed virtual machine with standard utilities and vulnerability analysis tools. The red team didn&#x27;t provide any specialized instructions, custom harnesses, or task-specific prompting. Just the model and the code.The red team focused on memory corruption vulnerabilities because they&#x27;re the easiest to confirm objectively. Crash monitoring and address sanitizers don&#x27;t leave room for debate. Claude filtered its own output, deduplicating and reprioritizing before human researchers touched anything. When the confirmed count kept climbing, Anthropic brought in external security professionals to validate findings and write patches.Every target was an open-source project underpinning enterprise systems and critical infrastructure. Small teams maintain many of them, staffed by volunteers, not security professionals. When a vulnerability sits in one of these projects for a decade, every product that pulls from it inherits the risk.Anthropic didn&#x27;t start with the product launch. The defensive research spans more than a year. The company entered Claude in competitive Capture-the-Flag events where it ranked in the top 3% of PicoCTF globally, solved 19 of 20 challenges in the HackTheBox AI vs Human CTF, and placed 6th out of 9 teams defending live networks against human red team attacks at Western Regional CCDC. Anthropic also partnered with Pacific Northwest National Laboratory to test Claude against a simulated water treatment plant. PNNL&#x27;s researchers estimated that the model completed adversary emulation in three hours. The traditional process takes multiple weeks.The dual-use question security leaders can&#x27;t avoidThe same reasoning that finds a vulnerability can help an attacker exploit one. Frontier Red Team leader Logan Graham acknowledged this directly to Fortune&#x27;s Sharon Goldman. He told Fortune the models can now explore codebases autonomously and follow investigative leads faster than a junior security researcher.Gabby Curtis, Anthropic&#x27;s communications lead, told VentureBeat in an exclusive interview the company built Claude Code Security to make defensive capabilities more widely available, \"tipping the scales towards defenders.\" She was equally direct about the tension: \"The same reasoning that helps Claude find and fix a vulnerability could help an attacker exploit it, so we&#x27;re being deliberate about how we release this.\"In interviews with more than 40 CISOs across industries, VentureBeat found that formal governance frameworks for reasoning-based scanning tools are the exception, not the norm. The most common responses are that the area was considered so nascent that many CISOs didn&#x27;t think this capability would arrive so early in 2026.The question every security director has to answer before deploying this: if I give my team a tool that finds zero-days through reasoning, have I unintentionally expanded my internal threat surface?\"You didn&#x27;t weaponize your internal surface, you revealed it,\" Baer told VentureBeat. \"These tools can be helpful, but they also may surface latent risk faster and more scalably. The same tool that finds zero-days for defense can expose gaps in your threat model. Keep in mind that most intrusions don&#x27;t come from zero-days, they come from misconfigurations.\"\"In addition to the access and attack path risk, there is IP risk,\" she said. \"Not just exfiltration, but transformation. Reasoning models can internalize and re-express proprietary insights in ways that blur the line between use and leakage.\"The release is deliberately constrained. Enterprise and Team customers only, through a limited research preview. Open-source maintainers apply for free expedited access. Findings go through multi-stage self-verification before reaching an analyst, with severity ratings and confidence scores attached. Every patch requires human approval.Anthropic also built detection into the model itself. In a blog post detailing the safeguards, the company described deploying probes that measure activations within the model as it generates responses, with new cyber-specific probes designed to track potential misuse. On the enforcement side, Anthropic is expanding its response capabilities to include real-time intervention, including blocking traffic it detects as malicious.Graham was direct with Axios: the models are extremely good at finding vulnerabilities, and he expects them to get much better still. VentureBeat asked Anthropic for the false-positive rate before and after self-verification, the number of disclosed vulnerabilities with patches landed versus still in triage, and the specific safeguards that distinguish attacker use from defender use. The lead researcher on the 500-vulnerability project was unavailable, and the company declined to share specific attacker-detection mechanisms to avoid tipping off threat actors.\"Offense and defense are converging in capability,\" Baer said. \"The differentiator is oversight. If you can&#x27;t audit and bound how the tool is used, you&#x27;ve created another risk.\"That speed advantage doesn&#x27;t favor defenders by default. It favors whoever adopts it first. Security directors who move early set the terms.Anthropic isn&#x27;t alone. The pattern is repeating.Security researcher Sean Heelan used OpenAI&#x27;s o3 model with no custom tooling and no agentic framework to discover CVE-2025-37899, a previously unknown use-after-free vulnerability in the Linux kernel&#x27;s SMB implementation. The model analyzed over 12,000 lines of code and identified a race condition that traditional static analysis tools consistently missed because detecting it requires understanding concurrent thread interactions across connections.Separately, AI security startup AISLE discovered all 12 zero-day vulnerabilities announced in OpenSSL&#x27;s January 2026 security patch, including a rare high-severity finding (CVE-2025-15467, a stack buffer overflow in CMS message parsing that is potentially remotely exploitable without valid key material). AISLE co-founder and chief scientist Stanislav Fort reported that his team&#x27;s AI system accounted for 13 of the 14 total OpenSSL CVEs assigned in 2025. OpenSSL is among the most scrutinized cryptographic libraries on the planet. Fuzzers have run against it for years. The AI found what they were not designed to find.The window is already openThose 500 vulnerabilities live in open-source projects that enterprise applications depend on. Anthropic is disclosing and patching, but the window between discovery and adoption of those patches is where attackers operate today.The same model improvements behind Claude Code Security are available to anyone with API access.If your team is evaluating these capabilities, the limited research preview is the right place to start, with clearly defined data handling rules, audit logging, and success criteria agreed up front.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4yR2yRPMyVDENsc3LePH0g/470f944f258db5254dc0ee056e52b1c9/hero_anthropic_story.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/best-affordable-windows-laptops-123000512.html",
          "published_at": "Mon, 23 Feb 2026 10:01:25 +0000",
          "title": "The best cheap Windows laptops for 2026",
          "standfirst": "You don’t need to spend a fortune to get a capable Windows laptop. For everyday tasks like web browsing, writing documents, streaming video or handling schoolwork, a well-chosen budget machine can still deliver a smooth, reliable experience. The challenge is cutting through the noise to find affordable options that balance performance, build quality and battery life without serious compromises.For many buyers, timing is no longer optional. With Windows 10 support now officially over, upgrading has become a necessity rather than a nice-to-have. The picks below focus on cheap Windows laptops that can handle day-to-day workloads comfortably while keeping you current on software and security updates. If you’re open to spending more for extra power or premium features, our broader guide to the best Windows laptops covers higher-end alternatives as well. What to look for in a budget-friendly Windows laptop While you can do a lot even when spending little on a Windows laptop, you must set your expectations accordingly. The biggest downside when purchasing a budget laptop (of any kind, really) is limited power. You’ll want to carefully consider a few specs, the most important among them being the processor (CPU). Many Windows laptops under $500 run on Intel Celeron or Pentium chipsets, but you can find some with Core i3/i5 and AMD Ryzen 3/5 CPUs at the higher end of the price spectrum. We recommend getting the most powerful CPU you can afford because it will dictate how fast the computer will feel overall. Memory (RAM) is also important because, the more you have, the easier it will be for the laptop to manage things like a dozen browser tabs while you edit a Word document and stream music in the background. When it comes to storage, consider how much you want to save locally. If you primarily work in Google Docs or save most things in the cloud, you may not need a machine with a ton of onboard storage. Just remember that your digital space will also be taken up by apps, so it may be worth getting a little extra storage than you think you need if you know you’ll be downloading big programs. A final side note: solid state drives (SSDs) are ubiquitous at this point, not to mention faster and more efficient than hard drives (HDDs), so we recommend getting a laptop with that type of storage. As for screens, there’s a healthy mix of HD (720p resolution) and FHD (1080p) options in this price range and we recommend springing for a notebook with a 1080p display if you can. Touchscreens aren’t as common in the budget space as standard panels, but you’ll only really miss one if you get a 2-in-1 laptop. Before we get to our recommended specs for a cheap Windows laptop, it’s worth mentioning that Microsoft clearly lays out the true minimum requirements for any Windows 11 machine. Those include a 1GHz or faster processor that includes two or more cores, at least 4GB of RAM and 64GB of available storage space. That’s the bare minimum to run Windows 11; we recommend giving yourself some wiggle room by choosing a machine that will perform well now and for years to come. Specs to look for in an affordable Windows laptop CPU: Intel Core i3 or AMD Ryzen 3 processors, at minimum RAM: At least 8GB Storage: At least 128GB SSD Screen: At least 1080p FHD It’s essential to prioritize what’s important to you. But at the lower end of the budget, a good laptop may not offer everything you need, whereas a great one might. Although most machines come with features like Bluetooth, built-in Wi-Fi and additional ports, you might find not all of them come with the specifics you require, like an SD card slot, webcam, charger, and so on. Be sure to check the spec list of any laptop you’re considering before you buy, especially if you need specific connectors and capabilities. See Also: Best Laptops for 2026 Best Gaming Laptops Best 2-in-1 Laptops for 2026 Best Chromebooks Best Laptops for College Students As for Copilot+, don’t expect to see much of it on truly affordable Windows laptops just yet. Microsoft’s AI features and Copilot assistant require certain specs to run, namely a powerful neural processing unit (NPU), 16GB of RAM and 256GB of storage. Currently, the cheapest Copilot+ AI PCs will run you about $700, so if you’re willing to pay more for those perks, check out our best laptops guide for more options. If you’re looking for either a gaming laptop or a “Windows on Arm” laptop, both categories will require you to spend more money than we’re discussing here. Best cheap Windows laptops for 2026 The cheap Windows laptop market moves fast, and — unlike nearly all of our other buying guides — we haven't necessarily tested each specific configuration listed below. However, the combination of these technical specifications and familiar brands represent exactly the sort of entry-level laptops we'd recommend to shoppers in this price range based on our thorough research and expert knowledge. What to know about the budget Windows laptop market The best cheap laptop models change all the time. Unlike more expensive, flagship machines, these notebooks can be updated a couple times each year. That can make it hard to track down a specific model at Amazon, Best Buy, Walmart or any other retailer. Also, we’ve seen prices vary widely depending on the configuration and retailer you’re looking at. You can ensure you’re getting a quality laptop by doing a few things. First and foremost, make sure you get a machine that follows the recommended specs we list above. Also, make sure you’re buying from a reputable retailer, including big-box stores like Walmart, Best Buy and Costco, online shops like Amazon or direct manufacturers like Dell, HP, Lenovo and others. If you have a physical store near you (likely a Best Buy in the US), it’s never a bad idea to go play around with some laptops in person before choosing one. If you decide to shop online from the likes of Amazon or Walmart, double check the seller of the laptop you’re considering. For example, many items on Amazon are “shipped and sold” by Amazon and those are typically the best options. You’ll see that information on Amazon on the right sidebar on a product page, under the Add to Cart and Buy Now buttons. Third-party sellers are common in the affordable laptop space. Amazon sometimes classifies laptop manufacturers as third-party sellers, so you may see a laptop shipped and sold by HP or Dell — that’s a good thing, since it’s coming directly from the manufacturer. However, there are other third-party electronics sellers out there. We recommend clicking on the third-party seller’s name on Amazon or Walmart (yes, Walmart has them, too) to see how much positive feedback and how many five-star ratings they’ve received from buyers. What about Chromebooks and tablets? You may be inclined to recommend a Chromebook or a tablet to anyone considering a budget Windows laptop computer. Those instincts aren’t wrong, but Chromebooks and tablets aren’t the best buy for everyone. Tablets have the most portability, but they will only work for the most mobile-competent users like kids who have been grabbing smartphones out of their parents’ hands since they’ve been dexterous enough to do so. Tablets can also be just as expensive as some of the cheapest Windows laptops, and that’s without a mouse or keyboard. Chromebooks are a good alternative for those that basically live in a browser, the trade-off being you must give up the “traditional desktop.” And Chrome OS is a more limited operating system than Windows when it comes to the programs you can install and run. What Windows laptops do well What can you realistically accomplish on a cheap Windows laptop? Quite a bit, especially if you’re doing one thing (or a limited number of things) at a time. They’re great for everyday tasks like web browsing, checking email, video streaming and more. All of those things can be done on Chromebooks as well, but Windows laptops have a big advantage in Microsoft Office. While yes, there is a browser based version, the native, desktop apps are considered a must have for many and will run smoothly on even the most bare-bones budget laptop. The only caveat is that you may run into some slowdown on low-powered devices if you’re multitasking or working with large data sets in Excel or a lot of photos and graphics in Powerpoint. When it comes to specs, a bright spot for Windows laptops is storage. Even the most affordable devices tend to have at least a 128GB solid state drive. That will come in handy if you prefer to keep your most important files saved locally on your laptop's hard drive. In contrast, cheaper Chromebooks often have less storage because they’re built on the assumption that you’ll save all of your documents in the cloud. Not only is that less convenient when you need to work offline, but it also limits the size of programs and files that you can download. So, Chromebooks aren't the best for hoarding Netflix shows before a long trip or for use as a gaming laptop. Windows also has thousands of apps that you can download from its app store. Chromebooks have some Chrome apps, numerous browser extensions and the ability to download Android apps, but quality control is… inconsistent. Android apps, in particular, often haven’t been optimized for Chrome OS, which makes for a wonky user experience. Windows may not have as many apps as Android, but at least the experience is fairly standard across the board. Windows also gives you the ability to download and use programs from other sources, like direct from the developer. You can run things like Adobe Creative Suite, certain VPNs and programs like GIMP, Audacity and ClipMate on a Windows device, which just isn’t possible on Chrome OS. Chromebooks limit you to the apps and programs in The Play Store and the Chrome Extensions store, reducing any others to unusable, space-sucking icons in your Downloads folder.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-affordable-windows-laptops-123000512.html?src=rss",
          "content": "You don’t need to spend a fortune to get a capable Windows laptop. For everyday tasks like web browsing, writing documents, streaming video or handling schoolwork, a well-chosen budget machine can still deliver a smooth, reliable experience. The challenge is cutting through the noise to find affordable options that balance performance, build quality and battery life without serious compromises.For many buyers, timing is no longer optional. With Windows 10 support now officially over, upgrading has become a necessity rather than a nice-to-have. The picks below focus on cheap Windows laptops that can handle day-to-day workloads comfortably while keeping you current on software and security updates. If you’re open to spending more for extra power or premium features, our broader guide to the best Windows laptops covers higher-end alternatives as well. What to look for in a budget-friendly Windows laptop While you can do a lot even when spending little on a Windows laptop, you must set your expectations accordingly. The biggest downside when purchasing a budget laptop (of any kind, really) is limited power. You’ll want to carefully consider a few specs, the most important among them being the processor (CPU). Many Windows laptops under $500 run on Intel Celeron or Pentium chipsets, but you can find some with Core i3/i5 and AMD Ryzen 3/5 CPUs at the higher end of the price spectrum. We recommend getting the most powerful CPU you can afford because it will dictate how fast the computer will feel overall. Memory (RAM) is also important because, the more you have, the easier it will be for the laptop to manage things like a dozen browser tabs while you edit a Word document and stream music in the background. When it comes to storage, consider how much you want to save locally. If you primarily work in Google Docs or save most things in the cloud, you may not need a machine with a ton of onboard storage. Just remember that your digital space will also be taken up by apps, so it may be worth getting a little extra storage than you think you need if you know you’ll be downloading big programs. A final side note: solid state drives (SSDs) are ubiquitous at this point, not to mention faster and more efficient than hard drives (HDDs), so we recommend getting a laptop with that type of storage. As for screens, there’s a healthy mix of HD (720p resolution) and FHD (1080p) options in this price range and we recommend springing for a notebook with a 1080p display if you can. Touchscreens aren’t as common in the budget space as standard panels, but you’ll only really miss one if you get a 2-in-1 laptop. Before we get to our recommended specs for a cheap Windows laptop, it’s worth mentioning that Microsoft clearly lays out the true minimum requirements for any Windows 11 machine. Those include a 1GHz or faster processor that includes two or more cores, at least 4GB of RAM and 64GB of available storage space. That’s the bare minimum to run Windows 11; we recommend giving yourself some wiggle room by choosing a machine that will perform well now and for years to come. Specs to look for in an affordable Windows laptop CPU: Intel Core i3 or AMD Ryzen 3 processors, at minimum RAM: At least 8GB Storage: At least 128GB SSD Screen: At least 1080p FHD It’s essential to prioritize what’s important to you. But at the lower end of the budget, a good laptop may not offer everything you need, whereas a great one might. Although most machines come with features like Bluetooth, built-in Wi-Fi and additional ports, you might find not all of them come with the specifics you require, like an SD card slot, webcam, charger, and so on. Be sure to check the spec list of any laptop you’re considering before you buy, especially if you need specific connectors and capabilities. See Also: Best Laptops for 2026 Best Gaming Laptops Best 2-in-1 Laptops for 2026 Best Chromebooks Best Laptops for College Students As for Copilot+, don’t expect to see much of it on truly affordable Windows laptops just yet. Microsoft’s AI features and Copilot assistant require certain specs to run, namely a powerful neural processing unit (NPU), 16GB of RAM and 256GB of storage. Currently, the cheapest Copilot+ AI PCs will run you about $700, so if you’re willing to pay more for those perks, check out our best laptops guide for more options. If you’re looking for either a gaming laptop or a “Windows on Arm” laptop, both categories will require you to spend more money than we’re discussing here. Best cheap Windows laptops for 2026 The cheap Windows laptop market moves fast, and — unlike nearly all of our other buying guides — we haven't necessarily tested each specific configuration listed below. However, the combination of these technical specifications and familiar brands represent exactly the sort of entry-level laptops we'd recommend to shoppers in this price range based on our thorough research and expert knowledge. What to know about the budget Windows laptop market The best cheap laptop models change all the time. Unlike more expensive, flagship machines, these notebooks can be updated a couple times each year. That can make it hard to track down a specific model at Amazon, Best Buy, Walmart or any other retailer. Also, we’ve seen prices vary widely depending on the configuration and retailer you’re looking at. You can ensure you’re getting a quality laptop by doing a few things. First and foremost, make sure you get a machine that follows the recommended specs we list above. Also, make sure you’re buying from a reputable retailer, including big-box stores like Walmart, Best Buy and Costco, online shops like Amazon or direct manufacturers like Dell, HP, Lenovo and others. If you have a physical store near you (likely a Best Buy in the US), it’s never a bad idea to go play around with some laptops in person before choosing one. If you decide to shop online from the likes of Amazon or Walmart, double check the seller of the laptop you’re considering. For example, many items on Amazon are “shipped and sold” by Amazon and those are typically the best options. You’ll see that information on Amazon on the right sidebar on a product page, under the Add to Cart and Buy Now buttons. Third-party sellers are common in the affordable laptop space. Amazon sometimes classifies laptop manufacturers as third-party sellers, so you may see a laptop shipped and sold by HP or Dell — that’s a good thing, since it’s coming directly from the manufacturer. However, there are other third-party electronics sellers out there. We recommend clicking on the third-party seller’s name on Amazon or Walmart (yes, Walmart has them, too) to see how much positive feedback and how many five-star ratings they’ve received from buyers. What about Chromebooks and tablets? You may be inclined to recommend a Chromebook or a tablet to anyone considering a budget Windows laptop computer. Those instincts aren’t wrong, but Chromebooks and tablets aren’t the best buy for everyone. Tablets have the most portability, but they will only work for the most mobile-competent users like kids who have been grabbing smartphones out of their parents’ hands since they’ve been dexterous enough to do so. Tablets can also be just as expensive as some of the cheapest Windows laptops, and that’s without a mouse or keyboard. Chromebooks are a good alternative for those that basically live in a browser, the trade-off being you must give up the “traditional desktop.” And Chrome OS is a more limited operating system than Windows when it comes to the programs you can install and run. What Windows laptops do well What can you realistically accomplish on a cheap Windows laptop? Quite a bit, especially if you’re doing one thing (or a limited number of things) at a time. They’re great for everyday tasks like web browsing, checking email, video streaming and more. All of those things can be done on Chromebooks as well, but Windows laptops have a big advantage in Microsoft Office. While yes, there is a browser based version, the native, desktop apps are considered a must have for many and will run smoothly on even the most bare-bones budget laptop. The only caveat is that you may run into some slowdown on low-powered devices if you’re multitasking or working with large data sets in Excel or a lot of photos and graphics in Powerpoint. When it comes to specs, a bright spot for Windows laptops is storage. Even the most affordable devices tend to have at least a 128GB solid state drive. That will come in handy if you prefer to keep your most important files saved locally on your laptop's hard drive. In contrast, cheaper Chromebooks often have less storage because they’re built on the assumption that you’ll save all of your documents in the cloud. Not only is that less convenient when you need to work offline, but it also limits the size of programs and files that you can download. So, Chromebooks aren't the best for hoarding Netflix shows before a long trip or for use as a gaming laptop. Windows also has thousands of apps that you can download from its app store. Chromebooks have some Chrome apps, numerous browser extensions and the ability to download Android apps, but quality control is… inconsistent. Android apps, in particular, often haven’t been optimized for Chrome OS, which makes for a wonky user experience. Windows may not have as many apps as Android, but at least the experience is fairly standard across the board. Windows also gives you the ability to download and use programs from other sources, like direct from the developer. You can run things like Adobe Creative Suite, certain VPNs and programs like GIMP, Audacity and ClipMate on a Windows device, which just isn’t possible on Chrome OS. Chromebooks limit you to the apps and programs in The Play Store and the Chrome Extensions store, reducing any others to unusable, space-sucking icons in your Downloads folder.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-affordable-windows-laptops-123000512.html?src=rss",
          "feed_position": 16,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2021-05/843ff380-b255-11eb-bddf-b4000d3cf728"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html",
          "published_at": "Mon, 23 Feb 2026 08:00:38 +0000",
          "title": "The best earbuds for Android devices in 2026",
          "standfirst": "If you’re using an Android phone, finding the right pair of wireless earbuds can take a little more work than it does for iPhone owners. Apple’s AirPods are tightly woven into iOS, but that same level of seamless integration doesn’t automatically carry over to Android. The good news is there are plenty of earbuds that play just as nicely with Android devices, and in some cases offer features AirPods simply don’t.From earbuds designed to pair especially well with Samsung Galaxy and Google Pixel phones to models that prioritize strong noise cancellation, long battery life or workout-friendly durability, the Android ecosystem has no shortage of solid options. We’ve tested a wide range of wireless earbuds to find the best picks for Android users, whether you’re after premium sound, reliable everyday performance or a more affordable alternative. Best Android earbuds for 2026 What to look for in wireless earbuds for Android devices Photo by Jeff Dunn / Engadget For the most part, the features you want from a set of “Android earbuds” are the same as what you want from any headphones. Great sound quality, a comfortable fit and sufficient battery life are still the foundations. Adequate water resistance is good for workouts, and nobody wants a crummy mic for making calls. Once you approach the $100 range, features like active noise cancellation (ANC), wireless charging, an ambient sound mode (which lets you better hear outside noise without turning off your music) and multipoint connectivity (the ability to pair with multiple devices simultaneously) should be expected. For Android devices specifically, there are a few extras to consider. A dedicated app that makes it easy to switch sound modes, customize the audio profile, locate your earbuds if they ever get misplaced or adjust other settings is strongly preferred. Features like Google Fast Pair or NFC-based pairing, which can help you avoid having to dig through your Bluetooth menu to connect your earbuds for the first time, are also nice perks. Some Android devices can also utilize higher-quality Bluetooth codecs such as aptX Adaptive or Sony’s LDAC — these aren’t nearly as important to audio quality as the actual architecture of your earbuds, but they can help wring out a little more detail if the buds are capable enough and you’re streaming lossless files. AptX Adaptive can also help reduce latency, which is good for streaming video or gaming. Diversity is Android’s greatest strength, but it also means that some wireless earbuds play nicer with certain devices, typically those made by the same company. Recent Samsung earbuds, for instance, come with a few perks that are only available if you use a Galaxy phone. We have a couple of recommendations related to this idea above. How we test Android earbuds Photo by Billy Steele/Engadget The best way to test earphones is simply to wear them as much as possible, so that’s what we do. We typically do this over a one- to two-week period, though embargo times occasionally force us to finish our review process a bit faster. We listen to a test playlist that includes several musical genres and podcasts, paying close attention to how each pair approaches the bass, mid and treble frequencies to get an accurate sense of its sound profile. We also test at high and low volumes to check for consistency in the tuning. We do not have access to a dummy head to take more objective measurements, but we’ll sometimes look to sites like Rtings, SoundGuys and others that do just to ensure our impressions are not wildly off-base. If a model supports custom EQ, we’ll tinker with that and use the available EQ presets to see if one sounds dramatically better than the others — though in general we base most of our impressions on the stock tuning each pair uses by default. To assess microphone quality, we record our own audio samples and take multiple calls with a partner both indoors and outside. For battery life, we play our test playlist on a loop with the volume around 75 percent and measure how long it takes for each set to drain. Where applicable, we do a thorough review of a pair’s companion app and test each available feature. While comfort is ultimately subjective, we take note of how secure each pair feels while we’re on the move. We also use certain pairs in especially crowded public spaces to get a better sense of their passive and active noise cancellation, as well as their ability to maintain a consistent Bluetooth connection. Recent updates February 2026: Updated to include new top picks. November 2025: The lightly updated Beats Powerbeats Fit replace the older Beats Fit Pro as our top pick for working out. We’ve also noted the new Google Pixel Buds 2a as a cheaper alternative to the Pixel Buds Pro 2, which remain our recommendation for Pixel phone users. August 2025: We’ve taken another sweep to ensure our advice is still up-to-date. May 2025: We’ve checked this guide to ensure our top picks still stand and noted a couple alternatives to the Noble Fokus Rex5, since that pair has had stock issues of late. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of our recommendations (and the consumer tech industry as a whole). All of our picks are still available in their normal price ranges today, but we’ll update this guide if that changes. February 2025: The Noble FoKus Rex5 is our new \"best for sound quality\" pick, replacing the Sennheiser Momentum True Wireless 4. Our other recommendations remain unchanged. December 2024: We’ve lightly edited this guide for clarity and ensured that our current picks are still accurate.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html?src=rss",
          "content": "If you’re using an Android phone, finding the right pair of wireless earbuds can take a little more work than it does for iPhone owners. Apple’s AirPods are tightly woven into iOS, but that same level of seamless integration doesn’t automatically carry over to Android. The good news is there are plenty of earbuds that play just as nicely with Android devices, and in some cases offer features AirPods simply don’t.From earbuds designed to pair especially well with Samsung Galaxy and Google Pixel phones to models that prioritize strong noise cancellation, long battery life or workout-friendly durability, the Android ecosystem has no shortage of solid options. We’ve tested a wide range of wireless earbuds to find the best picks for Android users, whether you’re after premium sound, reliable everyday performance or a more affordable alternative. Best Android earbuds for 2026 What to look for in wireless earbuds for Android devices Photo by Jeff Dunn / Engadget For the most part, the features you want from a set of “Android earbuds” are the same as what you want from any headphones. Great sound quality, a comfortable fit and sufficient battery life are still the foundations. Adequate water resistance is good for workouts, and nobody wants a crummy mic for making calls. Once you approach the $100 range, features like active noise cancellation (ANC), wireless charging, an ambient sound mode (which lets you better hear outside noise without turning off your music) and multipoint connectivity (the ability to pair with multiple devices simultaneously) should be expected. For Android devices specifically, there are a few extras to consider. A dedicated app that makes it easy to switch sound modes, customize the audio profile, locate your earbuds if they ever get misplaced or adjust other settings is strongly preferred. Features like Google Fast Pair or NFC-based pairing, which can help you avoid having to dig through your Bluetooth menu to connect your earbuds for the first time, are also nice perks. Some Android devices can also utilize higher-quality Bluetooth codecs such as aptX Adaptive or Sony’s LDAC — these aren’t nearly as important to audio quality as the actual architecture of your earbuds, but they can help wring out a little more detail if the buds are capable enough and you’re streaming lossless files. AptX Adaptive can also help reduce latency, which is good for streaming video or gaming. Diversity is Android’s greatest strength, but it also means that some wireless earbuds play nicer with certain devices, typically those made by the same company. Recent Samsung earbuds, for instance, come with a few perks that are only available if you use a Galaxy phone. We have a couple of recommendations related to this idea above. How we test Android earbuds Photo by Billy Steele/Engadget The best way to test earphones is simply to wear them as much as possible, so that’s what we do. We typically do this over a one- to two-week period, though embargo times occasionally force us to finish our review process a bit faster. We listen to a test playlist that includes several musical genres and podcasts, paying close attention to how each pair approaches the bass, mid and treble frequencies to get an accurate sense of its sound profile. We also test at high and low volumes to check for consistency in the tuning. We do not have access to a dummy head to take more objective measurements, but we’ll sometimes look to sites like Rtings, SoundGuys and others that do just to ensure our impressions are not wildly off-base. If a model supports custom EQ, we’ll tinker with that and use the available EQ presets to see if one sounds dramatically better than the others — though in general we base most of our impressions on the stock tuning each pair uses by default. To assess microphone quality, we record our own audio samples and take multiple calls with a partner both indoors and outside. For battery life, we play our test playlist on a loop with the volume around 75 percent and measure how long it takes for each set to drain. Where applicable, we do a thorough review of a pair’s companion app and test each available feature. While comfort is ultimately subjective, we take note of how secure each pair feels while we’re on the move. We also use certain pairs in especially crowded public spaces to get a better sense of their passive and active noise cancellation, as well as their ability to maintain a consistent Bluetooth connection. Recent updates February 2026: Updated to include new top picks. November 2025: The lightly updated Beats Powerbeats Fit replace the older Beats Fit Pro as our top pick for working out. We’ve also noted the new Google Pixel Buds 2a as a cheaper alternative to the Pixel Buds Pro 2, which remain our recommendation for Pixel phone users. August 2025: We’ve taken another sweep to ensure our advice is still up-to-date. May 2025: We’ve checked this guide to ensure our top picks still stand and noted a couple alternatives to the Noble Fokus Rex5, since that pair has had stock issues of late. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of our recommendations (and the consumer tech industry as a whole). All of our picks are still available in their normal price ranges today, but we’ll update this guide if that changes. February 2025: The Noble FoKus Rex5 is our new \"best for sound quality\" pick, replacing the Sennheiser Momentum True Wireless 4. Our other recommendations remain unchanged. December 2024: We’ve lightly edited this guide for clarity and ensured that our current picks are still accurate.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html?src=rss",
          "feed_position": 17,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-06/f9e8ef90-0c55-11ee-97db-1ddd4c19f474"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/ai-agents-are-delivering-real-roi-heres-what-1-100-developers-and-ctos",
          "published_at": "Mon, 23 Feb 2026 05:00:00 GMT",
          "title": "AI Agents are delivering real ROI — Here's what 1,100 developers and CTOs reveal about scaling them",
          "standfirst": "Presented by DigitalOceanFrom refactoring codebases to debugging production code, AI agents are already proving their value. But scaling them in production remains the exception, not the rule. In DigitalOcean’s 2026 Currents research report, based on a survey of more than 1,100 developers, CTOs, and founders, 67% of organizations using agents report productivity gains. Meanwhile, 60% of respondents say applications and agents represent the greatest long-term value in the AI stack. Yet, only 10% are scaling agents in production. The top blocker? Forty-nine percent cite the high cost of inference. It&#x27;s not just the price of a single API call. It&#x27;s the compounding cost as agents chain tasks and run autonomously. Nearly half of respondents now spend 76–100% of their AI budget on inference alone. This is a problem DigitalOcean is working to solve. What&#x27;s needed is infrastructure designed around inference economics: predictable performance, cost control under load, and fewer moving parts. That&#x27;s how 2026 becomes the year agents graduate from pilot to product.52% of companies are actively implementing AI solutions (including agents)Just a year ago when we ran this survey, only 35% of respondents were actively implementing AI solutions — most were still in exploration mode or running their first projects. Now it’s 52%. The shift from \"let&#x27;s see what this can do\" to \"let&#x27;s put this into production\" is well underway.There&#x27;s an agent boom underneath these numbers. 46% of those respondents are specifically deploying AI agents, autonomous systems that execute tasks on their own rather than wait for instructions at every step. OpenClaw (formerly Moltbot and Clawdbot) is one recent example, an open-source assistant that connects to messaging apps, browses the web, executes shell commands, and runs tasks autonomously.Where are those agents going? Mostly into code and operations:54% said code generation and refactoring, making it the clear frontrunner49% are automating internal operations45% are building customer support and chatbots43% are focused on business logic and task orchestration41% are using agents for written content generation27% are pursuing marketing workflow automation21% are conducting data analysisDevelopers are leading the charge here. For example, Y Combinator shared that a quarter of its Winter 2025 startups were building with codebases that are 95% AI-generated. Then there&#x27;s what Andrej Karpathy calls \"vibe coding\" — describing what you want in plain language and letting the AI write the code.The tooling has split to match different workflows. Cursor bakes AI into a VS Code fork for inline edits and rapid iteration. Claude Code runs in the terminal for deeper work across entire repositories. But both have moved well beyond autocomplete. These tools now operate in agentic loops, reading files, running tests, identifying failures, and iterating until the build passes. You describe a feature. The agent implements it. Some sessions stretch for hours — no one at the keyboard.But agents aren&#x27;t just for engineers. They&#x27;re making their way into marketing, customer success, and ops. We see this internally at DigitalOcean, too. Experimental showcases and hack days have surfaced demos of AI workflows to test ad copy at scale, personalize emails, and prioritize growth experiments.67% of organizations using agents report measurable productivity improvementsThe productivity question is the one everyone&#x27;s asking: are agents actually delivering results, or is this still hype? The data suggests the former. Overall, 67% of organizations using agents report measurable productivity improvements. And for some, the gains are substantial: 9% of respondents reported productivity increases of 75% or more. When asked what outcomes they&#x27;ve observed from using AI agents:53% said productivity and time savings for employees44% reported the creation of new business capabilities32% noted a reduced need to hire additional staff27% saw measurable cost savings26% reported improved customer experienceInternal research at Anthropic explores what these technologies unlock: when the company studied how its own engineers use Claude Code, it found that more than a quarter of AI-assisted work consisted of tasks that simply wouldn&#x27;t have been done otherwise. That includes scaling projects and building internal tools. It also includes exploratory work that previously wasn&#x27;t worth the time investment — but now is.What pushes those productivity numbers even higher? Agents are learning to work together. Google&#x27;s release of the Agent Development Kit as an open-source framework marked a shift from single-purpose agents to coordinated multi-agent systems that can discover one another, exchange information, and collaborate regardless of vendor or framework. That said, 14% have yet to see a benefit, and 19% say it&#x27;s too early to measure. From what we&#x27;re seeing, 2025 was largely a year of prototyping and experimentation, with 2026 shaping up to be when more teams move agents into production.60% bet on applications and agents as the biggest opportunity in AIBudgets follow the results. AI remains an active area of investment for the vast majority of organizations: only 4% of respondents said they don&#x27;t expect to invest in AI over the next 12 months. And where organizations are seeing productivity gains, they&#x27;re doubling down — on the application layer, not foundational infrastructure. When asked where respondents expect budget growth over the next 12 months, 37% pointed to applications and agents, more than double the share for infrastructure (14%) or platforms (17%). The long-term view is even stronger: 60% see applications and agents as the greatest opportunity in the AI stack, compared to just 19% for infrastructure. Market data backs this up. According to one report, the application layer captured $19 billion in 2025 — more than half of all generative AI spending. Coding tools led at $4 billion, representing 55% of departmental AI spend and the single largest category across the entire stack. Organizations are betting that the application layer, where AI actually touches users and workflows, will matter more than the underlying components.49% say the cost of running AI at scale is their top barrier to growthAgents only work if you can run them. And right now, inference is the bottleneck. Unlike training, which is a fixed upfront investment to build the model, each prompt to an agent generates tokens that incur a cost. That cost compounds with every reasoning step, retry, and self-correction cycle. At scale, this turns inference into an operational expense that can exceed the original investment in the model itself.When we asked respondents what limits their ability to scale AI, 49% identified the high cost of inference at scale as their top barrier. This tracks with where budgets are going: 44% of respondents now spend the majority of their AI budget (76-100%) on inference, not training.But solving for inference shouldn&#x27;t fall on developers. The complexity of optimizing GPU configurations, managing parallelization strategies, and fine-tuning model serving infrastructure is not the kind of work most teams should be doing themselves. That&#x27;s infrastructure-level complexity, and cloud providers need to absorb it.At DigitalOcean, this is central to how we think about our Gradient™ AI Inference Cloud. We&#x27;re investing in inference optimization so that the teams we serve don&#x27;t have to. Character.ai is a good example: they came to us needing to lower inference costs without sacrificing performance or latency. By migrating to our inference cloud platform and working closely with our team and AMD, they doubled their production inference throughput and reduced their cost per token by 50%. That kind of outcome is what becomes possible when the platform does the heavy lifting. As agents move from pilots to production, the companies that scale successfully will be the ones that aren&#x27;t stuck solving inference on their own. Wade Wegner is Chief Ecosystem and Growth Officer at DigitalOcean.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by DigitalOceanFrom refactoring codebases to debugging production code, AI agents are already proving their value. But scaling them in production remains the exception, not the rule. In DigitalOcean’s 2026 Currents research report, based on a survey of more than 1,100 developers, CTOs, and founders, 67% of organizations using agents report productivity gains. Meanwhile, 60% of respondents say applications and agents represent the greatest long-term value in the AI stack. Yet, only 10% are scaling agents in production. The top blocker? Forty-nine percent cite the high cost of inference. It&#x27;s not just the price of a single API call. It&#x27;s the compounding cost as agents chain tasks and run autonomously. Nearly half of respondents now spend 76–100% of their AI budget on inference alone. This is a problem DigitalOcean is working to solve. What&#x27;s needed is infrastructure designed around inference economics: predictable performance, cost control under load, and fewer moving parts. That&#x27;s how 2026 becomes the year agents graduate from pilot to product.52% of companies are actively implementing AI solutions (including agents)Just a year ago when we ran this survey, only 35% of respondents were actively implementing AI solutions — most were still in exploration mode or running their first projects. Now it’s 52%. The shift from \"let&#x27;s see what this can do\" to \"let&#x27;s put this into production\" is well underway.There&#x27;s an agent boom underneath these numbers. 46% of those respondents are specifically deploying AI agents, autonomous systems that execute tasks on their own rather than wait for instructions at every step. OpenClaw (formerly Moltbot and Clawdbot) is one recent example, an open-source assistant that connects to messaging apps, browses the web, executes shell commands, and runs tasks autonomously.Where are those agents going? Mostly into code and operations:54% said code generation and refactoring, making it the clear frontrunner49% are automating internal operations45% are building customer support and chatbots43% are focused on business logic and task orchestration41% are using agents for written content generation27% are pursuing marketing workflow automation21% are conducting data analysisDevelopers are leading the charge here. For example, Y Combinator shared that a quarter of its Winter 2025 startups were building with codebases that are 95% AI-generated. Then there&#x27;s what Andrej Karpathy calls \"vibe coding\" — describing what you want in plain language and letting the AI write the code.The tooling has split to match different workflows. Cursor bakes AI into a VS Code fork for inline edits and rapid iteration. Claude Code runs in the terminal for deeper work across entire repositories. But both have moved well beyond autocomplete. These tools now operate in agentic loops, reading files, running tests, identifying failures, and iterating until the build passes. You describe a feature. The agent implements it. Some sessions stretch for hours — no one at the keyboard.But agents aren&#x27;t just for engineers. They&#x27;re making their way into marketing, customer success, and ops. We see this internally at DigitalOcean, too. Experimental showcases and hack days have surfaced demos of AI workflows to test ad copy at scale, personalize emails, and prioritize growth experiments.67% of organizations using agents report measurable productivity improvementsThe productivity question is the one everyone&#x27;s asking: are agents actually delivering results, or is this still hype? The data suggests the former. Overall, 67% of organizations using agents report measurable productivity improvements. And for some, the gains are substantial: 9% of respondents reported productivity increases of 75% or more. When asked what outcomes they&#x27;ve observed from using AI agents:53% said productivity and time savings for employees44% reported the creation of new business capabilities32% noted a reduced need to hire additional staff27% saw measurable cost savings26% reported improved customer experienceInternal research at Anthropic explores what these technologies unlock: when the company studied how its own engineers use Claude Code, it found that more than a quarter of AI-assisted work consisted of tasks that simply wouldn&#x27;t have been done otherwise. That includes scaling projects and building internal tools. It also includes exploratory work that previously wasn&#x27;t worth the time investment — but now is.What pushes those productivity numbers even higher? Agents are learning to work together. Google&#x27;s release of the Agent Development Kit as an open-source framework marked a shift from single-purpose agents to coordinated multi-agent systems that can discover one another, exchange information, and collaborate regardless of vendor or framework. That said, 14% have yet to see a benefit, and 19% say it&#x27;s too early to measure. From what we&#x27;re seeing, 2025 was largely a year of prototyping and experimentation, with 2026 shaping up to be when more teams move agents into production.60% bet on applications and agents as the biggest opportunity in AIBudgets follow the results. AI remains an active area of investment for the vast majority of organizations: only 4% of respondents said they don&#x27;t expect to invest in AI over the next 12 months. And where organizations are seeing productivity gains, they&#x27;re doubling down — on the application layer, not foundational infrastructure. When asked where respondents expect budget growth over the next 12 months, 37% pointed to applications and agents, more than double the share for infrastructure (14%) or platforms (17%). The long-term view is even stronger: 60% see applications and agents as the greatest opportunity in the AI stack, compared to just 19% for infrastructure. Market data backs this up. According to one report, the application layer captured $19 billion in 2025 — more than half of all generative AI spending. Coding tools led at $4 billion, representing 55% of departmental AI spend and the single largest category across the entire stack. Organizations are betting that the application layer, where AI actually touches users and workflows, will matter more than the underlying components.49% say the cost of running AI at scale is their top barrier to growthAgents only work if you can run them. And right now, inference is the bottleneck. Unlike training, which is a fixed upfront investment to build the model, each prompt to an agent generates tokens that incur a cost. That cost compounds with every reasoning step, retry, and self-correction cycle. At scale, this turns inference into an operational expense that can exceed the original investment in the model itself.When we asked respondents what limits their ability to scale AI, 49% identified the high cost of inference at scale as their top barrier. This tracks with where budgets are going: 44% of respondents now spend the majority of their AI budget (76-100%) on inference, not training.But solving for inference shouldn&#x27;t fall on developers. The complexity of optimizing GPU configurations, managing parallelization strategies, and fine-tuning model serving infrastructure is not the kind of work most teams should be doing themselves. That&#x27;s infrastructure-level complexity, and cloud providers need to absorb it.At DigitalOcean, this is central to how we think about our Gradient™ AI Inference Cloud. We&#x27;re investing in inference optimization so that the teams we serve don&#x27;t have to. Character.ai is a good example: they came to us needing to lower inference costs without sacrificing performance or latency. By migrating to our inference cloud platform and working closely with our team and AMD, they doubled their production inference throughput and reduced their cost per token by 50%. That kind of outcome is what becomes possible when the platform does the heavy lifting. As agents move from pilots to production, the companies that scale successfully will be the ones that aren&#x27;t stuck solving inference on their own. Wade Wegner is Chief Ecosystem and Growth Officer at DigitalOcean.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6T8BoZdjL8FB8SPwV16gxU/174167a5afd38d806a5fb66859311d6e/AdobeStock_507970416.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/shadow-mode-drift-alerts-and-audit-logs-inside-the-modern-audit-loop",
          "published_at": "Sun, 22 Feb 2026 19:00:00 GMT",
          "title": "Shadow mode, drift alerts and audit logs: Inside the modern audit loop",
          "standfirst": "Traditional software governance often uses static compliance checklists, quarterly audits and after-the-fact reviews. But this method can&#x27;t keep up with AI systems that change in real time. A machine learning (ML) model might retrain or drift between quarterly operational syncs. This means that, by the time an issue is discovered, hundreds of bad decisions could already have been made. This can be almost impossible to untangle. In the fast-paced world of AI, governance must be inline, not an after-the-fact compliance review. In other words, organizations must adopt what I call an “audit loop\": A continuous, integrated compliance process that operates in real-time alongside AI development and deployment, without halting innovation. This article explains how to implement such continuous AI compliance through shadow mode rollouts, drift and misuse monitoring and audit logs engineered for direct legal defensibility.From reactive checks to an inline “audit loop”When systems moved at the speed of people, it made sense to do compliance checks every so often. But AI doesn&#x27;t wait for the next review meeting. The change to an inline audit loop means audits will no longer occur just once in a while; they happen all the time. Compliance and risk management should be \"baked in\" to the AI lifecycle from development to production, rather than just post-deployment. This means establishing live metrics and guardrails that monitor AI behavior as it occurs and raise red flags as soon as something seems off. For instance, teams can set up drift detectors that automatically alert when a model&#x27;s predictions go off course from the training distribution, or when confidence scores fall below acceptable levels. Governance is no longer just a set of quarterly snapshots; it&#x27;s a streaming process with alerts that go off in real time when a system goes outside of its defined confidence bands.Cultural shift is equally important: Compliance teams must act less like after-the-fact auditors and more like AI co-pilots. In practice, this might mean compliance and AI engineers working together to define policy guardrails and continuously monitor key indicators. With the right tools and mindset, real-time AI governance can “nudge” and intervene early, helping teams course-correct without slowing down innovation. In fact, when done well, continuous governance builds trust rather than friction, providing shared visibility into AI operations for both builders and regulators, instead of unpleasant surprises after deployment. The following strategies illustrate how to achieve this balance.Shadow mode rollouts: Testing compliance safelyOne effective framework for continuous AI compliance is “shadow mode” deployments with new models or agent features. This means a new AI system is deployed in parallel with the existing system, receiving real production inputs but not influencing real decisions or user-facing outputs. The legacy model or process continues to handle decisions, while the new AI’s outputs are captured only for analysis. This provides a safe sandbox to vet the AI’s behavior under real conditions. According to global law firm Morgan Lewis: “Shadow-mode operation requires the AI to run in parallel without influencing live decisions until its performance is validated,” giving organizations a safe environment to test changes. Teams can discover problems early by comparing the shadow model&#x27;s decisions to expectations (the current model&#x27;s decisions). For instance, when a model is running in shadow mode, they can check to see if its inputs and predictions differ from those of the current production model or the patterns seen in training. Sudden changes could indicate bugs in the data pipeline, unexpected bias or drops in performance. In short, shadow mode is a way to check compliance in real time: It ensures that the model handles inputs correctly and meets policy standards (accuracy, fairness) before it is fully released. One AI security framework showed how this method worked: Teams first ran AI in shadow mode (AI makes suggestions but doesn&#x27;t act on its own), then compared AI and human inputs to determine trust. They only let the AI suggest actions with human approval after it was reliable. For instance, Prophet Security eventually let the AI make low-risk decisions on its own. Using phased rollouts gives people confidence that an AI system meets requirements and works as expected, without putting production or customers at risk during testing. Real-time drift and misuse detectionEven after an AI model is fully deployed, the compliance job is never \"done.\" Over time, AI systems can drift, meaning that their performance or outputs change due to new data patterns, model retraining or bad inputs. They can also be misused or lead to results that go against policy (for example, inappropriate content or biased decisions) in unexpected ways. To remain compliant, teams must set up monitoring signals and processes to catch these issues as they happen. In SLA monitoring, they may only check for uptime or latency. In AI monitoring, however, the system must be able to tell when outputs are not what they should be. For example, if a model suddenly starts giving biased or harmful results. This means setting \"confidence bands\" or quantitative limits for how a model should behave and setting automatic alerts when those limits are crossed.Some signals to monitor include:Data or concept drift: When input data distributions change significantly or model predictions diverge from training-time patterns. For example, a model’s accuracy on certain segments might drop as the incoming data shifts, a sign to investigate and possibly retrain.Anomalous or harmful outputs: When outputs trigger policy violations or ethical red flags. An AI content filter might flag if a generative model produces disallowed content, or a bias monitor might detect if decisions for a protected group begin to skew negatively. Contracts for AI services now often require vendors to detect and address such noncompliant results promptly.User misuse patterns: When unusual usage behavior suggests someone is trying to manipulate or misuse the AI. For instance, rapid-fire queries attempting prompt injection or adversarial inputs could be automatically flagged by the system’s telemetry as potential misuse.When a drift or misuse signal crosses a critical threshold, the system should support “intelligent escalation” rather than waiting for a quarterly review. In practice, this could mean triggering an automated mitigation or immediately alerting a human overseer. Leading organizations build in fail-safes like kill-switches, or the ability to suspend an AI’s actions the moment it behaves unpredictably or unsafely. For example, a service contract might allow a company to instantly pause an AI agent if it’s outputting suspect results, even if the AI provider hasn’t acknowledged a problem. Likewise, teams should have playbooks for rapid model rollback or retraining windows: If drift or errors are detected, there’s a plan to retrain the model (or revert to a safe state) within a defined timeframe. This kind of agile response is crucial; it recognizes that AI behavior may drift or degrade in ways that cannot be fixed with a simple patch, so swift retraining or tuning is part of the compliance loop.By continuously monitoring and reacting to drift and misuse signals, companies transform compliance from a periodic audit to an ongoing safety net. Issues are caught and addressed in hours or days, not months. The AI stays within acceptable bounds, and governance keeps pace with the AI’s own learning and adaptation, rather than trailing behind it. This not only protects users and stakeholders; it gives regulators and executives peace of mind that the AI is under constant watchful oversight, even as it evolves.Audit logs designed for legal defensibilityContinuous compliance also means continuously documenting what your AI is doing and why. Robust audit logs demonstrate compliance, both for internal accountability and external legal defensibility. However, logging for AI requires more than simplistic logs. Imagine an auditor or regulator asking: “Why did the AI make this decision, and did it follow approved policy?” Your logs should be able to answer that.A good AI audit log keeps a permanent, detailed record of every important action and decision AI makes, along with the reasons and context. Legal experts say these logs \"provide detailed, unchangeable records of AI system actions with exact timestamps and written reasons for decisions.\" They are important evidence in court. This means that every important inference, suggestion or independent action taken by AI should be recorded with metadata, such as timestamps, the model/version used, the input received, the output produced and (if possible) the reasoning or confidence behind that output. Modern compliance platforms stress logging not only the result (\"X action taken\") but also the rationale (\"X action taken because conditions Y and Z were met according to policy\"). These enhanced logs let an auditor see, for example, not just that an AI approved a user&#x27;s access, but that it was approved \"based on continuous usage and alignment with the user&#x27;s peer group,\" according to Attorney Aaron Hall.Audit logs should also be well-organized and difficult to change if they are to be legally sound. Techniques like immutable storage or cryptographic hashing of logs ensure that records can&#x27;t be changed. Log data should be protected by access controls and encryption so that sensitive information, such as security keys and personal data, is hidden or protected while still being open. In regulated industries, keeping these logs can show examiners that you are not only keeping track of AI&#x27;s outputs, but you are retaining records for review. Regulators are expecting companies to show more than that an AI was checked before it was released. They want to see that it is being monitored continuously and there is a forensic trail to analyze its behavior over time. This evidentiary backbone comes from complete audit trails that include data inputs, model versions and decision outputs. They make AI less of a \"black box\" and more of a system that can be tracked and held accountable.If there is a disagreement or an event (for example, an AI made a biased choice that hurt a customer), these logs are your legal lifeline. They help you figure out what went wrong. Was it a problem with the data, a model drift or misuse? Who was in charge of the process? Did we stick to the rules we set? Well-kept AI audit logs show that the company did its homework and had controls in place. This not only lowers the risk of legal problems but makes people more trusting of AI systems. With AI, teams and executives can be sure that every decision made is safe because it is open and accountable.Inline governance as an enabler, not a roadblockImplementing an “audit loop” of continuous AI compliance might sound like extra work, but in reality, it enables faster and safer AI delivery. By integrating governance into each stage of the AI lifecycle, from shadow mode trial runs to real-time monitoring to immutable logging, organizations can move quickly and responsibly. Issues are caught early, so they don’t snowball into major failures that require project-halting fixes later. Developers and data scientists can iterate on models without endless back-and-forth with compliance reviewers, because many compliance checks are automated and happen in parallel. Rather than slowing down delivery, this approach often accelerates it: Teams spend less time on reactive damage control or lengthy audits, and more time on innovation because they are confident that compliance is under control in the background.There are bigger benefits to continuous AI compliance, too. It gives end-users, business leaders and regulators a reason to believe that AI systems are being handled responsibly. When every AI decision is clearly recorded, watched and checked for quality, stakeholders are much more likely to accept AI solutions. This trust benefits the whole industry and society, not just individual businesses. An audit-loop governance model can stop AI failures and ensure AI behavior is in line with moral and legal standards. In fact, strong AI governance benefits the economy and the public because it encourages innovation and protection. It can unlock AI&#x27;s potential in important areas like finance, healthcare and infrastructure without putting safety or values at risk. As national and international standards for AI change quickly, U.S. companies that set a good example by always following the rules are at the forefront of trustworthy AI.People say that if your AI governance isn&#x27;t keeping up with your AI, it&#x27;s not really governance; it&#x27;s \"archaeology.\" Forward-thinking companies are realizing this and adopting audit loops. By doing so, they not only avoid problems but make compliance a competitive advantage, ensuring that faster delivery and better oversight go hand in hand.Dhyey Mavani is working to accelerate gen AI and computational mathematics. Editor&#x27;s note: The opinions expressed in this article are the authors&#x27; personal opinions and do not reflect the opinions of their employers.",
          "content": "Traditional software governance often uses static compliance checklists, quarterly audits and after-the-fact reviews. But this method can&#x27;t keep up with AI systems that change in real time. A machine learning (ML) model might retrain or drift between quarterly operational syncs. This means that, by the time an issue is discovered, hundreds of bad decisions could already have been made. This can be almost impossible to untangle. In the fast-paced world of AI, governance must be inline, not an after-the-fact compliance review. In other words, organizations must adopt what I call an “audit loop\": A continuous, integrated compliance process that operates in real-time alongside AI development and deployment, without halting innovation. This article explains how to implement such continuous AI compliance through shadow mode rollouts, drift and misuse monitoring and audit logs engineered for direct legal defensibility.From reactive checks to an inline “audit loop”When systems moved at the speed of people, it made sense to do compliance checks every so often. But AI doesn&#x27;t wait for the next review meeting. The change to an inline audit loop means audits will no longer occur just once in a while; they happen all the time. Compliance and risk management should be \"baked in\" to the AI lifecycle from development to production, rather than just post-deployment. This means establishing live metrics and guardrails that monitor AI behavior as it occurs and raise red flags as soon as something seems off. For instance, teams can set up drift detectors that automatically alert when a model&#x27;s predictions go off course from the training distribution, or when confidence scores fall below acceptable levels. Governance is no longer just a set of quarterly snapshots; it&#x27;s a streaming process with alerts that go off in real time when a system goes outside of its defined confidence bands.Cultural shift is equally important: Compliance teams must act less like after-the-fact auditors and more like AI co-pilots. In practice, this might mean compliance and AI engineers working together to define policy guardrails and continuously monitor key indicators. With the right tools and mindset, real-time AI governance can “nudge” and intervene early, helping teams course-correct without slowing down innovation. In fact, when done well, continuous governance builds trust rather than friction, providing shared visibility into AI operations for both builders and regulators, instead of unpleasant surprises after deployment. The following strategies illustrate how to achieve this balance.Shadow mode rollouts: Testing compliance safelyOne effective framework for continuous AI compliance is “shadow mode” deployments with new models or agent features. This means a new AI system is deployed in parallel with the existing system, receiving real production inputs but not influencing real decisions or user-facing outputs. The legacy model or process continues to handle decisions, while the new AI’s outputs are captured only for analysis. This provides a safe sandbox to vet the AI’s behavior under real conditions. According to global law firm Morgan Lewis: “Shadow-mode operation requires the AI to run in parallel without influencing live decisions until its performance is validated,” giving organizations a safe environment to test changes. Teams can discover problems early by comparing the shadow model&#x27;s decisions to expectations (the current model&#x27;s decisions). For instance, when a model is running in shadow mode, they can check to see if its inputs and predictions differ from those of the current production model or the patterns seen in training. Sudden changes could indicate bugs in the data pipeline, unexpected bias or drops in performance. In short, shadow mode is a way to check compliance in real time: It ensures that the model handles inputs correctly and meets policy standards (accuracy, fairness) before it is fully released. One AI security framework showed how this method worked: Teams first ran AI in shadow mode (AI makes suggestions but doesn&#x27;t act on its own), then compared AI and human inputs to determine trust. They only let the AI suggest actions with human approval after it was reliable. For instance, Prophet Security eventually let the AI make low-risk decisions on its own. Using phased rollouts gives people confidence that an AI system meets requirements and works as expected, without putting production or customers at risk during testing. Real-time drift and misuse detectionEven after an AI model is fully deployed, the compliance job is never \"done.\" Over time, AI systems can drift, meaning that their performance or outputs change due to new data patterns, model retraining or bad inputs. They can also be misused or lead to results that go against policy (for example, inappropriate content or biased decisions) in unexpected ways. To remain compliant, teams must set up monitoring signals and processes to catch these issues as they happen. In SLA monitoring, they may only check for uptime or latency. In AI monitoring, however, the system must be able to tell when outputs are not what they should be. For example, if a model suddenly starts giving biased or harmful results. This means setting \"confidence bands\" or quantitative limits for how a model should behave and setting automatic alerts when those limits are crossed.Some signals to monitor include:Data or concept drift: When input data distributions change significantly or model predictions diverge from training-time patterns. For example, a model’s accuracy on certain segments might drop as the incoming data shifts, a sign to investigate and possibly retrain.Anomalous or harmful outputs: When outputs trigger policy violations or ethical red flags. An AI content filter might flag if a generative model produces disallowed content, or a bias monitor might detect if decisions for a protected group begin to skew negatively. Contracts for AI services now often require vendors to detect and address such noncompliant results promptly.User misuse patterns: When unusual usage behavior suggests someone is trying to manipulate or misuse the AI. For instance, rapid-fire queries attempting prompt injection or adversarial inputs could be automatically flagged by the system’s telemetry as potential misuse.When a drift or misuse signal crosses a critical threshold, the system should support “intelligent escalation” rather than waiting for a quarterly review. In practice, this could mean triggering an automated mitigation or immediately alerting a human overseer. Leading organizations build in fail-safes like kill-switches, or the ability to suspend an AI’s actions the moment it behaves unpredictably or unsafely. For example, a service contract might allow a company to instantly pause an AI agent if it’s outputting suspect results, even if the AI provider hasn’t acknowledged a problem. Likewise, teams should have playbooks for rapid model rollback or retraining windows: If drift or errors are detected, there’s a plan to retrain the model (or revert to a safe state) within a defined timeframe. This kind of agile response is crucial; it recognizes that AI behavior may drift or degrade in ways that cannot be fixed with a simple patch, so swift retraining or tuning is part of the compliance loop.By continuously monitoring and reacting to drift and misuse signals, companies transform compliance from a periodic audit to an ongoing safety net. Issues are caught and addressed in hours or days, not months. The AI stays within acceptable bounds, and governance keeps pace with the AI’s own learning and adaptation, rather than trailing behind it. This not only protects users and stakeholders; it gives regulators and executives peace of mind that the AI is under constant watchful oversight, even as it evolves.Audit logs designed for legal defensibilityContinuous compliance also means continuously documenting what your AI is doing and why. Robust audit logs demonstrate compliance, both for internal accountability and external legal defensibility. However, logging for AI requires more than simplistic logs. Imagine an auditor or regulator asking: “Why did the AI make this decision, and did it follow approved policy?” Your logs should be able to answer that.A good AI audit log keeps a permanent, detailed record of every important action and decision AI makes, along with the reasons and context. Legal experts say these logs \"provide detailed, unchangeable records of AI system actions with exact timestamps and written reasons for decisions.\" They are important evidence in court. This means that every important inference, suggestion or independent action taken by AI should be recorded with metadata, such as timestamps, the model/version used, the input received, the output produced and (if possible) the reasoning or confidence behind that output. Modern compliance platforms stress logging not only the result (\"X action taken\") but also the rationale (\"X action taken because conditions Y and Z were met according to policy\"). These enhanced logs let an auditor see, for example, not just that an AI approved a user&#x27;s access, but that it was approved \"based on continuous usage and alignment with the user&#x27;s peer group,\" according to Attorney Aaron Hall.Audit logs should also be well-organized and difficult to change if they are to be legally sound. Techniques like immutable storage or cryptographic hashing of logs ensure that records can&#x27;t be changed. Log data should be protected by access controls and encryption so that sensitive information, such as security keys and personal data, is hidden or protected while still being open. In regulated industries, keeping these logs can show examiners that you are not only keeping track of AI&#x27;s outputs, but you are retaining records for review. Regulators are expecting companies to show more than that an AI was checked before it was released. They want to see that it is being monitored continuously and there is a forensic trail to analyze its behavior over time. This evidentiary backbone comes from complete audit trails that include data inputs, model versions and decision outputs. They make AI less of a \"black box\" and more of a system that can be tracked and held accountable.If there is a disagreement or an event (for example, an AI made a biased choice that hurt a customer), these logs are your legal lifeline. They help you figure out what went wrong. Was it a problem with the data, a model drift or misuse? Who was in charge of the process? Did we stick to the rules we set? Well-kept AI audit logs show that the company did its homework and had controls in place. This not only lowers the risk of legal problems but makes people more trusting of AI systems. With AI, teams and executives can be sure that every decision made is safe because it is open and accountable.Inline governance as an enabler, not a roadblockImplementing an “audit loop” of continuous AI compliance might sound like extra work, but in reality, it enables faster and safer AI delivery. By integrating governance into each stage of the AI lifecycle, from shadow mode trial runs to real-time monitoring to immutable logging, organizations can move quickly and responsibly. Issues are caught early, so they don’t snowball into major failures that require project-halting fixes later. Developers and data scientists can iterate on models without endless back-and-forth with compliance reviewers, because many compliance checks are automated and happen in parallel. Rather than slowing down delivery, this approach often accelerates it: Teams spend less time on reactive damage control or lengthy audits, and more time on innovation because they are confident that compliance is under control in the background.There are bigger benefits to continuous AI compliance, too. It gives end-users, business leaders and regulators a reason to believe that AI systems are being handled responsibly. When every AI decision is clearly recorded, watched and checked for quality, stakeholders are much more likely to accept AI solutions. This trust benefits the whole industry and society, not just individual businesses. An audit-loop governance model can stop AI failures and ensure AI behavior is in line with moral and legal standards. In fact, strong AI governance benefits the economy and the public because it encourages innovation and protection. It can unlock AI&#x27;s potential in important areas like finance, healthcare and infrastructure without putting safety or values at risk. As national and international standards for AI change quickly, U.S. companies that set a good example by always following the rules are at the forefront of trustworthy AI.People say that if your AI governance isn&#x27;t keeping up with your AI, it&#x27;s not really governance; it&#x27;s \"archaeology.\" Forward-thinking companies are realizing this and adopting audit loops. By doing so, they not only avoid problems but make compliance a competitive advantage, ensuring that faster delivery and better oversight go hand in hand.Dhyey Mavani is working to accelerate gen AI and computational mathematics. Editor&#x27;s note: The opinions expressed in this article are the authors&#x27; personal opinions and do not reflect the opinions of their employers.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2lIgm3tWKNlbNcAqMNsFXB/d06b37a9da80c563b72a3c9311f38031/Audit_loop.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/how-to-send-a-message-via-satellite-on-iphone-130000418.html",
          "published_at": "Sun, 22 Feb 2026 13:00:00 +0000",
          "title": "How to send a message via satellite on iPhone",
          "standfirst": "Apple’s satellite features were originally designed for emergencies, allowing iPhone users to contact emergency services when cellular and Wi-Fi coverage is unavailable. With recent versions of iOS, Apple has expanded those capabilities to include sending and receiving messages via satellite. This makes it possible to stay in touch with friends and family from remote locations where traditional networks do not reach, such as hiking trails, rural areas or offshore locations.Messaging via satellite is built directly into the iPhone and works automatically when no cellular or Wi-Fi signal is available. While it is not intended to replace regular messaging, it can be a useful backup when coverage drops.How to send a message via satelliteBefore you can get started, you’ll need to turn on iMessage before you’re off the grid. It’s also important to set up an emergency contact as well as members of your Family Sharing group prior to your departure. This will enable them to message you via SMS without the need to message them first. To send a message via satellite, open the Messages app when no cellular or Wi-Fi signal is available. If the feature is supported in the current location, the app will display a prompt indicating that satellite messaging is available.Selecting the option to connect via satellite launches a guided connection screen. Your iPhone will provide real-time instructions to help maintain alignment with the satellite. Once connected, a text message can be typed and sent, although delivery may take longer than usual.The iPhone will notify you when the message has been sent successfully. Replies from the recipient will also be delivered via satellite, as long as the connection remains active.What you need before you can send satellite messagesSending messages via satellite requires a compatible iPhone model and the correct software version. The feature is supported on iPhone models with satellite hardware, beginning with iPhone 14 and later. The device must be running a version of iOS (iOS 18 or higher) that supports satellite messaging, which Apple has continued to refine through recent iOS updates.The feature also depends on location and availability. Satellite messaging is currently supported in select regions, including the United States and parts of Canada, with expanded support rolling out gradually. The iPhone must be outdoors with a clear view of the sky, as trees, buildings and terrain can interfere with the satellite connection.Satellite messaging is not designed for continuous conversations. Messages are compressed and sent at a slower pace than standard texts, and delivery times can vary depending on conditions and satellite positioning.How satellite messaging works on iPhoneWhen an iPhone loses access to cellular and Wi-Fi networks, the system automatically detects that only satellite connectivity is available. Instead of failing to send, the Messages app prompts the user to connect to a satellite.On-screen instructions guide the user to position the phone correctly. This typically involves holding the device up and following directional prompts to align it with an overhead satellite. The phone uses built-in sensors to help maintain the connection while the message is being sent.Messages sent via satellite are text-only and use a reduced data format to ensure they can be transmitted reliably. Images, videos, audio messages and large attachments are not supported.Who can receive satellite messages?Satellite messages can be sent to contacts using iMessage or standard SMS, depending on the recipient’s device and settings. If the recipient is using an Apple device with iMessage enabled, the message will be delivered through Apple’s messaging system. If not, the message will be sent as a standard text.Recipients do not need a satellite-capable device to receive messages. From their perspective, the message appears similar to a regular text, though delivery times may be longer.Tips for getting a reliable connectionA clear view of the sky is essential for satellite messaging to work properly. Open areas with minimal obstructions offer the best results. Movement, heavy foliage and nearby structures can interrupt the connection.Because satellite bandwidth is limited, keeping messages short improves reliability and delivery speed. The iPhone may prompt the user to edit longer messages to fit within satellite constraints.Battery life is also a consideration. Maintaining a satellite connection uses more power than standard messaging, so it helps to conserve battery when relying on satellite features for extended periods.Limitations to keep in mindSatellite messaging is designed for occasional use when other networks are unavailable. It does not support group messages, media attachments or read receipts in the same way as standard messaging.Delivery times can range from under a minute to several minutes, depending on environmental conditions and satellite availability. The feature should not be relied upon for time-sensitive communication unless no other option is available.Apple has also noted that satellite features may be offered free for a limited period, with potential pricing or subscription requirements introduced in the future depending on region and carrier arrangements.When satellite messaging can be usefulMessaging via satellite can be helpful for travelers, hikers and anyone spending time in remote areas where coverage is unreliable. It offers a way to check in, share basic updates or request non-emergency assistance when traditional networks are unavailable.While it is not a replacement for emergency services, it complements Apple’s existing emergency satellite features by providing an additional communication option when users are off the grid.As Apple continues to expand satellite support, messaging via satellite is likely to become a more familiar part of the iPhone experience, particularly for users who regularly venture beyond the reach of cellular networks.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/how-to-send-a-message-via-satellite-on-iphone-130000418.html?src=rss",
          "content": "Apple’s satellite features were originally designed for emergencies, allowing iPhone users to contact emergency services when cellular and Wi-Fi coverage is unavailable. With recent versions of iOS, Apple has expanded those capabilities to include sending and receiving messages via satellite. This makes it possible to stay in touch with friends and family from remote locations where traditional networks do not reach, such as hiking trails, rural areas or offshore locations.Messaging via satellite is built directly into the iPhone and works automatically when no cellular or Wi-Fi signal is available. While it is not intended to replace regular messaging, it can be a useful backup when coverage drops.How to send a message via satelliteBefore you can get started, you’ll need to turn on iMessage before you’re off the grid. It’s also important to set up an emergency contact as well as members of your Family Sharing group prior to your departure. This will enable them to message you via SMS without the need to message them first. To send a message via satellite, open the Messages app when no cellular or Wi-Fi signal is available. If the feature is supported in the current location, the app will display a prompt indicating that satellite messaging is available.Selecting the option to connect via satellite launches a guided connection screen. Your iPhone will provide real-time instructions to help maintain alignment with the satellite. Once connected, a text message can be typed and sent, although delivery may take longer than usual.The iPhone will notify you when the message has been sent successfully. Replies from the recipient will also be delivered via satellite, as long as the connection remains active.What you need before you can send satellite messagesSending messages via satellite requires a compatible iPhone model and the correct software version. The feature is supported on iPhone models with satellite hardware, beginning with iPhone 14 and later. The device must be running a version of iOS (iOS 18 or higher) that supports satellite messaging, which Apple has continued to refine through recent iOS updates.The feature also depends on location and availability. Satellite messaging is currently supported in select regions, including the United States and parts of Canada, with expanded support rolling out gradually. The iPhone must be outdoors with a clear view of the sky, as trees, buildings and terrain can interfere with the satellite connection.Satellite messaging is not designed for continuous conversations. Messages are compressed and sent at a slower pace than standard texts, and delivery times can vary depending on conditions and satellite positioning.How satellite messaging works on iPhoneWhen an iPhone loses access to cellular and Wi-Fi networks, the system automatically detects that only satellite connectivity is available. Instead of failing to send, the Messages app prompts the user to connect to a satellite.On-screen instructions guide the user to position the phone correctly. This typically involves holding the device up and following directional prompts to align it with an overhead satellite. The phone uses built-in sensors to help maintain the connection while the message is being sent.Messages sent via satellite are text-only and use a reduced data format to ensure they can be transmitted reliably. Images, videos, audio messages and large attachments are not supported.Who can receive satellite messages?Satellite messages can be sent to contacts using iMessage or standard SMS, depending on the recipient’s device and settings. If the recipient is using an Apple device with iMessage enabled, the message will be delivered through Apple’s messaging system. If not, the message will be sent as a standard text.Recipients do not need a satellite-capable device to receive messages. From their perspective, the message appears similar to a regular text, though delivery times may be longer.Tips for getting a reliable connectionA clear view of the sky is essential for satellite messaging to work properly. Open areas with minimal obstructions offer the best results. Movement, heavy foliage and nearby structures can interrupt the connection.Because satellite bandwidth is limited, keeping messages short improves reliability and delivery speed. The iPhone may prompt the user to edit longer messages to fit within satellite constraints.Battery life is also a consideration. Maintaining a satellite connection uses more power than standard messaging, so it helps to conserve battery when relying on satellite features for extended periods.Limitations to keep in mindSatellite messaging is designed for occasional use when other networks are unavailable. It does not support group messages, media attachments or read receipts in the same way as standard messaging.Delivery times can range from under a minute to several minutes, depending on environmental conditions and satellite availability. The feature should not be relied upon for time-sensitive communication unless no other option is available.Apple has also noted that satellite features may be offered free for a limited period, with potential pricing or subscription requirements introduced in the future depending on region and carrier arrangements.When satellite messaging can be usefulMessaging via satellite can be helpful for travelers, hikers and anyone spending time in remote areas where coverage is unreliable. It offers a way to check in, share basic updates or request non-emergency assistance when traditional networks are unavailable.While it is not a replacement for emergency services, it complements Apple’s existing emergency satellite features by providing an additional communication option when users are off the grid.As Apple continues to expand satellite support, messaging via satellite is likely to become a more familiar part of the iPhone experience, particularly for users who regularly venture beyond the reach of cellular networks.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/how-to-send-a-message-via-satellite-on-iphone-130000418.html?src=rss",
          "feed_position": 24
        }
      ],
      "featured_image": "https://s.yimg.com/os/creatr-uploaded-images/2021-04/02284fb0-a11a-11eb-aafb-70e6f3b5aa36",
      "popularity_score": 2018.419946388889
    },
    {
      "id": "cluster_23",
      "coverage": 2,
      "updated_at": "Tue, 24 Feb 2026 01:05:00 -0500",
      "title": "Bengaluru-based Xflow, which facilitates B2B cross-border payments, raised a $16.6M Series A led by GC at an $85M valuation, bringing its total raised to $32M+ (Jagmeet Singh/TechCrunch)",
      "neutral_headline": "Stripe, PayPal Ventures bet on India&#8217;s Xflow to fix cross-border B2B payments",
      "bullet_summary": [
        "Reported by TechMeme, TechCrunch"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260224/p2#a260224p2",
          "published_at": "Tue, 24 Feb 2026 01:05:00 -0500",
          "title": "Bengaluru-based Xflow, which facilitates B2B cross-border payments, raised a $16.6M Series A led by GC at an $85M valuation, bringing its total raised to $32M+ (Jagmeet Singh/TechCrunch)",
          "standfirst": "Jagmeet Singh / TechCrunch: Bengaluru-based Xflow, which facilitates B2B cross-border payments, raised a $16.6M Series A led by GC at an $85M valuation, bringing its total raised to $32M+ &mdash; Xflow, an Indian fintech startup, has secured backing from both Stripe and PayPal Ventures in a $16.6 million funding round.",
          "content": "Jagmeet Singh / TechCrunch: Bengaluru-based Xflow, which facilitates B2B cross-border payments, raised a $16.6M Series A led by GC at an $85M valuation, bringing its total raised to $32M+ &mdash; Xflow, an Indian fintech startup, has secured backing from both Stripe and PayPal Ventures in a $16.6 million funding round.",
          "feed_position": 7,
          "image_url": "http://www.techmeme.com/260224/i2.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/02/23/stripe-paypal-ventures-bet-on-indias-xflow-to-fix-cross-border-b2b-payments/",
          "published_at": "Tue, 24 Feb 2026 05:30:00 +0000",
          "title": "Stripe, PayPal Ventures bet on India&#8217;s Xflow to fix cross-border B2B payments",
          "standfirst": "Stripe and PayPal Ventures have participated in Xflow's $16.6 million round that gives it a post-money valuation of $85 million.",
          "content": "Stripe and PayPal Ventures have participated in Xflow's $16.6 million round that gives it a post-money valuation of $85 million.",
          "feed_position": 1
        }
      ],
      "featured_image": "http://www.techmeme.com/260224/i2.jpg",
      "popularity_score": 2014.4932797222223
    },
    {
      "id": "cluster_30",
      "coverage": 2,
      "updated_at": "Mon, 23 Feb 2026 19:50:01 -0500",
      "title": "Tesla sues California's DMV to reverse a ruling that found Tesla violated the law by falsely promoting its Autopilot and Full Self-Driving systems (Lora Kolodny/CNBC)",
      "neutral_headline": "Tesla sues California's DMV to reverse a ruling that found Tesla violated the law by falsely promoting...",
      "bullet_summary": [
        "Reported by TechMeme, TechCrunch"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260223/p32#a260223p32",
          "published_at": "Mon, 23 Feb 2026 19:50:01 -0500",
          "title": "Tesla sues California's DMV to reverse a ruling that found Tesla violated the law by falsely promoting its Autopilot and Full Self-Driving systems (Lora Kolodny/CNBC)",
          "standfirst": "Lora Kolodny / CNBC: Tesla sues California's DMV to reverse a ruling that found Tesla violated the law by falsely promoting its Autopilot and Full Self-Driving systems &mdash; Tesla is suing California's Department of Motor Vehicles to reverse a ruling that found the automaker violated the law by falsely promoting its cars' self-driving capabilities.",
          "content": "Lora Kolodny / CNBC: Tesla sues California's DMV to reverse a ruling that found Tesla violated the law by falsely promoting its Autopilot and Full Self-Driving systems &mdash; Tesla is suing California's Department of Motor Vehicles to reverse a ruling that found the automaker violated the law by falsely promoting its cars' self-driving capabilities.",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/260223/i32.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/02/23/teslas-battle-with-the-california-department-of-motor-vehicles-isnt-over-after-all/",
          "published_at": "Tue, 24 Feb 2026 00:36:12 +0000",
          "title": "Tesla&#8217;s battle with the California Department of Motor Vehicles isn&#8217;t over after all",
          "standfirst": "Tesla has filed a lawsuit against the California DMV in the ongoing battle around Autopilot.",
          "content": "Tesla has filed a lawsuit against the California DMV in the ongoing battle around Autopilot.",
          "feed_position": 3
        }
      ],
      "featured_image": "http://www.techmeme.com/260223/i32.jpg",
      "popularity_score": 2009.2435575
    },
    {
      "id": "cluster_38",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 22:14:28 +0000",
      "title": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
      "neutral_headline": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/02/pentagon-buyer-were-happy-with-our-launch-industry-but-payloads-are-lagging/",
          "published_at": "Mon, 23 Feb 2026 22:14:28 +0000",
          "title": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
          "standfirst": "\"The point is to get missions out the door as fast as possible. Two to three years is too slow.\"",
          "content": "DALLAS—The Space Force officer tasked with overseeing more than $24 billion in research and development spending says the Pentagon is more interested in supporting startups building new space sensors and payloads than adding yet another rocket company to its portfolio. The statement, made at a space finance conference in Dallas last week, was one of several points Maj. Gen. Stephen Purdy wanted to get across to a room full of investors and commercial space executives. The other points on Purdy's agenda were that the Space Force is more interested in high-volume production than spending money to develop the latest technologies, and that the military has, at least for now, lost one of its most important tools for supporting and diversifying the space industrial base.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1348666509-1152x648-1771883779.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1348666509-1152x648-1771883779.jpg",
      "popularity_score": 339.6510575
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 17:45:29 +0000",
      "title": "New Microsoft gaming chief has \"no tolerance for bad AI\"",
      "neutral_headline": "New Microsoft gaming chief has \"no tolerance for bad AI\"",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/02/new-microsoft-gaming-chief-has-no-tolerance-for-bad-ai/",
          "published_at": "Mon, 23 Feb 2026 17:45:29 +0000",
          "title": "New Microsoft gaming chief has \"no tolerance for bad AI\"",
          "standfirst": "But Asha Sharma faces scrutiny for lack of gaming experience.",
          "content": "Last week's surprise departure of Phil Spencer from Microsoft led to the promotion of Asha Sharma, who comes to head Microsoft's gaming division after two years as president of the company's CoreAI Product group. Despite that recent history, Sharma says in a new interview that she has \"no tolerance for bad AI\" in game development. Speaking with Variety, Sharma noted that \"AI has long been part of gaming and will continue to be,\" before adding that \"great stories are created by humans.\" The interview comes after Sharma promised in an introductory memo: \"We will not chase short-term efficiency or flood our ecosystem with soulless AI slop. Games are and always will be art, crafted by humans, and created with the most innovative technology provided by us.\" Those statements seem like a clear line in the sand from Sharma against the use of AI tools in Microsoft's first-party game development, at the very least. But what separates \"bad AI\" and \"soulless AI slop\" from \"innovative technology\" that humans can use to create artful games is a matter of some significant debate in the gaming world.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-1152x648.jpg",
      "popularity_score": 332.1680019444444
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 21:48:44 +0000",
      "title": "Data center builders thought farmers would willingly sell land, learn otherwise",
      "neutral_headline": "Data center builders thought farmers would willingly sell land, learn otherwise",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/",
          "published_at": "Mon, 23 Feb 2026 21:48:44 +0000",
          "title": "Data center builders thought farmers would willingly sell land, learn otherwise",
          "standfirst": "Even in a fragile farm economy, million-dollar offers can't sway dedicated farmers.",
          "content": "It seems that tech giants eyeing rural zones for data center development have underestimated how attached American farmers have grown to their lands in the decades they've been nurturing them. Across the country, several farmers have firmly rejected eye-popping offers—sometimes in the tens of millions. These offers dwarf the value of their properties, but farmers have refused to put a price on the lands that they love most. In a report on Monday, The Guardian highlighted a handful of cases nationwide where farmers' refusals have frustrated plans to build data centers in areas long deemed rural.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg",
      "popularity_score": 329.2221686111111
    },
    {
      "id": "cluster_45",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 21:16:20 +0000",
      "title": "Panasonic, the former plasma king, will no longer make its own TVs",
      "neutral_headline": "Panasonic, the former plasma king, will no longer make its own TVs",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/panasonic-the-former-plasma-king-will-no-longer-make-its-own-tvs/",
          "published_at": "Mon, 23 Feb 2026 21:16:20 +0000",
          "title": "Panasonic, the former plasma king, will no longer make its own TVs",
          "standfirst": "Panasonic was one of the last Japanese companies still manufacturing TVs.",
          "content": "Panasonic, once revered for its plasma TVs, is giving up on making its own TV sets. Today, it announced that Chinese company Skyworth will take over manufacturing, marketing, and selling Panasonic-branded TVs. Skyworth is a Shenzhen-headquartered TV brand. The company claims to be “a top three global provider of the Android TV platform.” In July, research firm Omdia reported that Skyworth was one of the top-five TV brands by sales revenue in Q1 2025; however, Skyworth hasn’t been able to maintain that position regularly. Panasonic made its announcement at a \"launch event,” FlatpanelsHD reported today. During the event, a Panasonic representative reportedly said:Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-56528381-1152x648-1771879994.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-56528381-1152x648-1771879994.jpg",
      "popularity_score": 318.68216861111114
    },
    {
      "id": "cluster_87",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 15:38:00 +0000",
      "title": "AIs can generate near-verbatim copies of novels from training data",
      "neutral_headline": "AIs can generate near-verbatim copies of novels from training data",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/ais-can-generate-near-verbatim-copies-of-novels-from-training-data/",
          "published_at": "Mon, 23 Feb 2026 15:38:00 +0000",
          "title": "AIs can generate near-verbatim copies of novels from training data",
          "standfirst": "LLMs memorize more training data than previously thought.",
          "content": "The world’s top AI models can be prompted to generate near-verbatim copies of bestselling novels, raising fresh questions about the industry’s claim that its systems do not store copyrighted works. A series of recent studies has shown that large language models from OpenAI, Google, Meta, Anthropic, and xAI memorize far more of their training data than previously thought. AI and legal experts told the FT this “memorization” ability could have serious ramifications on AI groups’ battle against dozens of copyright lawsuits around the world, as it undermines their core defense that LLMs “learn” from copyrighted works but do not store copies.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-1152x648-1768598730.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-1152x648-1768598730.jpg",
      "popularity_score": 298.0432797222222
    },
    {
      "id": "cluster_76",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 17:00:02 +0000",
      "title": "The 2026 Mazda CX-5, driven: It got bigger; plus, radical tech upgrade",
      "neutral_headline": "The 2026 Mazda CX-5, driven: It got bigger; plus, radical tech upgrade",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/02/the-2026-mazda-cx-5-driven-it-got-bigger-plus-radical-tech-upgrade/",
          "published_at": "Mon, 23 Feb 2026 17:00:02 +0000",
          "title": "The 2026 Mazda CX-5, driven: It got bigger; plus, radical tech upgrade",
          "standfirst": "Starting at $29,990, there's a lot to like about the all-new Mazda, but it's not perfect.",
          "content": "Mazda provided flights from Washington, DC, to San Diego and accommodation so Ars could drive the CX-5. Ars does not accept paid editorial content. ENCINITAS, Calif.—Its sales may have been buoyed of late by the big CX-90 and CX-70 SUVs, but for Mazda, the CX-5 is still where most of the action is. Unlike the similar-sized, similar-priced CX-50, which was designed just for North America, the all-new CX-5 is a global car, and it's also Mazda's standard-bearer for a range of new technologies. Gone is the basic but effective infotainment system, replaced by an all-new Google-based experience as Mazda starts its journey toward software-defined vehicles. There's even an in-house hybrid on the way, albeit not until next year. And it starts at a competitive $29,990. The new CX-5 is bigger than the car it replaces, 4.5 inches (114.5 mm) longer and half an inch (13 mm) wider than before, at 184.6 inches (4,689 mm) long, 73.2 inches (1,859 mm) wide, and 66.7 inches (1,694 mm) tall. Much of that extra space is between the axles—the wheelbase is now 110 inches (2,794 mm) long, which translates to more interior space. From the outside, there's a new light signature, and the way the bodywork curves around the front and wraps down the fenders gives me strong Range Rover vibes, even if I could never adequately capture what I'm talking about with a camera. As ever, Mazda's arresting Soul Red Crystal metallic paint (a $595 option) sparkles, even on a day when the sun remained hidden from view. The last time that Mazda evolved this compact crossover, it did so with a new upmarket interior. Since then, the brand has staked out that space across its model lineup, with cabins that punch well above their price tags. Happily, the company's designers haven't lost much mojo since then, with a restrained approach that looks good across the five different trim levels, each of which is a $2,000 step up from the one that precedes it. But if you're a current CX-5 driver, you'll find much has changed, perhaps not entirely for the better.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/2026-Mazda-CX-5-1-1152x648-1771861550.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/2026-Mazda-CX-5-1-1152x648-1771861550.jpg",
      "popularity_score": 294.41050194444443
    },
    {
      "id": "cluster_93",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 13:51:36 +0000",
      "title": "Review: Knight of the Seven Kingdoms brings back that Westeros magic",
      "neutral_headline": "Review: Knight of the Seven Kingdoms brings back that Westeros magic",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/02/review-knight-of-the-seven-kingdoms-brings-back-that-westeros-magic/",
          "published_at": "Mon, 23 Feb 2026 13:51:36 +0000",
          "title": "Review: Knight of the Seven Kingdoms brings back that Westeros magic",
          "standfirst": "Prequel series is just great storytelling, reminding GoT fans why they loved the original so much.",
          "content": "HBO has another critically acclaimed hit with A Knight of the Seven Kingdoms, based on George R.R. Martin’s Tales of Dunk and Egg novellas, and it deserves every bit of the praise heaped upon it. The immensely satisfying first season wrapped with last night's finale, dealing with the tragedy of the penultimate episode and setting the stage for the further adventures of Dunk and Egg. House of the Dragon is a solid series, but Knight of the Seven Kingdoms has reminded staunch GoT fans of everything they loved about the original series in the first place. (Spoilers below, but no major reveals until after the second gallery. We'll give you a heads up when we get there.) A Knight of the Seven Kingdoms adapts the first novella in the series, The Hedge Knight, and is set more than 50 years after the events of House of the Dragon. Dunk (Peter Claffey) is a lowly hedge knight who has just buried his aged mentor, Ser Arlan of Pennytree (Danny Webb). Ser Arlan was perhaps not the kindest of mentors and often stone drunk, but at least he was hung like the proverbial horse—as viewers discovered in a full-frontal moment that instantly went viral. Lacking any good employment options, Dunk decides to enter a local tournament, since he has inherited Ser Arlan's sword, shield, and three horses.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/dunkTOP-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/dunkTOP-1152x648.jpg",
      "popularity_score": 284
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 12:00:45 +0000",
      "title": "The first cars bold enough to drive themselves",
      "neutral_headline": "The first cars bold enough to drive themselves",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/features/2026/02/the-first-cars-bold-enough-to-drive-themselves/",
          "published_at": "Mon, 23 Feb 2026 12:00:45 +0000",
          "title": "The first cars bold enough to drive themselves",
          "standfirst": "Quevedo's telekino of 1904 was the first step on the road to autonomous Waymos.",
          "content": "No one knows exactly when the vehicles we drive will finally wrest the steering wheel from us. But the age of the autonomous automobile isn’t some sudden Big Bang. It’s more of a slow crawl, one that started during the Roosevelt administration. And that’s Theodore, not Franklin. And not in America, but in Spain, by someone you’ve probably never heard of. His name was Leonardo Torres Quevedo, a Spanish engineer born in Santa Cruz, Spain, in 1852. Smart? In 1914, he developed a mechanical chess machine that autonomously played against humans. But more than a decade earlier, he pioneered the development of remote-control systems. What he wrought was brilliant, if crude—and certainly ahead of its time. The first wireless control It was called the Telekino, a name drawn from the Greek “tele,” meaning at a distance, and “kino,” meaning movement. Patented in Spain, France, and the United States, it was conceived as a way to prevent airship accidents. The Telekino transmitted wireless signals to a small receiver known as a coherer, which detected electromagnetic waves and transformed them into an electrical current. This current was amplified and sent on to electromagnets that slowly rotated a switch controlling the proper servomotor. Quevedo could issue 19 distinct commands to the systems of an airship without ever touching a control cable.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/radio-controlled-vintage-cars-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/radio-controlled-vintage-cars-1152x648.jpg",
      "popularity_score": 260
    }
  ]
}