{
  "updated_at": "2026-02-24T19:45:17.681Z",
  "clusters": [
    {
      "id": "cluster_0",
      "coverage": 3,
      "updated_at": "Tue, 24 Feb 2026 14:40:01 -0500",
      "title": "YouTube updates its $7.99 per month Premium Lite subscription to let users download videos for offline access and play videos in the background (Sarah Perez/TechCrunch)",
      "neutral_headline": "YouTube&#8217;s cheaper subscription is getting background play and downloads",
      "bullet_summary": [
        "Reported by TechMeme, The Verge, TechCrunch"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260224/p39#a260224p39",
          "published_at": "Tue, 24 Feb 2026 14:40:01 -0500",
          "title": "YouTube updates its $7.99 per month Premium Lite subscription to let users download videos for offline access and play videos in the background (Sarah Perez/TechCrunch)",
          "standfirst": "Sarah Perez / TechCrunch: YouTube updates its $7.99 per month Premium Lite subscription to let users download videos for offline access and play videos in the background &mdash; YouTube is expanding its more affordable, $7.99 per month Premium Lite subscription service with new features, including the ability to download videos &hellip;",
          "content": "Sarah Perez / TechCrunch: YouTube updates its $7.99 per month Premium Lite subscription to let users download videos for offline access and play videos in the background &mdash; YouTube is expanding its more affordable, $7.99 per month Premium Lite subscription service with new features, including the ability to download videos &hellip;",
          "feed_position": 0,
          "image_url": "http://www.techmeme.com/260224/i39.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/streaming/883316/youtube-premium-lite-background-play-offline-downloads",
          "published_at": "2026-02-24T12:00:00-05:00",
          "title": "YouTube&#8217;s cheaper subscription is getting background play and downloads",
          "standfirst": "YouTube is adding background playback and offline downloads to YouTube Premium Lite, its $7.99-per-month subscription. With the change, you'll be able to keep YouTube videos going while you use another app on your phone or save videos to watch when you're offline. YouTube Premium Lite came to the US just about a year ago as [&#8230;]",
          "content": "YouTube is adding background playback and offline downloads to YouTube Premium Lite, its $7.99-per-month subscription. With the change, you'll be able to keep YouTube videos going while you use another app on your phone or save videos to watch when you're offline. YouTube Premium Lite came to the US just about a year ago as a more affordable way to watch YouTube with fewer ads, but it lacked background playback and offline downloads. In its blog post, the company said that it had \"heard feedback about wanting these additional features included in the service.\" The features are rolling out beginning Tuesday and will be available everywhere … Read the full story at The Verge.",
          "feed_position": 7
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/02/24/youtube-beefs-up-its-7-99-month-lite-subscription-with-downloads-and-background-play/",
          "published_at": "Tue, 24 Feb 2026 17:00:00 +0000",
          "title": "YouTube beefs up its $7.99/month Lite subscription with offline downloads and background play",
          "standfirst": "Now the only reason to get the full Premium subscription is ad-free music and music videos.",
          "content": "Now the only reason to get the full Premium subscription is ad-free music and music videos.",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/260224/i39.jpg",
      "popularity_score": 3019.9120330555556
    },
    {
      "id": "cluster_24",
      "coverage": 2,
      "updated_at": "Tue, 24 Feb 2026 18:16:43 +0000",
      "title": "iPhone Fold rumors: Everything we know right now, including the leaked design, upgrades, price and more",
      "neutral_headline": "Seattle Ultrasonics C-200 review: This is the future of kitchen knives",
      "bullet_summary": [
        "Apple still hasn’t revealed a foldable iPhone, but the steady drip of leaks suggests the project is moving closer to reality",
        "While Apple hasn’t confirmed anything publicly, the overall picture is starting to look more consistent",
        "The goal is a display with a nearly invisible crease, something Apple reportedly considers essential before entering the foldable market",
        "Liquidmetal is said to be stronger and more resistant to deformation than titanium, while remaining relatively lightweight"
      ],
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/iphone-fold-rumors-everything-we-know-right-now-including-the-leaked-design-upgrades-price-and-more-130000733.html",
          "published_at": "Tue, 24 Feb 2026 18:16:43 +0000",
          "title": "iPhone Fold rumors: Everything we know right now, including the leaked design, upgrades, price and more",
          "standfirst": "Apple still hasn’t revealed a foldable iPhone, but the steady drip of leaks suggests the project is moving closer to reality. Over the past few months, analysts and supply-chain watchers have continued to fill in key details, with most reports still pointing to a launch sometime in the second half of 2026. While Apple hasn’t confirmed anything publicly, the overall picture is starting to look more consistent.As always, plans for unreleased Apple hardware can change at any time. Features may shift, timelines can slip and some prototypes may never ship. Even so, recent reporting gives us the clearest sense yet of how Apple’s first foldable could take shape and where it might fit in the broader iPhone lineup.Below, we’ve rounded up the most credible rumors so far, and we’ll keep this guide updated as new details emerge.When could the iPhone Fold launch?Rumors of a foldable iPhone date back as far as 2017, but more recent reporting suggests Apple has finally locked onto a realistic window. Most sources now point to fall 2026, likely alongside the iPhone 18 lineup, with some supply-chain hints suggesting mass production could begin in mid-2026 if development stays on track.Mark Gurman has gone back and forth on timing, initially suggesting Apple could launch “as early as 2026,” before later writing that the device would ship at the end of 2026 and sell primarily in 2027. Analyst Ming-Chi Kuo has also repeatedly cited the second half of 2026 as Apple’s target.Some reports still claim the project could slip into 2027 if Apple runs into manufacturing or durability issues, particularly around the hinge or display. Given Apple’s history of delaying products that it feels aren’t ready, that remains a real possibility.What will the iPhone Fold look like?Current consensus suggests Apple has settled on a book-style foldable design, similar to Samsung’s Galaxy Z Fold series, rather than a clamshell flip phone.When unfolded, the iPhone Fold is expected to resemble a small tablet like the iPad mini (8.3 inches). Based on the rumor mill, though, the iPhone Fold may be a touch smaller, with an internal display measuring around 7.7 to 7.8 inches. When closed, it should function like a conventional smartphone, with an outer display in the 5.5-inch range.CAD leaks and alleged case-maker molds suggest the device may be shorter and wider than a standard iPhone when folded, creating a squarer footprint that better matches the aspect ratio of the inner display. Several reports have also pointed to the iPhone Air as a potential preview of Apple’s foldable design work, with its unusually thin chassis widely interpreted as a look at what one half of a future foldable iPhone could resemble.If that theory holds, it could help explain the Fold’s rumored dimensions. Thickness is expected to land around 4.5 to 4.8mm when unfolded, according to analyst Ming-Chi Kuo, putting it in a similar range to the iPhone Air, and roughly 9 to 9.5mm when folded, depending on the final hinge design and internal layering.iPhone 17 Pro, iPhone AirEngadgetDisplay and the crease questionThe display is arguably the biggest challenge for any foldable phone, and it’s an area where Apple appears to have invested years of development.Multiple reports say Apple will rely on Samsung Display as its primary supplier. At CES 2026, Samsung showcased a new crease-less foldable OLED panel, which several sources — including Bloomberg — suggested could be the same technology Apple plans to use.According to these reports, the panel combines a flexible OLED with a laser-drilled metal support plate that disperses stress when folding. The goal is a display with a nearly invisible crease, something Apple reportedly considers essential before entering the foldable market.If Apple does use this panel, it would mark a notable improvement over current foldables, which still show visible creasing under certain lighting conditions.Cameras and biometricsCamera rumors suggest Apple is planning a four-camera setup. That may include:Two rear cameras (main and ultra-wide, both rumored at 48MP)One punch-hole camera on the outer displayOne under-display camera on the inner screenSeveral sources claim Apple will avoid Face ID entirely on the iPhone Fold. Instead, it’s expected to rely on Touch ID built into the power button, similar to recent iPad models. This would allow Apple to keep both displays free of notches or Dynamic Island cutouts.Under-display camera technology has historically produced lower image quality, but a rumored 24MP sensor would be a significant step up compared to existing foldables, which typically use much lower-resolution sensors.iPhone Fold’s hinge and materialsThe hinge is another area where Apple may diverge from competitors. Multiple reports claim Apple will use Liquidmetal, which is a long-standing trade name for a metallic glass alloy the company has previously used in smaller components. While often referred to as “liquid metal” or “Liquid Metal” in reports, Liquidmetal is the branding Apple has historically associated with the material.Liquidmetal is said to be stronger and more resistant to deformation than titanium, while remaining relatively lightweight. If accurate, this could help improve long-term durability and reduce wear on the foldable display.Leaks from Jon Prosser also reference a metal plate beneath the display that works in tandem with the hinge to minimize creasing — a claim that aligns with reporting from Korean and Chinese supply-chain sources.Battery and other components Battery life is another potential differentiator. According to Ming-Chi Kuo and multiple Asian supply-chain reports, Apple is testing high-density battery cells in the 5,000 to 5,800mAh range.That would make it the largest battery ever used in an iPhone, and competitive with (or larger than) batteries in current Android foldables. The device is also expected to use a future A-series chip and Apple’s in-house modem, with some reports pointing specifically to a next-generation C2 modem as part of Apple’s broader push to reduce reliance on Qualcomm.PriceNone of this will come cheap, that’s for certain. Nearly every report agrees that the iPhone Fold will be Apple’s most expensive iPhone ever.Estimates currently place the price between $2,000 and $2,500 in the US. Bloomberg has said the price will be “at least $2,000,” while other analysts have narrowed the likely range to around $2,100 and $2,300. That positions the iPhone Fold well above the iPhone Pro Max and closer to Apple’s high-end Macs and iPads.Despite years of rumors, there’s still plenty that remains unclear. Apple hasn’t confirmed the name “iPhone Fold,” final dimensions, software features or how iOS would adapt to a folding form factor. Durability, repairability and long-term reliability are also open questions. For now, the safest assumption is that Apple is taking its time and that many of these details could still change before launch. This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/iphone-fold-rumors-everything-we-know-right-now-including-the-leaked-design-upgrades-price-and-more-130000733.html?src=rss",
          "content": "Apple still hasn’t revealed a foldable iPhone, but the steady drip of leaks suggests the project is moving closer to reality. Over the past few months, analysts and supply-chain watchers have continued to fill in key details, with most reports still pointing to a launch sometime in the second half of 2026. While Apple hasn’t confirmed anything publicly, the overall picture is starting to look more consistent.As always, plans for unreleased Apple hardware can change at any time. Features may shift, timelines can slip and some prototypes may never ship. Even so, recent reporting gives us the clearest sense yet of how Apple’s first foldable could take shape and where it might fit in the broader iPhone lineup.Below, we’ve rounded up the most credible rumors so far, and we’ll keep this guide updated as new details emerge.When could the iPhone Fold launch?Rumors of a foldable iPhone date back as far as 2017, but more recent reporting suggests Apple has finally locked onto a realistic window. Most sources now point to fall 2026, likely alongside the iPhone 18 lineup, with some supply-chain hints suggesting mass production could begin in mid-2026 if development stays on track.Mark Gurman has gone back and forth on timing, initially suggesting Apple could launch “as early as 2026,” before later writing that the device would ship at the end of 2026 and sell primarily in 2027. Analyst Ming-Chi Kuo has also repeatedly cited the second half of 2026 as Apple’s target.Some reports still claim the project could slip into 2027 if Apple runs into manufacturing or durability issues, particularly around the hinge or display. Given Apple’s history of delaying products that it feels aren’t ready, that remains a real possibility.What will the iPhone Fold look like?Current consensus suggests Apple has settled on a book-style foldable design, similar to Samsung’s Galaxy Z Fold series, rather than a clamshell flip phone.When unfolded, the iPhone Fold is expected to resemble a small tablet like the iPad mini (8.3 inches). Based on the rumor mill, though, the iPhone Fold may be a touch smaller, with an internal display measuring around 7.7 to 7.8 inches. When closed, it should function like a conventional smartphone, with an outer display in the 5.5-inch range.CAD leaks and alleged case-maker molds suggest the device may be shorter and wider than a standard iPhone when folded, creating a squarer footprint that better matches the aspect ratio of the inner display. Several reports have also pointed to the iPhone Air as a potential preview of Apple’s foldable design work, with its unusually thin chassis widely interpreted as a look at what one half of a future foldable iPhone could resemble.If that theory holds, it could help explain the Fold’s rumored dimensions. Thickness is expected to land around 4.5 to 4.8mm when unfolded, according to analyst Ming-Chi Kuo, putting it in a similar range to the iPhone Air, and roughly 9 to 9.5mm when folded, depending on the final hinge design and internal layering.iPhone 17 Pro, iPhone AirEngadgetDisplay and the crease questionThe display is arguably the biggest challenge for any foldable phone, and it’s an area where Apple appears to have invested years of development.Multiple reports say Apple will rely on Samsung Display as its primary supplier. At CES 2026, Samsung showcased a new crease-less foldable OLED panel, which several sources — including Bloomberg — suggested could be the same technology Apple plans to use.According to these reports, the panel combines a flexible OLED with a laser-drilled metal support plate that disperses stress when folding. The goal is a display with a nearly invisible crease, something Apple reportedly considers essential before entering the foldable market.If Apple does use this panel, it would mark a notable improvement over current foldables, which still show visible creasing under certain lighting conditions.Cameras and biometricsCamera rumors suggest Apple is planning a four-camera setup. That may include:Two rear cameras (main and ultra-wide, both rumored at 48MP)One punch-hole camera on the outer displayOne under-display camera on the inner screenSeveral sources claim Apple will avoid Face ID entirely on the iPhone Fold. Instead, it’s expected to rely on Touch ID built into the power button, similar to recent iPad models. This would allow Apple to keep both displays free of notches or Dynamic Island cutouts.Under-display camera technology has historically produced lower image quality, but a rumored 24MP sensor would be a significant step up compared to existing foldables, which typically use much lower-resolution sensors.iPhone Fold’s hinge and materialsThe hinge is another area where Apple may diverge from competitors. Multiple reports claim Apple will use Liquidmetal, which is a long-standing trade name for a metallic glass alloy the company has previously used in smaller components. While often referred to as “liquid metal” or “Liquid Metal” in reports, Liquidmetal is the branding Apple has historically associated with the material.Liquidmetal is said to be stronger and more resistant to deformation than titanium, while remaining relatively lightweight. If accurate, this could help improve long-term durability and reduce wear on the foldable display.Leaks from Jon Prosser also reference a metal plate beneath the display that works in tandem with the hinge to minimize creasing — a claim that aligns with reporting from Korean and Chinese supply-chain sources.Battery and other components Battery life is another potential differentiator. According to Ming-Chi Kuo and multiple Asian supply-chain reports, Apple is testing high-density battery cells in the 5,000 to 5,800mAh range.That would make it the largest battery ever used in an iPhone, and competitive with (or larger than) batteries in current Android foldables. The device is also expected to use a future A-series chip and Apple’s in-house modem, with some reports pointing specifically to a next-generation C2 modem as part of Apple’s broader push to reduce reliance on Qualcomm.PriceNone of this will come cheap, that’s for certain. Nearly every report agrees that the iPhone Fold will be Apple’s most expensive iPhone ever.Estimates currently place the price between $2,000 and $2,500 in the US. Bloomberg has said the price will be “at least $2,000,” while other analysts have narrowed the likely range to around $2,100 and $2,300. That positions the iPhone Fold well above the iPhone Pro Max and closer to Apple’s high-end Macs and iPads.Despite years of rumors, there’s still plenty that remains unclear. Apple hasn’t confirmed the name “iPhone Fold,” final dimensions, software features or how iOS would adapt to a folding form factor. Durability, repairability and long-term reliability are also open questions. For now, the safest assumption is that Apple is taking its time and that many of these details could still change before launch. This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/iphone-fold-rumors-everything-we-know-right-now-including-the-leaked-design-upgrades-price-and-more-130000733.html?src=rss",
          "feed_position": 3,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-09/a476c2e0-9780-11f0-bd4b-d87caa240702"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/samsung-galaxy-unpacked-2026-the-galaxy-s26-series-ai-and-other-products-we-might-see-on-february-25-130000135.html",
          "published_at": "Tue, 24 Feb 2026 15:48:57 +0000",
          "title": "Samsung Galaxy Unpacked 2026: The Galaxy S26 series, AI and other products we might see on February 25",
          "standfirst": "Samsung’s 2025 was filled with new foldables, an ultra-thin new form factor and the launch of Google's XR platform. After making some announcements at CES 2026, the company has announced its first Galaxy Unpacked of the year will take place on February 25, where it is expected to introduce the Galaxy S26 lineup. Official invites have been shared, but actual information on what devices are arriving then is still not completely confirmed. But as usual, we know a lot about what’s expected at Unpacked.Engadget will be covering Galaxy Unpacked live from San Francisco tomorrow, and we'll most likely have hands-on coverage of Samsung's new smartphones soon after they're announced. While we wait for the full details, here's everything we expect Samsung will introduce at the first Galaxy Unpacked event of 2026.When is Unpacked 2026 taking place?According to the official invite that Samsung shared on February 10, Unpacked will happen on February 25, 2026 in San Francisco. The keynote will start at 10AM PT (1PM ET) and be livestreamed on Samsung.com, as well as the company’s newsroom and YouTube channel. The announcement on February 10 also said this launch will mark “a new phase in the era of AI as intelligence becomes truly personal and adaptive.” It’s not a lot to go on, since we’ve heard a version of this from various companies over the last few years, but at least we won’t be shocked when we hear more about AI in just about two weeks.Galaxy S26, S26+ and S26 UltraSamsung Galaxy S25 Ultra hands-on photoPhoto by Sam Rutherford/EngadgetSamsung's restrained approach to updating its phones will likely continue with the Galaxy S26. Based on leaked images of the new lineup, the company is not expected to radically reinvent the look of the Galaxy S26, Galaxy S26+ or Galaxy S26 Ultra, and instead will stick with a similar design to what it used on the Galaxy S25. The phones will have a flat front screen and frame, with rounded corners and cameras housed in a vertical pill-shaped plateau on the back. Unlike Apple's move from the iPhone 16 Pro to the iPhone 17 Pro, the biggest difference here will likely be internal components like the screens, chips and camera sensors Samsung uses.Qualcomm's new Snapdragon 8 Elite Gen 5 chip is expected to be in all Samsung Galaxy S26 phones, though Korean news site Yonhap News reports Samsung's relatively new Exynos 2600 chip could be used in some phones in the lineup depending on the region, a strategy Samsung has deployed in the past. Either way the new phones should be more performant than the previous generation, and in the case of the models with the Snapdragon 8 Elite Gen 5, particularly good at on-device AI processing.I have compiled the most accurate comprehensive parameter comparison of Galaxy S25, S25+ and Galaxy S26、 S26+. Which one do you want to buy? pic.twitter.com/aQpoSvYjOz— Ice Universe (@UniverseIce) November 29, 2025One notable difference between the Galaxy S26 and the Galaxy S25 could be the phone's screen. The new phone will reportedly feature a 6.3-inch FHD+ display according to specs shared by leaker Ice Universe, which makes it ever so slightly larger than the 6.2-inch display used on the Galaxy S25. The S26 will also allegedly come with 12GB of RAM, either 256GB or 512GB of storage and a slightly larger 4,300mAh battery. Samsung isn't changing the cameras on the entry-level phone, though: leaks suggest it'll feature the same 50-megapixel main camera, 12-megapixel ultrawide, 10-megapixel 3x telephoto and 12-megapixel selfie camera as the previous generation. Changes appear to be even more minor on the Galaxy S26+. Other than the new Snapdragon chip, the phone will reportedly feature the same 6.7-inch FHD+ screen, 4,900mAh battery, 12GB of RAM and the same camera array used on the base Galaxy S26.The difference between the Galaxy S26 Ultra and Galaxy S25 Ultra is reportedly a bit clearer. According to Android Headlines, the new phone's cameras will be slightly more raised, and stand out thanks to a new metallic finish. Samsung may also switch back to using an aluminum frame on the Galaxy S26 Ultra, after using titanium frames on both the Galaxy S24 and S25 Ultras. Most importantly, to make the phone actually support Qi2 rather than only technically work with the standard when a case is attached, rumors suggest Samsung will remove the S Pen digitizer layer in the phone and adopt a new method for accepting stylus input. It's not clear what that new method will actually be, but it could let the Galaxy S26 Ultra more easily work with Qi2 accessories without losing its stylus.Android Headlines also recently shared what appear to be full image renders of the S26 series, and they generally line up with what has already been rumored, leaked and reported so far. If these pictures are accurate, they give us a clearer look at the camera bump and two color variants of the S26 Ultra.Fans of magnets may continue to be disappointed by Samsung if the latest rumors are accurate. Despite the launch of the Qi 2 wireless charging standard adding support for convenient magnetic alignment years ago, Samsung has yet to bring that feature to its phones. Though the S-series have the higher speed charging rates that the spec enables, Nieuwemobiel.nl is reporting that, due to images it received of cases with magnetic rings, the S26 series likely won’t have built-in magnets. Samsung has made these cases to add the magnetic capability to its S-series in the past, and the existence of the images of these accessories lends weight to the idea that the company will continue this approach.Galaxy Buds 4Galaxy Buds 3 Pro in case.EngadgetSamsung released the Galaxy Buds 3 and 3 Pro in 2024, with a major redesign that brought them much more in line with Apple's AirPods. The Galaxy Buds 4 and Buds 4 Pro Samsung is rumored to be announcing soon won't necessarily change that, though they will feature a more compact case and less angular stems, according to leaked images from the Samsung Tips app.Support for head gestures to accept and decline calls, a feature Apple includes on the AirPods Pro 3 and AirPods 4, is also rumored to work on both versions of the new Galaxy Buds. SamMobile reports the Galaxy Buds 4 and 4 Pro may also ship with a new Ultra Wideband chip that will make them easier to find with Google's Find Hub network.Galaxy Z TrifoldYes, the TriFold has a crease, two in fact. But they still don't ruin the experience. Sam Rutherford for EngadgetSamsung announced the Galaxy Z TriFold in late 2025 without firm details of when the new smartphone-that-folds-into-a-tablet would be available in North America. That info came on January 27, when the company announced the TriFold would be available in the US on January 30, for a whopping $2,900. Considering we’ve already seen the device in person at CES 2026 and people are most likely to have had a chance to look at, if not buy the foldable for themselves by the time Unpacked rolls around, we don’t expect Samsung to spend too much time dwelling on it, if at all.Galaxy S26 EdgeAt just 5.8mm thick, the Samsung Galaxy S25 Edge is one of the thinnest smartphones ever made. Sam Rutherford for EngadgetWhen the Galaxy S25 Edge was announced in 2025, it seemed possible that Samsung could replace its \"Plus\" smartphone with a unique form factor, just like Apple has opted to do with the iPhone Air. There have been conflicting reports on the matter, but it seems like Samsung will not be doing that with the Galaxy S26 Edge.Instead, the smartphone will reportedly remain another option, much like foldables are for customers not swayed by Samsung's traditional smartphones. The Galaxy S26 Edge is rumored to feature a slightly different design than last year's model, according to Android Headlines, with a large rectangular camera plateau that's reminiscent of Google's Pixel phones, and the raised oval Apple used on the iPhone Air. Beyond that, the phone is also expected to be ever so slightly thinner at 5.5mm than the 5.8mm Galaxy S25 Edge.Bixby and other AI featuresSamsung already acts as a first place Google can show off new AI features for Android, but the company is reportedly exploring other AI partnerships, too. In June 2025, Bloomberg reported that Samsung was nearing a deal with Perplexity to integrate its AI-powered search engine across OneUI and its homegrown mobile browser. Perplexity already has a deal with Motorola on its Razr phones, so the only thing that would make a deal with Samsung unusual is the close relationship the company already has with Google.The company also accidentally announced a new version of its Bixby AI assistant, which will likely also be integrated with Perplexity and could serve as an alternative to Google Gemini. Both a new Bixby and a deeper integration with Perplexity seem like natural new software features to show off at Galaxy Unpacked.On February 17, Samsung teased some mobile AI photography features ahead of Unpacked. These expand the S-series’ existing image-editing tools by bringing the ability “to turn a photo from day to night in seconds, restore missing parts of objects in images, capture detailed photos in low light, and seamlessly merge multiple photos into a single, cohesive result.” A lot of these things are already possible in other photo-editing apps or even in the Google Photos app, but we’ll have to wait to see them in action on the S26 phones for more details on whether they’re different or more effective.The company continued to drip feed more teasers in the week leading up to Unpacked 2026, announcing just a few days later that it’s updated its Bixby assistant to be more conversational. Then, over the weekend, it shared that the S26 series will offer third-party AI agents within Galaxy AI, including Perplexity’s offering. It will allow for the devices to respond to the wake phrase “Hey Plex,” which is sure to be popular and not at all confusing to those who already use a similarly named media server and streaming app. Until we find out more at Unpacked 2026, it’s tricky to determine if and how effective these updates will be, so we’ll just have to be patient until we get the phones in our hands. Update, January 27 2026, 11:55AM ET: This story has been updated to reflect the latest news around the Galaxy Z TriFold’s price and availability in the US.Update, January 30 2026, 12:45PM ET: This story has been updated to include the latest leaks on the possible dates for Unpacked 2026.Update, February 02 2026, 11:30AM ET: This story has been updated to include the latest leaks with full image renders of the S26 trio of devices.Update, February 03 2026, 11:00AM ET: This story has been updated to include the latest leaks about the possible lack of magnetic support on the S26 series.Update, February 10 2026, 7:15PM ET: This story has been updated to include the official date of Galaxy Unpacked as Samsung announced it today. The intro was also edited to reflect that detail.Update, February 17 2026, 4:55PM ET: This story has been updated to add Samsung’s teaser about its upcoming mobile AI photography tools. The intro was also edited for timeliness.Update, February 24 2026, 10:45AM ET: This story has been updated to add Samsung’s recent updates about Bixby and Galaxy AI’s integration with Perplexity. The intro was also edited for timeliness.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/samsung-galaxy-unpacked-2026-the-galaxy-s26-series-ai-and-other-products-we-might-see-on-february-25-130000135.html?src=rss",
          "content": "Samsung’s 2025 was filled with new foldables, an ultra-thin new form factor and the launch of Google's XR platform. After making some announcements at CES 2026, the company has announced its first Galaxy Unpacked of the year will take place on February 25, where it is expected to introduce the Galaxy S26 lineup. Official invites have been shared, but actual information on what devices are arriving then is still not completely confirmed. But as usual, we know a lot about what’s expected at Unpacked.Engadget will be covering Galaxy Unpacked live from San Francisco tomorrow, and we'll most likely have hands-on coverage of Samsung's new smartphones soon after they're announced. While we wait for the full details, here's everything we expect Samsung will introduce at the first Galaxy Unpacked event of 2026.When is Unpacked 2026 taking place?According to the official invite that Samsung shared on February 10, Unpacked will happen on February 25, 2026 in San Francisco. The keynote will start at 10AM PT (1PM ET) and be livestreamed on Samsung.com, as well as the company’s newsroom and YouTube channel. The announcement on February 10 also said this launch will mark “a new phase in the era of AI as intelligence becomes truly personal and adaptive.” It’s not a lot to go on, since we’ve heard a version of this from various companies over the last few years, but at least we won’t be shocked when we hear more about AI in just about two weeks.Galaxy S26, S26+ and S26 UltraSamsung Galaxy S25 Ultra hands-on photoPhoto by Sam Rutherford/EngadgetSamsung's restrained approach to updating its phones will likely continue with the Galaxy S26. Based on leaked images of the new lineup, the company is not expected to radically reinvent the look of the Galaxy S26, Galaxy S26+ or Galaxy S26 Ultra, and instead will stick with a similar design to what it used on the Galaxy S25. The phones will have a flat front screen and frame, with rounded corners and cameras housed in a vertical pill-shaped plateau on the back. Unlike Apple's move from the iPhone 16 Pro to the iPhone 17 Pro, the biggest difference here will likely be internal components like the screens, chips and camera sensors Samsung uses.Qualcomm's new Snapdragon 8 Elite Gen 5 chip is expected to be in all Samsung Galaxy S26 phones, though Korean news site Yonhap News reports Samsung's relatively new Exynos 2600 chip could be used in some phones in the lineup depending on the region, a strategy Samsung has deployed in the past. Either way the new phones should be more performant than the previous generation, and in the case of the models with the Snapdragon 8 Elite Gen 5, particularly good at on-device AI processing.I have compiled the most accurate comprehensive parameter comparison of Galaxy S25, S25+ and Galaxy S26、 S26+. Which one do you want to buy? pic.twitter.com/aQpoSvYjOz— Ice Universe (@UniverseIce) November 29, 2025One notable difference between the Galaxy S26 and the Galaxy S25 could be the phone's screen. The new phone will reportedly feature a 6.3-inch FHD+ display according to specs shared by leaker Ice Universe, which makes it ever so slightly larger than the 6.2-inch display used on the Galaxy S25. The S26 will also allegedly come with 12GB of RAM, either 256GB or 512GB of storage and a slightly larger 4,300mAh battery. Samsung isn't changing the cameras on the entry-level phone, though: leaks suggest it'll feature the same 50-megapixel main camera, 12-megapixel ultrawide, 10-megapixel 3x telephoto and 12-megapixel selfie camera as the previous generation. Changes appear to be even more minor on the Galaxy S26+. Other than the new Snapdragon chip, the phone will reportedly feature the same 6.7-inch FHD+ screen, 4,900mAh battery, 12GB of RAM and the same camera array used on the base Galaxy S26.The difference between the Galaxy S26 Ultra and Galaxy S25 Ultra is reportedly a bit clearer. According to Android Headlines, the new phone's cameras will be slightly more raised, and stand out thanks to a new metallic finish. Samsung may also switch back to using an aluminum frame on the Galaxy S26 Ultra, after using titanium frames on both the Galaxy S24 and S25 Ultras. Most importantly, to make the phone actually support Qi2 rather than only technically work with the standard when a case is attached, rumors suggest Samsung will remove the S Pen digitizer layer in the phone and adopt a new method for accepting stylus input. It's not clear what that new method will actually be, but it could let the Galaxy S26 Ultra more easily work with Qi2 accessories without losing its stylus.Android Headlines also recently shared what appear to be full image renders of the S26 series, and they generally line up with what has already been rumored, leaked and reported so far. If these pictures are accurate, they give us a clearer look at the camera bump and two color variants of the S26 Ultra.Fans of magnets may continue to be disappointed by Samsung if the latest rumors are accurate. Despite the launch of the Qi 2 wireless charging standard adding support for convenient magnetic alignment years ago, Samsung has yet to bring that feature to its phones. Though the S-series have the higher speed charging rates that the spec enables, Nieuwemobiel.nl is reporting that, due to images it received of cases with magnetic rings, the S26 series likely won’t have built-in magnets. Samsung has made these cases to add the magnetic capability to its S-series in the past, and the existence of the images of these accessories lends weight to the idea that the company will continue this approach.Galaxy Buds 4Galaxy Buds 3 Pro in case.EngadgetSamsung released the Galaxy Buds 3 and 3 Pro in 2024, with a major redesign that brought them much more in line with Apple's AirPods. The Galaxy Buds 4 and Buds 4 Pro Samsung is rumored to be announcing soon won't necessarily change that, though they will feature a more compact case and less angular stems, according to leaked images from the Samsung Tips app.Support for head gestures to accept and decline calls, a feature Apple includes on the AirPods Pro 3 and AirPods 4, is also rumored to work on both versions of the new Galaxy Buds. SamMobile reports the Galaxy Buds 4 and 4 Pro may also ship with a new Ultra Wideband chip that will make them easier to find with Google's Find Hub network.Galaxy Z TrifoldYes, the TriFold has a crease, two in fact. But they still don't ruin the experience. Sam Rutherford for EngadgetSamsung announced the Galaxy Z TriFold in late 2025 without firm details of when the new smartphone-that-folds-into-a-tablet would be available in North America. That info came on January 27, when the company announced the TriFold would be available in the US on January 30, for a whopping $2,900. Considering we’ve already seen the device in person at CES 2026 and people are most likely to have had a chance to look at, if not buy the foldable for themselves by the time Unpacked rolls around, we don’t expect Samsung to spend too much time dwelling on it, if at all.Galaxy S26 EdgeAt just 5.8mm thick, the Samsung Galaxy S25 Edge is one of the thinnest smartphones ever made. Sam Rutherford for EngadgetWhen the Galaxy S25 Edge was announced in 2025, it seemed possible that Samsung could replace its \"Plus\" smartphone with a unique form factor, just like Apple has opted to do with the iPhone Air. There have been conflicting reports on the matter, but it seems like Samsung will not be doing that with the Galaxy S26 Edge.Instead, the smartphone will reportedly remain another option, much like foldables are for customers not swayed by Samsung's traditional smartphones. The Galaxy S26 Edge is rumored to feature a slightly different design than last year's model, according to Android Headlines, with a large rectangular camera plateau that's reminiscent of Google's Pixel phones, and the raised oval Apple used on the iPhone Air. Beyond that, the phone is also expected to be ever so slightly thinner at 5.5mm than the 5.8mm Galaxy S25 Edge.Bixby and other AI featuresSamsung already acts as a first place Google can show off new AI features for Android, but the company is reportedly exploring other AI partnerships, too. In June 2025, Bloomberg reported that Samsung was nearing a deal with Perplexity to integrate its AI-powered search engine across OneUI and its homegrown mobile browser. Perplexity already has a deal with Motorola on its Razr phones, so the only thing that would make a deal with Samsung unusual is the close relationship the company already has with Google.The company also accidentally announced a new version of its Bixby AI assistant, which will likely also be integrated with Perplexity and could serve as an alternative to Google Gemini. Both a new Bixby and a deeper integration with Perplexity seem like natural new software features to show off at Galaxy Unpacked.On February 17, Samsung teased some mobile AI photography features ahead of Unpacked. These expand the S-series’ existing image-editing tools by bringing the ability “to turn a photo from day to night in seconds, restore missing parts of objects in images, capture detailed photos in low light, and seamlessly merge multiple photos into a single, cohesive result.” A lot of these things are already possible in other photo-editing apps or even in the Google Photos app, but we’ll have to wait to see them in action on the S26 phones for more details on whether they’re different or more effective.The company continued to drip feed more teasers in the week leading up to Unpacked 2026, announcing just a few days later that it’s updated its Bixby assistant to be more conversational. Then, over the weekend, it shared that the S26 series will offer third-party AI agents within Galaxy AI, including Perplexity’s offering. It will allow for the devices to respond to the wake phrase “Hey Plex,” which is sure to be popular and not at all confusing to those who already use a similarly named media server and streaming app. Until we find out more at Unpacked 2026, it’s tricky to determine if and how effective these updates will be, so we’ll just have to be patient until we get the phones in our hands. Update, January 27 2026, 11:55AM ET: This story has been updated to reflect the latest news around the Galaxy Z TriFold’s price and availability in the US.Update, January 30 2026, 12:45PM ET: This story has been updated to include the latest leaks on the possible dates for Unpacked 2026.Update, February 02 2026, 11:30AM ET: This story has been updated to include the latest leaks with full image renders of the S26 trio of devices.Update, February 03 2026, 11:00AM ET: This story has been updated to include the latest leaks about the possible lack of magnetic support on the S26 series.Update, February 10 2026, 7:15PM ET: This story has been updated to include the official date of Galaxy Unpacked as Samsung announced it today. The intro was also edited to reflect that detail.Update, February 17 2026, 4:55PM ET: This story has been updated to add Samsung’s teaser about its upcoming mobile AI photography tools. The intro was also edited for timeliness.Update, February 24 2026, 10:45AM ET: This story has been updated to add Samsung’s recent updates about Bixby and Galaxy AI’s integration with Perplexity. The intro was also edited for timeliness.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/samsung-galaxy-unpacked-2026-the-galaxy-s26-series-ai-and-other-products-we-might-see-on-february-25-130000135.html?src=rss",
          "feed_position": 11,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-01/59db82d0-d8d0-11ef-babd-deb856accfc5"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/kilo-launches-kiloclaw-allowing-anyone-to-deploy-hosted-openclaw-agents-into",
          "published_at": "Tue, 24 Feb 2026 15:00:00 GMT",
          "title": "Kilo launches KiloClaw, allowing anyone to deploy hosted OpenClaw agents into production in 60 seconds",
          "standfirst": "In the rapidly evolving landscape of artificial intelligence, the distance between a developer’s idea and a functioning agent has historically been measured in hours of configuration, dependency conflicts, and terminal-induced headaches. That friction point changed today. Kilo, the AI infrastructure startup backed by GitLab co-founder Sid Sijbrandij, has announced the general availability of KiloClaw, a fully managed service designed to deploy a production-ready OpenClaw agent in under 60 seconds.By eliminating the “SSH, Docker, and YAML” barriers that have gatekept high-end AI agents, Kilo is betting that the next phase of software development—often called \"vibe coding\"—will be defined not just by the quality of a model, but by the reliability of the infrastructure that hosts it.Technology: Re-engineering the agentic sandboxOpenClaw has emerged as a viral phenomenon, amassing over 161,000 GitHub stars by offering a capability that many proprietary tools lack: the ability to actually perform tasks—controlling browsers, managing files, and connecting to over 50 chat platforms like Telegram and Signal. However, as Kilo co-founder and CEO Scott Breitenother noted in an exclusive interview with VentureBeat, \"OpenClaw itself isn&#x27;t the hard part... getting it running is\".The technical architecture of KiloClaw is a departure from the \"Mac Mini on a desk\" model that many early adopters have relied on. Instead of requiring users to provision their own hardware or Virtual Private Servers (VPS), KiloClaw runs on a multi-tenant Virtual Machine (VM) architecture powered by Fly.io, a Chicago remote-first startup offering a developer-focused public cloud. This setup provides a level of isolation and security that is difficult for individual developers to replicate.\"What we&#x27;re doing is making KiloClaw the safest way to claw,\" Breitenother explained during the interview. \"We have a virtual machine that is a hosted OpenClaw instance, and we&#x27;re handling all that network security, sandboxing, and proxies that an enterprise company would require. We are essentially running multi-tenant, hosted OpenClaw\".To ensure security, KiloClaw utilizes two distinct proxies that sit outside the VM to manage traffic and protect the instance from the open internet. This prevents the common \"user error\" of accidentally exposing an agent’s API keys or leaving a local instance vulnerable to external attacks. \"It&#x27;s going to be better than [a local setup] in every single way,\" Breitenother asserted. \"If you were to set it up yourself, you&#x27;d probably miss a setting and end up with it accidentally on the internet or exposing an API key\".Product: The &#x27;mech suit&#x27; and the 3 am crashA primary pain point for OpenClaw users is the “3 am crash”—the tendency for locally hosted Node.js processes to die silently overnight without health monitoring or auto-restart capabilities. KiloClaw addresses this with built-in process monitoring and a cloud-native \"always on\" state.Unlike standard Kilo Code workflows, which spin up a terminal session only when a developer initiates a command, KiloClaw is persistent. \"KiloClaw is just running and listening,\" said Breitenother. \"It&#x27;s always on, waiting for your WhatsApp message or your Slack message. It has to be always on. That&#x27;s a different paradigm—always-on infrastructure to engage with\".This persistence allows for a suite of \"agentic affordances\" that Kilo calls an \"exoskeleton for the mind\":Scheduled automations: Users can set cron jobs for the agent to perform research, monitor repositories, or generate reports while the human user is offline.Persistent memory: Utilizing a \"Memory Bank\" system, the agent stores context in structured Markdown files within the repository, ensuring it retains the state of a project even if the underlying model is swapped.Cross-platform command: The agent can be triggered from Slack, Telegram, or a terminal, maintaining a unified execution state across all entry points.Breitenother highlighted the shift in the developer’s role during the interview: \"We&#x27;ve actually moved our engineers to be product owners. The time they freed up from writing code, they&#x27;re actually doing much more thinking. They&#x27;re setting the strategy for the product\".The “gateway” advantage: 500+ models, no lock-inA core component of the KiloClaw architecture is its native integration with the Kilo Gateway. While the original OpenClaw was initially tied closely to Anthropic&#x27;s models, KiloClaw allows users to toggle between over 500 different models from providers like OpenAI, Google, and MiniMax, as well as open-weight models like Qwen or GLM.\"Your preferred model today may not be the same—and honestly shouldn&#x27;t be the same—a month and a half from now,\" Breitenother said, emphasizing the speed of the industry. \"You may want different models for different tasks. Maybe you use Opus for something complex, or you switch to a tighter-budget open-weight model for routine work\".This flexibility is supported by Kilo&#x27;s transparent pricing model. The company offers \"zero markup\" on AI tokens, charging users the exact API rates provided by the model vendors. For power users, this is managed through Kilo Pass, a subscription tier that provides bonus credits (e.g., $199/month for $278.60 in credits) to subsidize high-volume agentic work.How to get started with KiloClaw right nowSign in or register: Navigate to the Kilo Code application on the web (desktop) at app.kilo.ai and sign in using your existing account. Kilo supports several authentication methods, including GitHub and Google OAuth.Create your instance: Select the \"Claw\" tab from the side navigation menu to access the KiloClaw dashboard. Click the \"Create Instance\" button to begin provisioning your agent (see image above for where to find it). Choose your model: Select a default AI model to power your agent from the dropdown menu. Users can choose from a wide array of options, including free (for the time being) models like MiniMax.Configure messaging channels (optional): During setup, you can optionally connect your agent to Discord, Telegram, or Slack and communicate with your KiloClaw agent directly over those channels — instead of on the Kilo Code website. But to move faster, you may skip this step and are always able to add these supported bot keys and configure these channels later in the instance settings.Provision and start: Click \"Create and Provision\" to set up your virtual machine. Once the instance is provisioned, click \"Start\" to boot the agent, which typically takes only a few secondVerify and access: Click the \"Open\" button to enter the OpenClaw interface. For security, you will need to click \"Access Code\" to generate a one-time verification token that validates your device for the first time.Begin vibe coding: Once verified, you can begin interacting with your agent directly in the chat interface. The agent will remain running 24/7 on a dedicated virtual machine, listening for commands across all connected platforms.According to Brendan O&#x27;Leary, Developer Relations at Kilo Code and former Developer Evangelist at GitLab, users unsure which model to select should consult PinchBench, an open-source benchmarking tool developed to evaluate models on 23 real-world agentic tasks, such as email sorting and blog post generation.Benchmarking the agentic era: the launch of PinchBench, a new open-source benchmarking suite specifically for Claw tasksTo help developers navigate the choice between 500+ models, Kilo has also released PinchBench, an open-source benchmark specifically for agentic workloads. While traditional benchmarks like MMLU or HumanEval test chat prompts in isolation, PinchBench tests agents on 23 real-world, multi-step tasks such as calendar management and multi-source research.The project was spearheaded by O&#x27;Leary, who noted during a demonstration that the benchmark was \"kind of inspired by... other little kind of fun benches\" like those created by developer YouTuber Theo Browne (@t3dotgg), CEO/Founder of Ping Labs. O&#x27;Leary explained that while existing benchmarks are often highly specialized, he wanted a way to \"benchmark the kind of things that we asked OpenClaw to do\". He has personally run the benchmark \"hundreds and hundreds of times against OpenClaw\" to ensure its accuracy, and taking a page out of Browne&#x27;s book (er, video playbook?), also launched a YouTube series to find out if KiloClaw can handle various tasks, entitled, fittingly, \"Will It Claw?\"To maintain high standards of evaluation for subjective tasks like writing blog posts, O&#x27;Leary designed a system where a high-end \"judge model\"—specifically Claude 4.5 Opus—is used to grade the output of other models. \"We actually have... not the model under test, but always Opus... [judge] the output of each of the models,\" O&#x27;Leary stated, adding that the judge model even provides specific notes on execution quality. The benchmark allows users to view a scatter plot comparing \"Cost to Intelligence,\" identifying which models offer the highest proficiency for the lowest price. This specific visualization is a priority for O&#x27;Leary, who noted it is \"my favorite graph for looking at models... how much do you spend versus how much is the success rate\". For those who prefer to host their own infrastructure, O&#x27;Leary has made the process entirely transparent, providing a \"skill file that people can download\" so they can \"benchmark their own OpenClaw instance\" independently\"We&#x27;re doing this work anyway to know which defaults we should recommend,\" Breitenother added in a separate interview. \"We decided to open source it because the individual developer shouldn&#x27;t have to think about which model is best for the job. We want to give people more and more information\". O&#x27;Leary expanded on this philosophy, describing the benchmark as being \"kind of like the Olympics in a lot of ways,\" where tasks range from \"very objectively graded\" to those requiring a more nuanced assessment.Industry context: Distinguishing from the growing OpenClaw family of offshoots KiloClaw enters a market increasingly crowded with OpenClaw variants. Projects like Nanoclaw have gained traction for being lightweight, while companies like Runlayer have targeted the enterprise \"Virtual Private Server\" niche.However, Kilo distinguishes itself by refusing to \"fork\" the code. \"It’s not a fork, and that’s what’s important,\" Breitenother stated. \"OpenClaw moves so quickly that we are hosting the actual OpenClaw [version]. It is literally OpenClaw on a really well-tuned, well-set-up managed virtual machine\". This ensures that as the core OpenClaw project evolves, KiloClaw users receive updates automatically without manual \"git pull\" operations.This \"open core\" philosophy extends to the licensing. While KiloClaw is a paid hosted service, the underlying Kilo CLI and core extensions remain MIT-licensed. This allows for community auditing—a critical feature for security-conscious enterprises.Conclusion: toward an agentic futureThe launch of KiloClaw marks a strategic move by Kilo to expand its user base beyond \"wonky\" developers to enterprise managers and non-technical professionals. By offering a \"one-click\" path to a production agent, the company is attempting to democratize the \"magical moments\" of AI.According to a release provided to VentureBeat by Kilo ahead of the launch, in the first two weeks, more than 3,500 developers joined the waitlist. These early adopters have been \"really pushing KiloClaw in all kinds of directions,\" using it to automate everything from Discord management to repository maintenance.\"Our mission is to build the best all-in-one AI work platform,\" Breitenother concluded. \"Whether you are a developer, a product manager, or a data engineer, we want all of these personas to experience the magic of the exoskeleton for the mind\".KiloClaw is available now, offering 7 days of free compute for all new users. With thousands of developers already having cleared the waitlist, the era of the managed AI agent appears to have arrived—no Mac Mini required.",
          "content": "In the rapidly evolving landscape of artificial intelligence, the distance between a developer’s idea and a functioning agent has historically been measured in hours of configuration, dependency conflicts, and terminal-induced headaches. That friction point changed today. Kilo, the AI infrastructure startup backed by GitLab co-founder Sid Sijbrandij, has announced the general availability of KiloClaw, a fully managed service designed to deploy a production-ready OpenClaw agent in under 60 seconds.By eliminating the “SSH, Docker, and YAML” barriers that have gatekept high-end AI agents, Kilo is betting that the next phase of software development—often called \"vibe coding\"—will be defined not just by the quality of a model, but by the reliability of the infrastructure that hosts it.Technology: Re-engineering the agentic sandboxOpenClaw has emerged as a viral phenomenon, amassing over 161,000 GitHub stars by offering a capability that many proprietary tools lack: the ability to actually perform tasks—controlling browsers, managing files, and connecting to over 50 chat platforms like Telegram and Signal. However, as Kilo co-founder and CEO Scott Breitenother noted in an exclusive interview with VentureBeat, \"OpenClaw itself isn&#x27;t the hard part... getting it running is\".The technical architecture of KiloClaw is a departure from the \"Mac Mini on a desk\" model that many early adopters have relied on. Instead of requiring users to provision their own hardware or Virtual Private Servers (VPS), KiloClaw runs on a multi-tenant Virtual Machine (VM) architecture powered by Fly.io, a Chicago remote-first startup offering a developer-focused public cloud. This setup provides a level of isolation and security that is difficult for individual developers to replicate.\"What we&#x27;re doing is making KiloClaw the safest way to claw,\" Breitenother explained during the interview. \"We have a virtual machine that is a hosted OpenClaw instance, and we&#x27;re handling all that network security, sandboxing, and proxies that an enterprise company would require. We are essentially running multi-tenant, hosted OpenClaw\".To ensure security, KiloClaw utilizes two distinct proxies that sit outside the VM to manage traffic and protect the instance from the open internet. This prevents the common \"user error\" of accidentally exposing an agent’s API keys or leaving a local instance vulnerable to external attacks. \"It&#x27;s going to be better than [a local setup] in every single way,\" Breitenother asserted. \"If you were to set it up yourself, you&#x27;d probably miss a setting and end up with it accidentally on the internet or exposing an API key\".Product: The &#x27;mech suit&#x27; and the 3 am crashA primary pain point for OpenClaw users is the “3 am crash”—the tendency for locally hosted Node.js processes to die silently overnight without health monitoring or auto-restart capabilities. KiloClaw addresses this with built-in process monitoring and a cloud-native \"always on\" state.Unlike standard Kilo Code workflows, which spin up a terminal session only when a developer initiates a command, KiloClaw is persistent. \"KiloClaw is just running and listening,\" said Breitenother. \"It&#x27;s always on, waiting for your WhatsApp message or your Slack message. It has to be always on. That&#x27;s a different paradigm—always-on infrastructure to engage with\".This persistence allows for a suite of \"agentic affordances\" that Kilo calls an \"exoskeleton for the mind\":Scheduled automations: Users can set cron jobs for the agent to perform research, monitor repositories, or generate reports while the human user is offline.Persistent memory: Utilizing a \"Memory Bank\" system, the agent stores context in structured Markdown files within the repository, ensuring it retains the state of a project even if the underlying model is swapped.Cross-platform command: The agent can be triggered from Slack, Telegram, or a terminal, maintaining a unified execution state across all entry points.Breitenother highlighted the shift in the developer’s role during the interview: \"We&#x27;ve actually moved our engineers to be product owners. The time they freed up from writing code, they&#x27;re actually doing much more thinking. They&#x27;re setting the strategy for the product\".The “gateway” advantage: 500+ models, no lock-inA core component of the KiloClaw architecture is its native integration with the Kilo Gateway. While the original OpenClaw was initially tied closely to Anthropic&#x27;s models, KiloClaw allows users to toggle between over 500 different models from providers like OpenAI, Google, and MiniMax, as well as open-weight models like Qwen or GLM.\"Your preferred model today may not be the same—and honestly shouldn&#x27;t be the same—a month and a half from now,\" Breitenother said, emphasizing the speed of the industry. \"You may want different models for different tasks. Maybe you use Opus for something complex, or you switch to a tighter-budget open-weight model for routine work\".This flexibility is supported by Kilo&#x27;s transparent pricing model. The company offers \"zero markup\" on AI tokens, charging users the exact API rates provided by the model vendors. For power users, this is managed through Kilo Pass, a subscription tier that provides bonus credits (e.g., $199/month for $278.60 in credits) to subsidize high-volume agentic work.How to get started with KiloClaw right nowSign in or register: Navigate to the Kilo Code application on the web (desktop) at app.kilo.ai and sign in using your existing account. Kilo supports several authentication methods, including GitHub and Google OAuth.Create your instance: Select the \"Claw\" tab from the side navigation menu to access the KiloClaw dashboard. Click the \"Create Instance\" button to begin provisioning your agent (see image above for where to find it). Choose your model: Select a default AI model to power your agent from the dropdown menu. Users can choose from a wide array of options, including free (for the time being) models like MiniMax.Configure messaging channels (optional): During setup, you can optionally connect your agent to Discord, Telegram, or Slack and communicate with your KiloClaw agent directly over those channels — instead of on the Kilo Code website. But to move faster, you may skip this step and are always able to add these supported bot keys and configure these channels later in the instance settings.Provision and start: Click \"Create and Provision\" to set up your virtual machine. Once the instance is provisioned, click \"Start\" to boot the agent, which typically takes only a few secondVerify and access: Click the \"Open\" button to enter the OpenClaw interface. For security, you will need to click \"Access Code\" to generate a one-time verification token that validates your device for the first time.Begin vibe coding: Once verified, you can begin interacting with your agent directly in the chat interface. The agent will remain running 24/7 on a dedicated virtual machine, listening for commands across all connected platforms.According to Brendan O&#x27;Leary, Developer Relations at Kilo Code and former Developer Evangelist at GitLab, users unsure which model to select should consult PinchBench, an open-source benchmarking tool developed to evaluate models on 23 real-world agentic tasks, such as email sorting and blog post generation.Benchmarking the agentic era: the launch of PinchBench, a new open-source benchmarking suite specifically for Claw tasksTo help developers navigate the choice between 500+ models, Kilo has also released PinchBench, an open-source benchmark specifically for agentic workloads. While traditional benchmarks like MMLU or HumanEval test chat prompts in isolation, PinchBench tests agents on 23 real-world, multi-step tasks such as calendar management and multi-source research.The project was spearheaded by O&#x27;Leary, who noted during a demonstration that the benchmark was \"kind of inspired by... other little kind of fun benches\" like those created by developer YouTuber Theo Browne (@t3dotgg), CEO/Founder of Ping Labs. O&#x27;Leary explained that while existing benchmarks are often highly specialized, he wanted a way to \"benchmark the kind of things that we asked OpenClaw to do\". He has personally run the benchmark \"hundreds and hundreds of times against OpenClaw\" to ensure its accuracy, and taking a page out of Browne&#x27;s book (er, video playbook?), also launched a YouTube series to find out if KiloClaw can handle various tasks, entitled, fittingly, \"Will It Claw?\"To maintain high standards of evaluation for subjective tasks like writing blog posts, O&#x27;Leary designed a system where a high-end \"judge model\"—specifically Claude 4.5 Opus—is used to grade the output of other models. \"We actually have... not the model under test, but always Opus... [judge] the output of each of the models,\" O&#x27;Leary stated, adding that the judge model even provides specific notes on execution quality. The benchmark allows users to view a scatter plot comparing \"Cost to Intelligence,\" identifying which models offer the highest proficiency for the lowest price. This specific visualization is a priority for O&#x27;Leary, who noted it is \"my favorite graph for looking at models... how much do you spend versus how much is the success rate\". For those who prefer to host their own infrastructure, O&#x27;Leary has made the process entirely transparent, providing a \"skill file that people can download\" so they can \"benchmark their own OpenClaw instance\" independently\"We&#x27;re doing this work anyway to know which defaults we should recommend,\" Breitenother added in a separate interview. \"We decided to open source it because the individual developer shouldn&#x27;t have to think about which model is best for the job. We want to give people more and more information\". O&#x27;Leary expanded on this philosophy, describing the benchmark as being \"kind of like the Olympics in a lot of ways,\" where tasks range from \"very objectively graded\" to those requiring a more nuanced assessment.Industry context: Distinguishing from the growing OpenClaw family of offshoots KiloClaw enters a market increasingly crowded with OpenClaw variants. Projects like Nanoclaw have gained traction for being lightweight, while companies like Runlayer have targeted the enterprise \"Virtual Private Server\" niche.However, Kilo distinguishes itself by refusing to \"fork\" the code. \"It’s not a fork, and that’s what’s important,\" Breitenother stated. \"OpenClaw moves so quickly that we are hosting the actual OpenClaw [version]. It is literally OpenClaw on a really well-tuned, well-set-up managed virtual machine\". This ensures that as the core OpenClaw project evolves, KiloClaw users receive updates automatically without manual \"git pull\" operations.This \"open core\" philosophy extends to the licensing. While KiloClaw is a paid hosted service, the underlying Kilo CLI and core extensions remain MIT-licensed. This allows for community auditing—a critical feature for security-conscious enterprises.Conclusion: toward an agentic futureThe launch of KiloClaw marks a strategic move by Kilo to expand its user base beyond \"wonky\" developers to enterprise managers and non-technical professionals. By offering a \"one-click\" path to a production agent, the company is attempting to democratize the \"magical moments\" of AI.According to a release provided to VentureBeat by Kilo ahead of the launch, in the first two weeks, more than 3,500 developers joined the waitlist. These early adopters have been \"really pushing KiloClaw in all kinds of directions,\" using it to automate everything from Discord management to repository maintenance.\"Our mission is to build the best all-in-one AI work platform,\" Breitenother concluded. \"Whether you are a developer, a product manager, or a data engineer, we want all of these personas to experience the magic of the exoskeleton for the mind\".KiloClaw is available now, offering 7 days of free compute for all new users. With thousands of developers already having cleared the waitlist, the era of the managed AI agent appears to have arrived—no Mac Mini required.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1QiGGYPYc6tdMLKflF0YdI/17bfba48715e6ef3811d63ae394fb13b/iEcN1uWLBkQLckQpBDapb_9aWch0lT.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/kitchen-tech/seattle-ultrasonics-c-200-review-this-is-the-future-of-kitchen-knives-140000051.html",
          "published_at": "Tue, 24 Feb 2026 14:00:00 +0000",
          "title": "Seattle Ultrasonics C-200 review: This is the future of kitchen knives",
          "standfirst": "There’s a type of knife tech often seen in science fiction that revolves around vibrating a blade to increase its sharpness. We’ve seen examples of this in franchises like Star Wars (vibroblades), Evangelion (the prog knife), Dune (pulse-swords) and the Marvel universe (vibranium), but what might surprise you is that the underlying science is sound. By vibrating a cutting tool at high frequencies, not only do you reduce friction, you essentially turn the blade into a saw, as tiny oscillations enhance the inherent sharpness of a blade. However, up until recently, this tech largely only existed in fiction or for large companies that have the money to utilize the tech on an industrial scale. But that's changing in a big way for home cooks this year thanks to Seattle Ultrasonics, which is releasing the world's first ultrasonic chef's knife: the C-200. After chopping, smashing and cooking with it for about a month, I'm convinced that this is the future of kitchen knives. Design From afar, the C-200 looks a lot like a regular 8-inch chef's knife, but with a slightly more contemporary design. It features a three-layer san mai blade made from Japanese AUS-10 steel with a 13-degree edge angle per side (26 degrees total). However, upon closer inspection, you'll notice there are some features that seem a bit out of place on a premium knife. The first is that the C-200 doesn't have a full tang, which is the back end of a blade that ideally extends into the handle to provide added strength and durability. This is usually a major no-no, particularly on a $400 knife. However, when you consider that Seattle Ultrasonics needed somewhere to put its vibration tech, there really isn't any room for it other than inside the knife's grip. You won't be able to feel it, but pressing this button allows the C-200's blade to vibrate 30,000 times per second. Sam Rutherford for Engadget The knife's second quirk is that the back of the plastic handle features small indicator lights on either side, which is obviously a bit weird. Furthermore, the entire gray section can be removed to reveal a small 1,100mAh battery with an onboard USB-C port. Frankly, the presence of a battery in a knife is just kind of funky. But hey, the power to vibrate the knife has to come from somewhere because it definitely isn't being generated by your hands. And while Seattle Ultrasonics doesn't include a charging adapter or cable in the box, I don't mind because the company wisely took cues from the larger gadget industry and went with a power spec that's already widely in use. Honestly, I wish more kitchen tech makers would do the same. However, the knife's biggest oddity is the big orange button on the bottom of its handle. This is what you use to make the blade vibrate, which it does at 33kHz. It's positioned well so that it's easy to press regardless of whether you do a traditional pinch grip or if you're a bit more casual and prefer to hold the knife only using its handle. In the future, I can see this button becoming a touch-sensitive sensor, but for now, it's simple and effective. Here's a small selection of knives I own sorted by weight (from top to bottom) compared to the C200: 6-inch Kyocera ceramic knife (97 grams), MAC molybdenum steel chef's knife (110g), Furtif Evercut titanium carbide chef's knife (190g), Seattle Ultrasonics C-200 (328g), Korin carbon steel cleaver (396g). Sam Rutherford for Engadget The main downside to the C-200's design is that at 328 grams (around 0.75 pounds) it's heavier and bulkier than a typical knife. When compared to other knives I own, which are made from a wide variety of materials including, ceramic, molybdenum steel, carbon steel and even titanium carbide, it weighs more than everything else aside from my big Chinese cleaver (396 grams). And while it fits nicely in my hand, my wife said it takes a bit more effort for her to wield. It's not too much to the point where you don't want to use it. But for quick tasks, sometimes I found myself subconsciously reaching for lighter options like my 6-inch ceramic knife, which weighs just 97 grams. How it works From a user standpoint, putting the C-200 to work couldn't be simpler. Just press the button and let the knife do its thing. The big difference from how knives like this work in sci-fi is that there's no audible hum or detectable vibration when it's on. It's practically silent (well, most of the time, but more on that later), so you have to trust that it's on or check the indicator light on the handle. That said, if you still don't believe anything is happening, you can run the edge of the blade under water or scrape it over some cut citrus, at which point the blade's vibration will atomize nearby liquid into a fine mist. It's a cool party trick that also doubles as a way to amp up a cocktail by adding a faint essence of lemon, lime or anything else you can think of. Pushing the C-200's button is super easy, regardless of what kind of knife grip you prefer. Sam Rutherford for Engadget Inside, the knife relies on PZT-8 piezoelectric ceramic crystals to generate up to 30,000 vibrations per second, which propagate down the blade and make the knife function as if it's sharper than it actually is. This all sounds rather fantastic, so how does it function in the real world? In-use To really put the C-200 through its paces, I cooked over a dozen meals that involved neatly slicing or preparing a wide variety of foods — including Hasselback potatoes, flank steak, pork belly, chives, sushi-grade tuna and all sorts of fruit. After prepping four pounds of pork belly with various knives, the C-200 really showed off how much of a difference its vibration tech makes. Sam Rutherford for Engadget In short, the C-200's effectiveness depends a lot on what you're chopping. For soft things like strawberries or a piece of cake, I didn't notice much of a difference. To make things even more difficult, the knife arrived out of the box with an incredibly fine edge — the kind that makes shearing through a sheet of paper child's play. So even though Seattle Ultrasonics says its knife can reduce cutting effort by up to 50 percent, there's not much gain to be had when slicing foods that could just as easily be cut by a butter knife. However, as I used it more, I found that the C-200 excels at cutting through delicate items like tomatoes, scallions and fish, where using a dull knife often results in bruising the food as you chop. This was most evident when I made poke at home, where Seattle Ultrasonic's knife delivered cleaner, more precise cuts than anything else I own. For me, the C-200's $399 price tag is almost worth it just so I have an an easier time making my one of my all-time favorite dishes (lu rou fan). Sam Rutherford for Engadget When I whipped up some pico de gallo, I distinctly noticed how neatly the C-200 sliced through the skin of a tomato, instead of initially putting a crease in it before cleanly passing through its interior — which often happens when using dull knives. An additional benefit is that because of the vibrations, I found some foods like garlic didn't stick to the side of the blade as much. This made it easier to keep track of how much I chopped while simultaneously reducing the mess from things falling willy-nilly during prep. But perhaps the most obvious demonstration of the knife's prowess was when I diced an onion. When using my other knives or the C-200 without powering it on, I could feel when I tried to cut through thicker, more sturdy layers. But then, at the touch of a button, I was able to slice down with practically no resistance. It's almost shocking because it feels like magic. The C-200 truly excels at cutting denser foods like flank steak. Sadly mine ended up closer to medium than medium rare, but that's not the knife's fault. Sam Rutherford for Engadget The C-200 even has the ability to reduce the importance of certain knife techniques. Anyone who's seen all the posts on r/kitchenconfidential about cutting chives will already know what I'm talking about. As J Kenji Lopez-Alt neatly demonstrated, the ideal way to get crisp, clean slices is to do a subtle forward or back cut instead of simply chopping straight down. But with Seattle Ultrasonics' knife, I've found that it's so sharp you can get away with almost any motion and still get good results. And if you do it the right way, things are even better. Other types of food that makes the C-200 really shine are denser ingredients like meat and potatoes, where you can really feel the added cutting power. Previously, when I had to break down thick cuts of protein, I sometimes wished I owned a serrated electric knife. You know, the kind you break out once a year on Thanksgiving and then it sits and gathers dust for the other 364 days. But the C-200 made that desire a thing of that past, as it quickly and easily worked through flank steak while once again producing neat, uniform slices. Sushi-grade tuna is another food that really shows off how the C-200's increased sharpness is better at preserving the delicate texture of the fish. Sam Rutherford for Engadget My favorite application of the C-200 was when I was doing prep for Taiwanese braised pork (aka 滷肉飯). Despite this being one of my most beloved dishes that I taught myself how to cook because I couldn't easily find it from local restaurants, I don't make it very often because it's a lot of work to cut multiple pounds of pork belly into small lardon-shaped pieces. Here, the knife's vibrations made it so much easier to cut through all those layers of meat, fat and skin. If there's any situation where the C-200 makes it 50 percent easier to slice through something, it's this. It might be hard to tell, but I was able to cut chives a little finer and more neatly with the C-200 (left) than with my other knives. Sam Rutherford for Engadget During my testing, two small issues cropped up. While it was quite rare, the knife would sometimes emit a faint high-pitched whine. When I asked Seattle Ultrasonic's founder Scott Heimendinger about this behavior, he was rather frank, saying that this can occur when water or moisture accumulates in just the right spots on the blade. Furthermore, he said this only happens on a small number of V1 models, which the company is working to fix in the future. Thankfully, I don't mind, but if it bothers you, making the noise go away is as easy as wiping down the knife down with a cloth or paper towel. The C-200s battery can be easily removed for cleaning and charging. Sam Rutherford for Engadget The other complication came while I was working through the multiple pounds of pork belly I mentioned earlier. After 10 to 15 minutes of continuous use, the knife beeped and its indicator light turned red. Turns out the knife had overheated, which was something I had not even considered. This led to higher-than-normal temperatures inside the knife's sealed electronics causing it to shut off. But after just 30 seconds, it returned to form. During later uses, I learned that simply taking my finger off the button between tasks, which happens naturally as you prep anyway, was more than enough to stop that situation from happening ever again. On the flipside, I was happy to discover that despite lacking a full tang, the C-200 can handle fairly rough tasks, including laying the knife on its side to smash garlic or jamming it into an avocado to remove its pit. That said, I would really recommend against doing the latter, because between its inherent sharpness and its vibration tech, this is the first knife I've used that can slice cleanly through an entire avocado with almost no extra effort. Cleaning and care The Seattle Ultrasonics C-200 8-inch chef's knife features an IP65 rating for the whole device, though the front half is actually a bit more resistant thanks to an IP67 rating for its button and bolster. Sam Rutherford for Engadget The last big concern about a knife with built-in electronics is how it handles clean-up. Thankfully, the C-200 features an IP65 rating for dust and water resistance. That means it can withstand rinsing and splashes without issue. And it's actually even tougher than that, because the front of the knife, including its bolster and button, are rated IP67. This means it can take full submersions in water if need be. However, just because you can, doesn't mean you should. Good kitchen protocol says you don't throw knives you care about in the sink and forget them, just like how you wouldn't put one in the dishwasher either. But perhaps the greatest advantage of this tech is that it allows you to go longer between needing to get your knives sharpened, which if you're like most home cooks, is probably never. To be clear, I haven't tested this and in some respects I wish I had been able to test out a dull version of the C-200. That said, science dictates that slice for slice, an ultrasonic knife will simply cut better than an equivalent blade without the extra tech. So if you believe in the adage that a dull knife is more dangerous than a sharp one because you need to apply more force to get the same results, this is another bonus for both safety and convenience. The not-so-optional accessory Seattle Ultrasonics' wireless charging tile makes it incredibly easy to forget that the C-200 needs to be topped up between uses. Sam Rutherford for Engadget I fully admit the need to keep a knife charged up is a major annoyance and something I or anyone else probably doesn't want to do. Thankfully, Seattle Ultrasonics thought of that by including support for wireless charging via the C-200's magnetic tile and it's dead simple to use. Just toss it on the charger when you're not using and it will take care of itself, so you never have to worry about how much of its normal 20-minute runtime it may or may not have left. There are also holes around back so you can easily mount the charger on a wall or shelf. In short, the added convenience the charging tile brings is so valuable that I don't really consider it an optional accessory. If you're getting the C-200, you need to buy this too, which sadly means you're looking at an all-in price of $500 for the bundle instead of just $400 for the knife by itself. Wrap-up As much as I love old-school knives, they'll simply never be as sharp an equivalent blade with this newfound tech. Sam Rutherford for Engadget After using the C-200, I don't think people need to rush out and throw all their old-school knives in the trash. The beauty of an ultrasonic blade like this is that it can handle everything your old cutlery is meant for, but with the touch of a button, it delivers sharpness unlike anything you've experienced before. And while it has some quirks, they're nothing like the kind you typically encounter on first-gen gadgets. Its biggest drawback is that its magnetic charging tile feels like an essential accessory, but it adds extra cost on top of a product that already has a deservedly premium price tag. Even though I'm sure knife makers will continue tweaking blade shapes and alloy mixes from now until the end of time, the addition of ultrasonic vibrations to a chef's knife unlocks a completely new tier of performance. That's because this technology is additive. All it does is enhance what a blade already does best. And when you look at related gadgets in the maker space, I don't think it's a coincidence that there's a similar revolution that resulted in Adam Savage of Mythbusters fame naming a sonic cutter as one of his favorite things of 2025. When viewed that way, it makes me even more confident that the C-200 is the flagbearer for a new breed of kitchen knives. This article originally appeared on Engadget at https://www.engadget.com/home/kitchen-tech/seattle-ultrasonics-c-200-review-this-is-the-future-of-kitchen-knives-140000051.html?src=rss",
          "content": "There’s a type of knife tech often seen in science fiction that revolves around vibrating a blade to increase its sharpness. We’ve seen examples of this in franchises like Star Wars (vibroblades), Evangelion (the prog knife), Dune (pulse-swords) and the Marvel universe (vibranium), but what might surprise you is that the underlying science is sound. By vibrating a cutting tool at high frequencies, not only do you reduce friction, you essentially turn the blade into a saw, as tiny oscillations enhance the inherent sharpness of a blade. However, up until recently, this tech largely only existed in fiction or for large companies that have the money to utilize the tech on an industrial scale. But that's changing in a big way for home cooks this year thanks to Seattle Ultrasonics, which is releasing the world's first ultrasonic chef's knife: the C-200. After chopping, smashing and cooking with it for about a month, I'm convinced that this is the future of kitchen knives. Design From afar, the C-200 looks a lot like a regular 8-inch chef's knife, but with a slightly more contemporary design. It features a three-layer san mai blade made from Japanese AUS-10 steel with a 13-degree edge angle per side (26 degrees total). However, upon closer inspection, you'll notice there are some features that seem a bit out of place on a premium knife. The first is that the C-200 doesn't have a full tang, which is the back end of a blade that ideally extends into the handle to provide added strength and durability. This is usually a major no-no, particularly on a $400 knife. However, when you consider that Seattle Ultrasonics needed somewhere to put its vibration tech, there really isn't any room for it other than inside the knife's grip. You won't be able to feel it, but pressing this button allows the C-200's blade to vibrate 30,000 times per second. Sam Rutherford for Engadget The knife's second quirk is that the back of the plastic handle features small indicator lights on either side, which is obviously a bit weird. Furthermore, the entire gray section can be removed to reveal a small 1,100mAh battery with an onboard USB-C port. Frankly, the presence of a battery in a knife is just kind of funky. But hey, the power to vibrate the knife has to come from somewhere because it definitely isn't being generated by your hands. And while Seattle Ultrasonics doesn't include a charging adapter or cable in the box, I don't mind because the company wisely took cues from the larger gadget industry and went with a power spec that's already widely in use. Honestly, I wish more kitchen tech makers would do the same. However, the knife's biggest oddity is the big orange button on the bottom of its handle. This is what you use to make the blade vibrate, which it does at 33kHz. It's positioned well so that it's easy to press regardless of whether you do a traditional pinch grip or if you're a bit more casual and prefer to hold the knife only using its handle. In the future, I can see this button becoming a touch-sensitive sensor, but for now, it's simple and effective. Here's a small selection of knives I own sorted by weight (from top to bottom) compared to the C200: 6-inch Kyocera ceramic knife (97 grams), MAC molybdenum steel chef's knife (110g), Furtif Evercut titanium carbide chef's knife (190g), Seattle Ultrasonics C-200 (328g), Korin carbon steel cleaver (396g). Sam Rutherford for Engadget The main downside to the C-200's design is that at 328 grams (around 0.75 pounds) it's heavier and bulkier than a typical knife. When compared to other knives I own, which are made from a wide variety of materials including, ceramic, molybdenum steel, carbon steel and even titanium carbide, it weighs more than everything else aside from my big Chinese cleaver (396 grams). And while it fits nicely in my hand, my wife said it takes a bit more effort for her to wield. It's not too much to the point where you don't want to use it. But for quick tasks, sometimes I found myself subconsciously reaching for lighter options like my 6-inch ceramic knife, which weighs just 97 grams. How it works From a user standpoint, putting the C-200 to work couldn't be simpler. Just press the button and let the knife do its thing. The big difference from how knives like this work in sci-fi is that there's no audible hum or detectable vibration when it's on. It's practically silent (well, most of the time, but more on that later), so you have to trust that it's on or check the indicator light on the handle. That said, if you still don't believe anything is happening, you can run the edge of the blade under water or scrape it over some cut citrus, at which point the blade's vibration will atomize nearby liquid into a fine mist. It's a cool party trick that also doubles as a way to amp up a cocktail by adding a faint essence of lemon, lime or anything else you can think of. Pushing the C-200's button is super easy, regardless of what kind of knife grip you prefer. Sam Rutherford for Engadget Inside, the knife relies on PZT-8 piezoelectric ceramic crystals to generate up to 30,000 vibrations per second, which propagate down the blade and make the knife function as if it's sharper than it actually is. This all sounds rather fantastic, so how does it function in the real world? In-use To really put the C-200 through its paces, I cooked over a dozen meals that involved neatly slicing or preparing a wide variety of foods — including Hasselback potatoes, flank steak, pork belly, chives, sushi-grade tuna and all sorts of fruit. After prepping four pounds of pork belly with various knives, the C-200 really showed off how much of a difference its vibration tech makes. Sam Rutherford for Engadget In short, the C-200's effectiveness depends a lot on what you're chopping. For soft things like strawberries or a piece of cake, I didn't notice much of a difference. To make things even more difficult, the knife arrived out of the box with an incredibly fine edge — the kind that makes shearing through a sheet of paper child's play. So even though Seattle Ultrasonics says its knife can reduce cutting effort by up to 50 percent, there's not much gain to be had when slicing foods that could just as easily be cut by a butter knife. However, as I used it more, I found that the C-200 excels at cutting through delicate items like tomatoes, scallions and fish, where using a dull knife often results in bruising the food as you chop. This was most evident when I made poke at home, where Seattle Ultrasonic's knife delivered cleaner, more precise cuts than anything else I own. For me, the C-200's $399 price tag is almost worth it just so I have an an easier time making my one of my all-time favorite dishes (lu rou fan). Sam Rutherford for Engadget When I whipped up some pico de gallo, I distinctly noticed how neatly the C-200 sliced through the skin of a tomato, instead of initially putting a crease in it before cleanly passing through its interior — which often happens when using dull knives. An additional benefit is that because of the vibrations, I found some foods like garlic didn't stick to the side of the blade as much. This made it easier to keep track of how much I chopped while simultaneously reducing the mess from things falling willy-nilly during prep. But perhaps the most obvious demonstration of the knife's prowess was when I diced an onion. When using my other knives or the C-200 without powering it on, I could feel when I tried to cut through thicker, more sturdy layers. But then, at the touch of a button, I was able to slice down with practically no resistance. It's almost shocking because it feels like magic. The C-200 truly excels at cutting denser foods like flank steak. Sadly mine ended up closer to medium than medium rare, but that's not the knife's fault. Sam Rutherford for Engadget The C-200 even has the ability to reduce the importance of certain knife techniques. Anyone who's seen all the posts on r/kitchenconfidential about cutting chives will already know what I'm talking about. As J Kenji Lopez-Alt neatly demonstrated, the ideal way to get crisp, clean slices is to do a subtle forward or back cut instead of simply chopping straight down. But with Seattle Ultrasonics' knife, I've found that it's so sharp you can get away with almost any motion and still get good results. And if you do it the right way, things are even better. Other types of food that makes the C-200 really shine are denser ingredients like meat and potatoes, where you can really feel the added cutting power. Previously, when I had to break down thick cuts of protein, I sometimes wished I owned a serrated electric knife. You know, the kind you break out once a year on Thanksgiving and then it sits and gathers dust for the other 364 days. But the C-200 made that desire a thing of that past, as it quickly and easily worked through flank steak while once again producing neat, uniform slices. Sushi-grade tuna is another food that really shows off how the C-200's increased sharpness is better at preserving the delicate texture of the fish. Sam Rutherford for Engadget My favorite application of the C-200 was when I was doing prep for Taiwanese braised pork (aka 滷肉飯). Despite this being one of my most beloved dishes that I taught myself how to cook because I couldn't easily find it from local restaurants, I don't make it very often because it's a lot of work to cut multiple pounds of pork belly into small lardon-shaped pieces. Here, the knife's vibrations made it so much easier to cut through all those layers of meat, fat and skin. If there's any situation where the C-200 makes it 50 percent easier to slice through something, it's this. It might be hard to tell, but I was able to cut chives a little finer and more neatly with the C-200 (left) than with my other knives. Sam Rutherford for Engadget During my testing, two small issues cropped up. While it was quite rare, the knife would sometimes emit a faint high-pitched whine. When I asked Seattle Ultrasonic's founder Scott Heimendinger about this behavior, he was rather frank, saying that this can occur when water or moisture accumulates in just the right spots on the blade. Furthermore, he said this only happens on a small number of V1 models, which the company is working to fix in the future. Thankfully, I don't mind, but if it bothers you, making the noise go away is as easy as wiping down the knife down with a cloth or paper towel. The C-200s battery can be easily removed for cleaning and charging. Sam Rutherford for Engadget The other complication came while I was working through the multiple pounds of pork belly I mentioned earlier. After 10 to 15 minutes of continuous use, the knife beeped and its indicator light turned red. Turns out the knife had overheated, which was something I had not even considered. This led to higher-than-normal temperatures inside the knife's sealed electronics causing it to shut off. But after just 30 seconds, it returned to form. During later uses, I learned that simply taking my finger off the button between tasks, which happens naturally as you prep anyway, was more than enough to stop that situation from happening ever again. On the flipside, I was happy to discover that despite lacking a full tang, the C-200 can handle fairly rough tasks, including laying the knife on its side to smash garlic or jamming it into an avocado to remove its pit. That said, I would really recommend against doing the latter, because between its inherent sharpness and its vibration tech, this is the first knife I've used that can slice cleanly through an entire avocado with almost no extra effort. Cleaning and care The Seattle Ultrasonics C-200 8-inch chef's knife features an IP65 rating for the whole device, though the front half is actually a bit more resistant thanks to an IP67 rating for its button and bolster. Sam Rutherford for Engadget The last big concern about a knife with built-in electronics is how it handles clean-up. Thankfully, the C-200 features an IP65 rating for dust and water resistance. That means it can withstand rinsing and splashes without issue. And it's actually even tougher than that, because the front of the knife, including its bolster and button, are rated IP67. This means it can take full submersions in water if need be. However, just because you can, doesn't mean you should. Good kitchen protocol says you don't throw knives you care about in the sink and forget them, just like how you wouldn't put one in the dishwasher either. But perhaps the greatest advantage of this tech is that it allows you to go longer between needing to get your knives sharpened, which if you're like most home cooks, is probably never. To be clear, I haven't tested this and in some respects I wish I had been able to test out a dull version of the C-200. That said, science dictates that slice for slice, an ultrasonic knife will simply cut better than an equivalent blade without the extra tech. So if you believe in the adage that a dull knife is more dangerous than a sharp one because you need to apply more force to get the same results, this is another bonus for both safety and convenience. The not-so-optional accessory Seattle Ultrasonics' wireless charging tile makes it incredibly easy to forget that the C-200 needs to be topped up between uses. Sam Rutherford for Engadget I fully admit the need to keep a knife charged up is a major annoyance and something I or anyone else probably doesn't want to do. Thankfully, Seattle Ultrasonics thought of that by including support for wireless charging via the C-200's magnetic tile and it's dead simple to use. Just toss it on the charger when you're not using and it will take care of itself, so you never have to worry about how much of its normal 20-minute runtime it may or may not have left. There are also holes around back so you can easily mount the charger on a wall or shelf. In short, the added convenience the charging tile brings is so valuable that I don't really consider it an optional accessory. If you're getting the C-200, you need to buy this too, which sadly means you're looking at an all-in price of $500 for the bundle instead of just $400 for the knife by itself. Wrap-up As much as I love old-school knives, they'll simply never be as sharp an equivalent blade with this newfound tech. Sam Rutherford for Engadget After using the C-200, I don't think people need to rush out and throw all their old-school knives in the trash. The beauty of an ultrasonic blade like this is that it can handle everything your old cutlery is meant for, but with the touch of a button, it delivers sharpness unlike anything you've experienced before. And while it has some quirks, they're nothing like the kind you typically encounter on first-gen gadgets. Its biggest drawback is that its magnetic charging tile feels like an essential accessory, but it adds extra cost on top of a product that already has a deservedly premium price tag. Even though I'm sure knife makers will continue tweaking blade shapes and alloy mixes from now until the end of time, the addition of ultrasonic vibrations to a chef's knife unlocks a completely new tier of performance. That's because this technology is additive. All it does is enhance what a blade already does best. And when you look at related gadgets in the maker space, I don't think it's a coincidence that there's a similar revolution that resulted in Adam Savage of Mythbusters fame naming a sonic cutter as one of his favorite things of 2025. When viewed that way, it makes me even more confident that the C-200 is the flagbearer for a new breed of kitchen knives. This article originally appeared on Engadget at https://www.engadget.com/home/kitchen-tech/seattle-ultrasonics-c-200-review-this-is-the-future-of-kitchen-knives-140000051.html?src=rss",
          "feed_position": 13,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/c200-button.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-122906428.html",
          "published_at": "Tue, 24 Feb 2026 12:29:12 +0000",
          "title": "The Morning After: What to expect at Samsung’s Galaxy Unpacked event tomorrow",
          "standfirst": "Samsung’s ready to launch its first new devices of 2026, and it’s got a Galaxy Unpacked event in San Francisco to stream everything. The keynote starts at 10AM PT (1PM ET) and will be livestreamed on YouTube. The announcement on February 10 also said this launch will mark “a new phase in the era of AI as intelligence becomes truly personal and adaptive.” What are we expecting? Based on leaked images of the new lineup, the company is not likely to have radically reinvented the look of the Galaxy S26, Galaxy S26+ or Galaxy S26 Ultra, sticking instead with a similar design to the Galaxy S25. We’re expecting Qualcomm’s new Snapdragon 8 Elite Gen 5 chip to be in all Samsung Galaxy S26 phones, though Korean news site Yonhap News reports Samsung’s relatively new Exynos 2600 chip could be in some devices, depending on the region. Despite the launch of the Qi 2 wireless charging standard, which added support for convenient magnetic alignment, Samsung still hasn’t brought the feature to its phones. Rumours suggest the S-series will have the spec’s higher speed wireless charging rates but will lack built-in magnets and have to depend on cases to add MagSafe-style charging and mounting. Honestly? It could be a pretty mild launch event, especially after wowing everyone with the Galaxy TriFold at the start of the year. Are you more interested in foldables than traditional candy bar devices? (If so, get ready for MWC 2026, kicking off next week. We’re expecting a lot of foldables.) — Mat Smith The other big stories (and deals) this morning The creators of Dark Sky have a new weather app Engadget review recap: Sony WF-1000XM6, ASUS Zenbook Duo and more Falcon Northwest FragBox review: A compact gaming rig that does everything right A new Evangelion series is coming Yoko Taro, creator of NieR, will pen it. Gainmax Yes, a truly new Neon Genesis Evangelion series is coming. The announcement came during a 30th-anniversary event in Japan. However, franchise creator Hideaki Anno won’t write the scripts. His replacement will be Yoko Taro, creator of the NieR video game series, who wears a giant spooky moon mask for interviews and game briefings. He’s also a cool guy underneath. Evangelion veteran Kazuya Tsurumaki will be on hand to direct episodes, produced by Studio Khara and Cloverworks. Continue reading. Bungie says ‘no second chances’ if you’re caught cheating in its new game Marathon players found cheating or developing cheats will receive a permaban. In a detailed blog post, Bungie took a very declarative position against those caught trying to cheat: “We are taking a strong stance against cheating and anyone found to be cheating or developing cheats will be permanently banned from playing Marathon forever, no second chances.” The blog post added that an appeals system will be in place. However, Bungie’s anti-cheat standards go beyond punishment. In the blog post, Bungie explained that Marathon’s dedicated servers have full authority on movement, shooting, actions and inventory. Since these key actions rely on the server, it will translate to smoother gunplay for players as well as the prevention of cheats related to teleportation, unlimited ammo or damage manipulation. Continue reading. This is the Nothing Phone 4(a) And it’s coming March 5. Nothing Looks like Nothing. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-122906428.html?src=rss",
          "content": "Samsung’s ready to launch its first new devices of 2026, and it’s got a Galaxy Unpacked event in San Francisco to stream everything. The keynote starts at 10AM PT (1PM ET) and will be livestreamed on YouTube. The announcement on February 10 also said this launch will mark “a new phase in the era of AI as intelligence becomes truly personal and adaptive.” What are we expecting? Based on leaked images of the new lineup, the company is not likely to have radically reinvented the look of the Galaxy S26, Galaxy S26+ or Galaxy S26 Ultra, sticking instead with a similar design to the Galaxy S25. We’re expecting Qualcomm’s new Snapdragon 8 Elite Gen 5 chip to be in all Samsung Galaxy S26 phones, though Korean news site Yonhap News reports Samsung’s relatively new Exynos 2600 chip could be in some devices, depending on the region. Despite the launch of the Qi 2 wireless charging standard, which added support for convenient magnetic alignment, Samsung still hasn’t brought the feature to its phones. Rumours suggest the S-series will have the spec’s higher speed wireless charging rates but will lack built-in magnets and have to depend on cases to add MagSafe-style charging and mounting. Honestly? It could be a pretty mild launch event, especially after wowing everyone with the Galaxy TriFold at the start of the year. Are you more interested in foldables than traditional candy bar devices? (If so, get ready for MWC 2026, kicking off next week. We’re expecting a lot of foldables.) — Mat Smith The other big stories (and deals) this morning The creators of Dark Sky have a new weather app Engadget review recap: Sony WF-1000XM6, ASUS Zenbook Duo and more Falcon Northwest FragBox review: A compact gaming rig that does everything right A new Evangelion series is coming Yoko Taro, creator of NieR, will pen it. Gainmax Yes, a truly new Neon Genesis Evangelion series is coming. The announcement came during a 30th-anniversary event in Japan. However, franchise creator Hideaki Anno won’t write the scripts. His replacement will be Yoko Taro, creator of the NieR video game series, who wears a giant spooky moon mask for interviews and game briefings. He’s also a cool guy underneath. Evangelion veteran Kazuya Tsurumaki will be on hand to direct episodes, produced by Studio Khara and Cloverworks. Continue reading. Bungie says ‘no second chances’ if you’re caught cheating in its new game Marathon players found cheating or developing cheats will receive a permaban. In a detailed blog post, Bungie took a very declarative position against those caught trying to cheat: “We are taking a strong stance against cheating and anyone found to be cheating or developing cheats will be permanently banned from playing Marathon forever, no second chances.” The blog post added that an appeals system will be in place. However, Bungie’s anti-cheat standards go beyond punishment. In the blog post, Bungie explained that Marathon’s dedicated servers have full authority on movement, shooting, actions and inventory. Since these key actions rely on the server, it will translate to smoother gunplay for players as well as the prevention of cheats related to teleportation, unlimited ammo or damage manipulation. Continue reading. This is the Nothing Phone 4(a) And it’s coming March 5. Nothing Looks like Nothing. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-122906428.html?src=rss",
          "feed_position": 16,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2026-02/f80765c0-117b-11f1-aeb3-f144f309e7bf"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-webcams-123047068.html",
          "published_at": "Tue, 24 Feb 2026 10:00:36 +0000",
          "title": "The best webcams for 2026",
          "standfirst": "Whether you’re on back-to-back video meetings, live streaming or just trying to look presentable on a family call, your webcam matters more than most might expect. The cameras built into laptops are fine in a pinch, but they rarely deliver consistent image quality, especially in less-than-ideal lighting. A dedicated webcam can noticeably improve sharpness, color accuracy and overall reliability. There’s no single “best” webcam for everyone, though. Some models are built around higher resolutions, while others focus on smoother video, better low-light performance or stronger onboard microphones. We’ve tested a wide range of options to see which ones are actually worth using day to day. Best webcams for 2026 Factors to consider before buying a webcam Resolution and field of view While some newer computers have 1080p webcams, most built-in cameras have a resolution of 720p, so you’ll want to look for an external webcam that has a higher resolution. FHD webcams will give you better video quality; ideally, you’re looking for something that can handle 1080p at 60fps or 30fps. If you’re considering a cheap 720p webcam, make sure to get one that supports at least 30fps (most will) or, even better, 60fps. However, if your primary concern is better picture quality during video calls, 1080p is the way to go. Some webcams can shoot in 4K, but that’s overkill for most people. Not to mention most video conferencing services like Zoom, Google Meet and Skype don’t even support 4K video. When it comes to streaming, Twitch maxes out at 1080p video, but YouTube added 4K live streaming back in 2016. Ultimately, with 4K webcam shots having such limited use, most people can get by with a solid 1080p camera. Field of view (FOV) controls how much can fit in the frame when you’re recording. Most webcams I tested had a default field of view of around 78 degrees, which captured me and enough of my background to prove that I really need to organize my home office. On cheaper webcams you’ll usually see narrower fields of view (around 60 degrees), and those aren’t necessarily bad. They won’t show as much of your background, but that also means you won’t be able to squeeze as many friends or family members into frame when you’re having Zoom birthday parties. On the flip side, more expensive webcams may let you adjust the field of view to be even wider than average, and some even offer features like digital zoom. Autofocus and other “auto” features Webcams with autofocus will keep the image quality sharp without much work on your part. You should be able to move around, step back and forth, and remain in focus the whole time. Some standalone webcam models let you manually adjust focus, too, if you have specific needs. Devices with fixed focus are less convenient, but they tend to be more affordable. In the same vein is auto framing, a feature that some high-end webcams now offer. Similarly to Apple’s Center Stage feature, the camera automatically adjusts to keep you in the center of the frame even as you move around. This used to be a feature only available on the most premium webcams, but now you can find it on sub-$200 devices. You’ll also see other “auto” features listed in webcam specs, most notably auto light correction. This will adjust the camera’s settings to make up for a dimly lit room. If you don’t have bright lights, or often take calls in places where you can’t control the lighting, this feature will be valuable. Alternatively, you might consider using your mirrorless camera as a high-quality webcam solution, taking all of the benefits and features with you (albeit in a cumbersome package). Microphones Most webcams have built-in microphones that, depending on your setup, might end up being closer to you than your computer’s own mics. Check to see if the model you’re considering has mono or stereo mics, as the latter is better. Some even use noise-reduction technology to keep your voice loud and clear. While audiophiles and streamers will want to invest in a standalone microphone, most others can get by using a webcam’s built-in mic. Design There aren’t a ton of fascinating breakthroughs when it comes to external webcam design. Most are round or rectangular devices that clip onto a monitor or your laptop screen. Some have the ability to swivel or screw onto a tripod stand and others can simply sit on your desk beside your computer. But unless you really like having people stare up your nose, the latter isn’t ideal. We recommend clipping your webcam to your monitor and ensuring that it’s at or slightly above eye level. A few webcams go above and beyond by adding hardware extras like built-in lights and lens covers, too. The former can help you stand out in a dark room, while the latter makes it so hackers can’t view you through your webcam without your knowledge. Price Most external webcams that are just good enough to be a step up from your computer’s built-in camera cost between $60 and $150. If the webcam has the same resolution as the internal one on your laptop, you should look out for other specs like auto light correction, a wider field of view or an extra-long connecting cable that can provide a step-up in quality or ease of use. Spending $150 or more means you might get advanced features that tend to be present in a pro webcam like 4K resolution, vertical and horizontal recording options, stereo mics, customizable video settings and more. But unless you’re spending hours on video calls each day or streaming multiple times each week, you can settle on a budget webcam and safely skip most of those high-end options. How we test webcams We primarily test webcams by putting them through as much real-world use as possible. We examine their design, how flexible they are and how easy they are to reposition, and make note of how heavy they are and if that affects their ability to stay put while sitting on top of a screen. We use each webcam for at least a week straight as our primary camera for all video chats, and we make sure to use the device in different lighting environments to test low-light performance. We also use any built-in microphones as our primary audio inputs on video calls as well. Finally, although most of these webcams are plug-and-play, we test out any proprietary software that’s intended to work with each webcam, tweaking things like field of view, video resolution and effects, and using any special features like Show Mode on Logitech webcams. Others webcams we tested Logitech C920s Pro HD Our previous top pick, the Logitech C920s Pro HD webcam remains a solid option for those with less than $100 to spend and really only need a basic 1080p camera to upgrade their setup, or something affordable to make them look better on those inevitable Zoom calls. It has a 78-degree field of view, decent microphones and handy privacy shutter built in. The Brio 500 took the top spot away from this model thanks to its advanced light correction, auto-framing and Show Mode. Webcam FAQs Should I get a 4K or 1080p webcam? It depends on how you plan to use it. A 1080p webcam is more than enough for most video calls, online classes and casual streaming. The picture looks clear, loads quickly and works well even on slower internet connections. A 4K webcam makes sense if you want sharper detail, especially for content creation, professional streaming or recordings you plan to upload. The extra resolution also helps if you crop or zoom in during a call without losing much quality. Keep in mind that 4K requires more bandwidth and not every platform supports it, so think about whether your setup and audience will benefit before spending more. Georgie Peru contributed to this report.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-webcams-123047068.html?src=rss",
          "content": "Whether you’re on back-to-back video meetings, live streaming or just trying to look presentable on a family call, your webcam matters more than most might expect. The cameras built into laptops are fine in a pinch, but they rarely deliver consistent image quality, especially in less-than-ideal lighting. A dedicated webcam can noticeably improve sharpness, color accuracy and overall reliability. There’s no single “best” webcam for everyone, though. Some models are built around higher resolutions, while others focus on smoother video, better low-light performance or stronger onboard microphones. We’ve tested a wide range of options to see which ones are actually worth using day to day. Best webcams for 2026 Factors to consider before buying a webcam Resolution and field of view While some newer computers have 1080p webcams, most built-in cameras have a resolution of 720p, so you’ll want to look for an external webcam that has a higher resolution. FHD webcams will give you better video quality; ideally, you’re looking for something that can handle 1080p at 60fps or 30fps. If you’re considering a cheap 720p webcam, make sure to get one that supports at least 30fps (most will) or, even better, 60fps. However, if your primary concern is better picture quality during video calls, 1080p is the way to go. Some webcams can shoot in 4K, but that’s overkill for most people. Not to mention most video conferencing services like Zoom, Google Meet and Skype don’t even support 4K video. When it comes to streaming, Twitch maxes out at 1080p video, but YouTube added 4K live streaming back in 2016. Ultimately, with 4K webcam shots having such limited use, most people can get by with a solid 1080p camera. Field of view (FOV) controls how much can fit in the frame when you’re recording. Most webcams I tested had a default field of view of around 78 degrees, which captured me and enough of my background to prove that I really need to organize my home office. On cheaper webcams you’ll usually see narrower fields of view (around 60 degrees), and those aren’t necessarily bad. They won’t show as much of your background, but that also means you won’t be able to squeeze as many friends or family members into frame when you’re having Zoom birthday parties. On the flip side, more expensive webcams may let you adjust the field of view to be even wider than average, and some even offer features like digital zoom. Autofocus and other “auto” features Webcams with autofocus will keep the image quality sharp without much work on your part. You should be able to move around, step back and forth, and remain in focus the whole time. Some standalone webcam models let you manually adjust focus, too, if you have specific needs. Devices with fixed focus are less convenient, but they tend to be more affordable. In the same vein is auto framing, a feature that some high-end webcams now offer. Similarly to Apple’s Center Stage feature, the camera automatically adjusts to keep you in the center of the frame even as you move around. This used to be a feature only available on the most premium webcams, but now you can find it on sub-$200 devices. You’ll also see other “auto” features listed in webcam specs, most notably auto light correction. This will adjust the camera’s settings to make up for a dimly lit room. If you don’t have bright lights, or often take calls in places where you can’t control the lighting, this feature will be valuable. Alternatively, you might consider using your mirrorless camera as a high-quality webcam solution, taking all of the benefits and features with you (albeit in a cumbersome package). Microphones Most webcams have built-in microphones that, depending on your setup, might end up being closer to you than your computer’s own mics. Check to see if the model you’re considering has mono or stereo mics, as the latter is better. Some even use noise-reduction technology to keep your voice loud and clear. While audiophiles and streamers will want to invest in a standalone microphone, most others can get by using a webcam’s built-in mic. Design There aren’t a ton of fascinating breakthroughs when it comes to external webcam design. Most are round or rectangular devices that clip onto a monitor or your laptop screen. Some have the ability to swivel or screw onto a tripod stand and others can simply sit on your desk beside your computer. But unless you really like having people stare up your nose, the latter isn’t ideal. We recommend clipping your webcam to your monitor and ensuring that it’s at or slightly above eye level. A few webcams go above and beyond by adding hardware extras like built-in lights and lens covers, too. The former can help you stand out in a dark room, while the latter makes it so hackers can’t view you through your webcam without your knowledge. Price Most external webcams that are just good enough to be a step up from your computer’s built-in camera cost between $60 and $150. If the webcam has the same resolution as the internal one on your laptop, you should look out for other specs like auto light correction, a wider field of view or an extra-long connecting cable that can provide a step-up in quality or ease of use. Spending $150 or more means you might get advanced features that tend to be present in a pro webcam like 4K resolution, vertical and horizontal recording options, stereo mics, customizable video settings and more. But unless you’re spending hours on video calls each day or streaming multiple times each week, you can settle on a budget webcam and safely skip most of those high-end options. How we test webcams We primarily test webcams by putting them through as much real-world use as possible. We examine their design, how flexible they are and how easy they are to reposition, and make note of how heavy they are and if that affects their ability to stay put while sitting on top of a screen. We use each webcam for at least a week straight as our primary camera for all video chats, and we make sure to use the device in different lighting environments to test low-light performance. We also use any built-in microphones as our primary audio inputs on video calls as well. Finally, although most of these webcams are plug-and-play, we test out any proprietary software that’s intended to work with each webcam, tweaking things like field of view, video resolution and effects, and using any special features like Show Mode on Logitech webcams. Others webcams we tested Logitech C920s Pro HD Our previous top pick, the Logitech C920s Pro HD webcam remains a solid option for those with less than $100 to spend and really only need a basic 1080p camera to upgrade their setup, or something affordable to make them look better on those inevitable Zoom calls. It has a 78-degree field of view, decent microphones and handy privacy shutter built in. The Brio 500 took the top spot away from this model thanks to its advanced light correction, auto-framing and Show Mode. Webcam FAQs Should I get a 4K or 1080p webcam? It depends on how you plan to use it. A 1080p webcam is more than enough for most video calls, online classes and casual streaming. The picture looks clear, loads quickly and works well even on slower internet connections. A 4K webcam makes sense if you want sharper detail, especially for content creation, professional streaming or recordings you plan to upload. The extra resolution also helps if you crop or zoom in during a call without losing much quality. Keep in mind that 4K requires more bandwidth and not every platform supports it, so think about whether your setup and audience will benefit before spending more. Georgie Peru contributed to this report.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-webcams-123047068.html?src=rss",
          "feed_position": 21,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2021-04/02284fb0-a11a-11eb-aafb-70e6f3b5aa36"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/how-smarsh-built-an-ai-front-door-for-regulated-industries-and-drove-59-self",
          "published_at": "Tue, 24 Feb 2026 05:00:00 GMT",
          "title": "How Smarsh built an AI front door for regulated industries — and drove 59% self-service adoption",
          "standfirst": "Presented by Salesforce Smarsh, a global provider of cloud-native, AI-driven solutions that capture, archive, and analyze communications data and intelligence for highly regulated industries, set an ambitious goal: use AI to scale its workforce and increase productivity by 30%. But its customer service team had already identified the real challenge — customers were navigating a maze of products, documentation, and compliance requirements.The solution wasn’t just more automation. It was a single, intelligent entry point into support.\"At the team level we asked ourselves, how can we become a better support organization for our regulated industry customers given that we keep on acquiring companies and have so many products to support?\" says Rohit Khanna, Smarsh chief customer officer. \"How do we harness the knowledge we have internally and present that to these customers in a way that makes our teams more efficient, and customer service more effective?\"In practice, that meant building an intelligent, human-centric “front door” trained on Smarsh’s proprietary knowledge. The system centralizes the support journey, distilling complex AI infrastructure into a simple, practical experience. Customers bypass complex navigation trees and describe what they need in plain language, and the AI directs them to the right solution — reducing the friction of traditional self-service.Archie, the Smarsh AI support agentSmarsh named its AI support agent \"Archie.\" While many AI initiatives stall during the last mile — the difficult transition from a successful pilot to a durable, production-scale operation — Smarsh avoided this by building on a deeply unified platform. The company chose Salesforce’s Agentforce 360 Platform to ensure Archie had the shared context, controlled execution, and orchestration required for an agentic enterprise.By deploying Agentforce rather than a bespoke DIY solution, Smarsh ensures Archie can plan and execute work across systems for smarter self-service and faster resolutions. This approach allows Smarsh to move work forward automatically across data and workflows, achieving greater efficiency without compromising the strict compliance rigor required by their industry. As a result, the company expects to see a 20% increase in its customer self-service success rates, 25% faster issue resolution compared to traditional self-service search and browse methods, and a 30% boost in service representative productivity.The bleeding edge of customer service AIBoth generative and agentic AI are rewriting the customer service playbook, yet the technology’s nascency can create intimidating hurdles. An organization can reap major rewards by moving decisively when launching AI initiatives, but it still requires care, forethought, and the right partnerships, Khanna says. Part of that is careful vendor choice.\"We&#x27;re a Salesforce shop,” he shared. “We use a core set of Salesforce products, including Data 360, Agentforce Service, Agentforce Sales and more, so it was wise to hang our hat on an AI agent provided to us by Salesforce rather than buying something from outside. We know that in the beginning, as new tech comes, it will be challenging, but Salesforce is up to the task and we&#x27;ll evolve together.\"From day one, effective AI has demanded a single non-negotiable prerequisite: clean, secure data. Grounding generative AI in an organization’s verified corporate knowledge and internal data slashes hallucination risk while delivering a significantly better user experience. Smarsh, however, didn’t wait for the industry to catch up. The company anticipated this need nearly half a decade ago, spending years meticulously rationalizing, annotating, and anonymizing its data to prepare for this exact moment. \"A lot of people run into challenges and don’t complete their AI projects because the data’s not ready and it&#x27;s not there,\" Khanna says. \"We started out strong, right out of the gate because our data was already clean and locked down, and today we’re in production with a service agent as we speak.\"Prioritizing data trustGiven Smarsh’s focus on regulatory compliance, Archie was introduced to replace the company’s previous self-service customer support chatbot. Janine Deegan, digital support program manager at Smarsh, worked with the Salesforce admin team on Smarsh&#x27;s Agentforce deployment. \"With Archie, the goal was to move beyond experimentation and make AI genuinely usable in a regulated environment. It wasn’t as simple as just switching on an agent; we had to build a system that gave that raw intelligence the context and control our industry actually requires, which is why we chose Salesforce,” Deegan says. “By connecting our documentation directly to Agentforce, which is backed by the Salesforce Trust Layer, we turned our static data into a live, trusted resource that handles the precision needed for a regulated space.\"Given its criticality, Khanna adds that maintaining pristine, secure documentation and data requires constant vigilance. To guarantee this, Smarsh erased the lines between departments, fusing the documentation team with the AI team. Now the two work in a tight loop: all of the material the document team produces, the AI team checks, verifies, and opens it up to the LLM. AI and regulatory compliance\"We’re in a compliance world. We’re custodians of archival data for all of our financial institutions, and our data is so sacred that we don’t give it away, \" Khanna explains. \"We have to be very cognizant of security and identity as we open up our systems to agentic AI.\"Infosec requirements were a critical consideration for rolling out Agentforce. Smarsh is regularly audited not just by regulatory bodies but also by the banks and financial institutions that have to comply with stringent data protection rules and ask for model risk management, (MRM). \"The safety regulators and banks ask for MRM,\" Khanna says. \"They say, ‘Tell me that all my data is not going to the public because it’s connecting with an LLM. Tell me about the LLM. Tell me about the model you’re using.’ We worked with Salesforce so we could get MRM approval for our customers. And thanks to Salesforce’s knowledge base and documentation, we&#x27;re always able to explain to these regulatory bodies what and why Archie is answering.\" Boosting customer adoptionCustomer buy-in is always a major challenge when it comes to new AI tools, and Archie was no exception. On the initial rollout of the new interface, some customers were confused by the new text box in the center of their screen and didn’t immediately understand how to interact with it.“We learned the hard way that we needed better change management, and to make sure our industry customers understood they could simply ask questions in natural language,” Khanna says.Personalization, they soon realized, was the key to gen AI adoption. \"Once customers had a better understanding of how Archie could be used for more efficient self-service, suddenly our adoption rate went up to 59%,\" he says. \"Personalization was very critical for us. Now we see the uptake, and we hope to see that continue when we roll out Archie to the rest of our products.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by Salesforce Smarsh, a global provider of cloud-native, AI-driven solutions that capture, archive, and analyze communications data and intelligence for highly regulated industries, set an ambitious goal: use AI to scale its workforce and increase productivity by 30%. But its customer service team had already identified the real challenge — customers were navigating a maze of products, documentation, and compliance requirements.The solution wasn’t just more automation. It was a single, intelligent entry point into support.\"At the team level we asked ourselves, how can we become a better support organization for our regulated industry customers given that we keep on acquiring companies and have so many products to support?\" says Rohit Khanna, Smarsh chief customer officer. \"How do we harness the knowledge we have internally and present that to these customers in a way that makes our teams more efficient, and customer service more effective?\"In practice, that meant building an intelligent, human-centric “front door” trained on Smarsh’s proprietary knowledge. The system centralizes the support journey, distilling complex AI infrastructure into a simple, practical experience. Customers bypass complex navigation trees and describe what they need in plain language, and the AI directs them to the right solution — reducing the friction of traditional self-service.Archie, the Smarsh AI support agentSmarsh named its AI support agent \"Archie.\" While many AI initiatives stall during the last mile — the difficult transition from a successful pilot to a durable, production-scale operation — Smarsh avoided this by building on a deeply unified platform. The company chose Salesforce’s Agentforce 360 Platform to ensure Archie had the shared context, controlled execution, and orchestration required for an agentic enterprise.By deploying Agentforce rather than a bespoke DIY solution, Smarsh ensures Archie can plan and execute work across systems for smarter self-service and faster resolutions. This approach allows Smarsh to move work forward automatically across data and workflows, achieving greater efficiency without compromising the strict compliance rigor required by their industry. As a result, the company expects to see a 20% increase in its customer self-service success rates, 25% faster issue resolution compared to traditional self-service search and browse methods, and a 30% boost in service representative productivity.The bleeding edge of customer service AIBoth generative and agentic AI are rewriting the customer service playbook, yet the technology’s nascency can create intimidating hurdles. An organization can reap major rewards by moving decisively when launching AI initiatives, but it still requires care, forethought, and the right partnerships, Khanna says. Part of that is careful vendor choice.\"We&#x27;re a Salesforce shop,” he shared. “We use a core set of Salesforce products, including Data 360, Agentforce Service, Agentforce Sales and more, so it was wise to hang our hat on an AI agent provided to us by Salesforce rather than buying something from outside. We know that in the beginning, as new tech comes, it will be challenging, but Salesforce is up to the task and we&#x27;ll evolve together.\"From day one, effective AI has demanded a single non-negotiable prerequisite: clean, secure data. Grounding generative AI in an organization’s verified corporate knowledge and internal data slashes hallucination risk while delivering a significantly better user experience. Smarsh, however, didn’t wait for the industry to catch up. The company anticipated this need nearly half a decade ago, spending years meticulously rationalizing, annotating, and anonymizing its data to prepare for this exact moment. \"A lot of people run into challenges and don’t complete their AI projects because the data’s not ready and it&#x27;s not there,\" Khanna says. \"We started out strong, right out of the gate because our data was already clean and locked down, and today we’re in production with a service agent as we speak.\"Prioritizing data trustGiven Smarsh’s focus on regulatory compliance, Archie was introduced to replace the company’s previous self-service customer support chatbot. Janine Deegan, digital support program manager at Smarsh, worked with the Salesforce admin team on Smarsh&#x27;s Agentforce deployment. \"With Archie, the goal was to move beyond experimentation and make AI genuinely usable in a regulated environment. It wasn’t as simple as just switching on an agent; we had to build a system that gave that raw intelligence the context and control our industry actually requires, which is why we chose Salesforce,” Deegan says. “By connecting our documentation directly to Agentforce, which is backed by the Salesforce Trust Layer, we turned our static data into a live, trusted resource that handles the precision needed for a regulated space.\"Given its criticality, Khanna adds that maintaining pristine, secure documentation and data requires constant vigilance. To guarantee this, Smarsh erased the lines between departments, fusing the documentation team with the AI team. Now the two work in a tight loop: all of the material the document team produces, the AI team checks, verifies, and opens it up to the LLM. AI and regulatory compliance\"We’re in a compliance world. We’re custodians of archival data for all of our financial institutions, and our data is so sacred that we don’t give it away, \" Khanna explains. \"We have to be very cognizant of security and identity as we open up our systems to agentic AI.\"Infosec requirements were a critical consideration for rolling out Agentforce. Smarsh is regularly audited not just by regulatory bodies but also by the banks and financial institutions that have to comply with stringent data protection rules and ask for model risk management, (MRM). \"The safety regulators and banks ask for MRM,\" Khanna says. \"They say, ‘Tell me that all my data is not going to the public because it’s connecting with an LLM. Tell me about the LLM. Tell me about the model you’re using.’ We worked with Salesforce so we could get MRM approval for our customers. And thanks to Salesforce’s knowledge base and documentation, we&#x27;re always able to explain to these regulatory bodies what and why Archie is answering.\" Boosting customer adoptionCustomer buy-in is always a major challenge when it comes to new AI tools, and Archie was no exception. On the initial rollout of the new interface, some customers were confused by the new text box in the center of their screen and didn’t immediately understand how to interact with it.“We learned the hard way that we needed better change management, and to make sure our industry customers understood they could simply ask questions in natural language,” Khanna says.Personalization, they soon realized, was the key to gen AI adoption. \"Once customers had a better understanding of how Archie could be used for more efficient self-service, suddenly our adoption rate went up to 59%,\" he says. \"Personalization was very critical for us. Now we see the uptake, and we hope to see that continue when we roll out Archie to the rest of our products.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/tpGhXZcbNGfxfdbzyArgw/a690a08505d7402f5d7149901398e9bf/AdobeStock_109891076.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/google-clamps-down-on-antigravity-malicious-usage-cutting-off-openclaw-users",
          "published_at": "Mon, 23 Feb 2026 23:01:00 GMT",
          "title": "Google clamps down on Antigravity 'malicious usage', cutting off OpenClaw users in sweeping ToS enforcement move",
          "standfirst": "Google caused controversy among some developers this weekend and today, Monday, February 23rd, after restricting their usage of its new Antigravity \"vibe coding\" platform, alleging \"maliciously usage.\" Some users who had been using the open source autonomous AI agent OpenClaw in conjunction with agents built on Antigravity, as well as those who had connected OpenClaw agents to their Gmails, claimed on social media that they lost access to their Google accounts. According to Google, said users had been using Antigravity to access a larger number of Gemini tokens via third-party platforms like OpenClaw, which overwhelmed the system for other Antigravity customers. This move has cut off several users, underscoring the architectural and trust issues that can arise with OpenClaw. The timing of Google’s crackdown is particularly pointed. Just one week ago, on February 15, OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI to lead its “next generation of personal agents.” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival. By cutting off OpenClaw’s access to Antigravity, Google isn’t just protecting its server load; it is effectively severing a pipeline that allows an OpenAI-adjacent tool to leverage Google’s most advanced Gemini models.Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, said in an X post that the company noticed “malicious usage” that led to service degradation.“We’ve been seeing a massive increase in malicious usage of the Antigravity backend that has tremendously degraded the quality of service for our users. We needed to find a path to quickly shut off access to these users that are not using the product as intended. We understand that a subset of these users were not aware that this was against our ToS [Terms of Service] and will get a path for them to come back on but we have limited capacity and want to be fair to our actual users,” the post said. A Google DeepMind spokesperson told VentureBeat that the move is not to permanently ban the use of Antigravity to access third-party platforms, but to align its use with the platform’s terms of service. Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result. Infrastructure and connection uncertaintyOpenClaw emerged as a way for individual users to run shell commands and access local files, fulfilling a major promise of AI agents: efficiently running workflows for users. But, as VentureBeat has frequently pointed out, it can often run into security and guardrail issues. There are companies building ways for enterprise customers to access OpenClaw securely and with a governance layer, though OpenClaw is so new that we should expect more announcements soon.However, Google’s move was not framed as a security issue but rather as one of access and runtime, further showing that there is still significant uncertainty when users want to bring in something like OpenClaw into their workflow. This is not the first time developers and power users of agentic AI found their access curtailed. Last year, Anthropic throttled access to Claude Code after the company claimed some users were abusing the system by running it 24/7. What this does highlight is the disconnect between companies like Google and OpenClaw users. OpenClaw offered many interesting possibilities for creating workflows with agents. However, because it is continually evolving, users may inadvertently run afoul of ToS or rate limits. Mohan said Google is working to bring the banned users back, but whether this means the company will amend its ToS or figure out a secure connection between OpenClaw agents and Antigravity models remains to be seen. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Affected usersSeveral users said on both the Y Combinator chat boards and X that they no longer had access to their Google accounts after running OpenClaw instances for certain Google products. Google’s move mirrors a broader industry shift toward \"walled garden\" agent ecosystems. Earlier this year, Anthropic introduced \"client fingerprinting\" to ensure that its Claude Code environment remains the exclusive interface for its models, effectively locking out third-party wrappers like OpenClaw. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Some have said they will no longer use Google or Gemini for their projects. Right now, people who still want to keep using Antigravity will need to wait until Google figures out a way for them to use OpenClaw and access Gemini tokens in a manner Google deems “fair.” Google DeepMind reiterated that it had only cut access to Antigravity, not to other Google applications. Conclusion: the enterprise takeawayFor enterprise technical decision-makers, the \"Antigravity Ban\" serves as a definitive case study in the risks of agentic dependency. As the industry moves from chatbots to autonomous agents, the following realities must now dictate strategy:Platform fragility is the new normal: The sudden lockout of $250/month \"Ultra\" users proves that even high-paying enterprise customers have little leverage when a provider decides to change its \"fair use\" definitions. Relying on OAuth-based third-party wrappers for core business logic is now a high-risk gamble.The rise of local-first governance: With OpenClaw moving toward an OpenAI-backed foundation and Google/Anthropic tightening their clouds, enterprises should prioritize agent frameworks that can run \"local-first\" or within VPCs. The \"token loophole\" that OpenClaw exploited is being closed; future agentic scale will require direct, high-cost API contracts rather than subsidized consumer seats.Account portability as a requirement: The fact that users \"lost access to their Google accounts\" underscores the danger of bundling development environments with primary identity providers. Decision-makers should decouple AI development from core corporate identity (SSO) where possible to avoid a single ToS violation paralyzing an entire team&#x27;s communications.Ultimately, the Antigravity incident marks the end of the \"Wild West\" for AI agents. As Google and OpenAI stake their claims, the enterprise must choose between the stability of the walled garden or the complexity (and cost) of truly independent, self-hosted infrastructure.",
          "content": "Google caused controversy among some developers this weekend and today, Monday, February 23rd, after restricting their usage of its new Antigravity \"vibe coding\" platform, alleging \"maliciously usage.\" Some users who had been using the open source autonomous AI agent OpenClaw in conjunction with agents built on Antigravity, as well as those who had connected OpenClaw agents to their Gmails, claimed on social media that they lost access to their Google accounts. According to Google, said users had been using Antigravity to access a larger number of Gemini tokens via third-party platforms like OpenClaw, which overwhelmed the system for other Antigravity customers. This move has cut off several users, underscoring the architectural and trust issues that can arise with OpenClaw. The timing of Google’s crackdown is particularly pointed. Just one week ago, on February 15, OpenAI CEO Sam Altman announced that OpenClaw creator Peter Steinberger had joined OpenAI to lead its “next generation of personal agents.” While OpenClaw remains an open-source project under an independent foundation, it is now financially backed and strategically guided by Google’s primary rival. By cutting off OpenClaw’s access to Antigravity, Google isn’t just protecting its server load; it is effectively severing a pipeline that allows an OpenAI-adjacent tool to leverage Google’s most advanced Gemini models.Google DeepMind engineer and former CEO and founder of Windsurf, Varun Mohan, said in an X post that the company noticed “malicious usage” that led to service degradation.“We’ve been seeing a massive increase in malicious usage of the Antigravity backend that has tremendously degraded the quality of service for our users. We needed to find a path to quickly shut off access to these users that are not using the product as intended. We understand that a subset of these users were not aware that this was against our ToS [Terms of Service] and will get a path for them to come back on but we have limited capacity and want to be fair to our actual users,” the post said. A Google DeepMind spokesperson told VentureBeat that the move is not to permanently ban the use of Antigravity to access third-party platforms, but to align its use with the platform’s terms of service. Unsurprisingly, Google’s move has caused a furor among OpenClaw users, including from OpenClaw creator Peter Steinberger, who announced that OpenClaw will remove Google support as a result. Infrastructure and connection uncertaintyOpenClaw emerged as a way for individual users to run shell commands and access local files, fulfilling a major promise of AI agents: efficiently running workflows for users. But, as VentureBeat has frequently pointed out, it can often run into security and guardrail issues. There are companies building ways for enterprise customers to access OpenClaw securely and with a governance layer, though OpenClaw is so new that we should expect more announcements soon.However, Google’s move was not framed as a security issue but rather as one of access and runtime, further showing that there is still significant uncertainty when users want to bring in something like OpenClaw into their workflow. This is not the first time developers and power users of agentic AI found their access curtailed. Last year, Anthropic throttled access to Claude Code after the company claimed some users were abusing the system by running it 24/7. What this does highlight is the disconnect between companies like Google and OpenClaw users. OpenClaw offered many interesting possibilities for creating workflows with agents. However, because it is continually evolving, users may inadvertently run afoul of ToS or rate limits. Mohan said Google is working to bring the banned users back, but whether this means the company will amend its ToS or figure out a secure connection between OpenClaw agents and Antigravity models remains to be seen. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Affected usersSeveral users said on both the Y Combinator chat boards and X that they no longer had access to their Google accounts after running OpenClaw instances for certain Google products. Google’s move mirrors a broader industry shift toward \"walled garden\" agent ecosystems. Earlier this year, Anthropic introduced \"client fingerprinting\" to ensure that its Claude Code environment remains the exclusive interface for its models, effectively locking out third-party wrappers like OpenClaw. For developers, the message is clear: the era of \"bring your own agent\" to a frontier model is ending. Providers are now prioritizing vertically integrated experiences where they can capture 100% of the telemetry and subscription revenue, often at the expense of the open-source interoperability that defined the early days of the LLM boom.Some have said they will no longer use Google or Gemini for their projects. Right now, people who still want to keep using Antigravity will need to wait until Google figures out a way for them to use OpenClaw and access Gemini tokens in a manner Google deems “fair.” Google DeepMind reiterated that it had only cut access to Antigravity, not to other Google applications. Conclusion: the enterprise takeawayFor enterprise technical decision-makers, the \"Antigravity Ban\" serves as a definitive case study in the risks of agentic dependency. As the industry moves from chatbots to autonomous agents, the following realities must now dictate strategy:Platform fragility is the new normal: The sudden lockout of $250/month \"Ultra\" users proves that even high-paying enterprise customers have little leverage when a provider decides to change its \"fair use\" definitions. Relying on OAuth-based third-party wrappers for core business logic is now a high-risk gamble.The rise of local-first governance: With OpenClaw moving toward an OpenAI-backed foundation and Google/Anthropic tightening their clouds, enterprises should prioritize agent frameworks that can run \"local-first\" or within VPCs. The \"token loophole\" that OpenClaw exploited is being closed; future agentic scale will require direct, high-cost API contracts rather than subsidized consumer seats.Account portability as a requirement: The fact that users \"lost access to their Google accounts\" underscores the danger of bundling development environments with primary identity providers. Decision-makers should decouple AI development from core corporate identity (SSO) where possible to avoid a single ToS violation paralyzing an entire team&#x27;s communications.Ultimately, the Antigravity incident marks the end of the \"Wild West\" for AI agents. As Google and OpenAI stake their claims, the enterprise must choose between the stability of the walled garden or the complexity (and cost) of truly independent, self-hosted infrastructure.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/vwGThxYtgeCfHxjn466tk/5630229b22b3017cc1edff8e5021d66a/crimedy7_illustration_of_a_lobster_thats_in_a_cage_--ar_169_-_2081b356-9d2f-480b-9bef-99c11d44bb10_1.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/one-engineer-made-a-production-saas-product-in-an-hour-heres-the-governance",
          "published_at": "Mon, 23 Feb 2026 22:50:00 GMT",
          "title": "One engineer made a production SaaS product in an hour: here's the governance system that made it possible",
          "standfirst": "Every engineering leader watching the agentic coding wave is eventually going to face the same question: if AI can generate production-quality code faster than any team, what does governance look like when the human isn&#x27;t writing the code anymore?Most teams don&#x27;t have a good answer yet. Treasure Data, a SoftBank-backed customer data platform serving more than 450 global brands, now has one, though they learned parts of it the hard way.The company today officially announced Treasure Code, a new AI-native command-line interface that lets data engineers and platform teams operate its full CDP through natural language, with Claude Code handling creation and iteration underneath. It was built by a single engineer. The company says the coding itself took roughly 60 minutes. But that number is almost beside the point. The more important story is what had to be true before those 60 minutes were possible, and what broke after.\"From a planning standpoint, we still have to plan to derisk the business, and that did take a couple of weeks,\" Rafa Flores, Chief Product Officer at Treasure Data, told VentureBeat. \"From an ideation and execution standpoint, that&#x27;s where you kind of just blend the two and you just go, go, go. And it&#x27;s not just prototyping, it&#x27;s rolling things out in production in a safe way.\"Build the governance layer firstBefore even a single line of code was written, Treasure Data had to answer a harder question: what does the system need to be prohibited from doing, and how do you enforce that at the platform level rather than hoping the code respects it?The guardrails Treasure Data built live upstream of the code itself. When any user connects to the CDP through Treasure Code, access control and permission management are inherited directly from the platform. Users can only reach resources they already have permission for. PII cannot be exposed. API keys cannot be surfaced. The system cannot speak disparagingly about a brand or competitor.\"We had to get CISOs involved. I was involved. Our CTO, heads of engineering, just to make sure that this thing didn&#x27;t just go rogue,\" Flores said.This foundation made the next step possible: letting AI generate 100% of the codebase, with a three-tier quality pipeline enforcing production standards throughout.The three-tier pipeline for AI code generation The first tier is an AI-based code reviewer also using Claude Code. The code reviewer sits at the pull request stage and runs a structured review checklist against every proposed merge, checking for architectural alignment, security compliance, proper error handling, test coverage and documentation quality. When all criteria are satisfied it can merge automatically. When they aren&#x27;t, it flags for human intervention.The fact that Treasure Data built the code reviewer in Claude Code is not incidental. It means the tool validating AI-generated code was itself AI-generated, a proof point that the workflow is self-reinforcing rather than dependent on a separate human-written quality layer.The second tier is a standard CI/CD pipeline running automated unit, integration and end-to-end tests, static analysis, linting and security checks against every change. The third is human review, required wherever automated systems flag risk or enterprise policy demands sign-off.The internal principle Treasure Data operates under: AI writes code, but AI does not ship code.Why this isn&#x27;t just Cursor pointed at a databaseThe obvious question for any engineering team is why not just point an existing tool like Cursor at your data platform, or expose it as an MCP server and let Claude Code query it directly.Flores argued the difference is governance depth. A generic connection gives you natural language access to data but inherits none of the platform&#x27;s existing permission structures, meaning every query runs with whatever access the API key allows. Treasure Code inherits Treasure Data&#x27;s full access control and permissioning layer, so what a user can do through natural language is bounded by what they&#x27;re already authorized to do in the platform. The second distinction is orchestration. Because Treasure Code connects directly to Treasure Data&#x27;s AI Agent Foundry, it can coordinate sub-agents and skills across the platform rather than executing single tasks in isolation: the difference between telling an AI to run an analysis and having it orchestrate that analysis across omni-channel activation, segmentation and reporting simultaneously.What broke anywayEven with the governance architecture in place, the launch didn&#x27;t go cleanly, and Flores was candid about it.Treasure Data initially made Treasure Code available to customers without a go-to-market plan. The assumption was that it would stay quiet while the team figured out next steps. Customers found it anyway. More than 100 customers and close to 1,000 users adopted it within two weeks, entirely through organic discovery.\"We didn&#x27;t put any go-to-market motions behind it. We didn&#x27;t think people were going to find it. Well, they did,\" Flores said. \"We were left scrambling with, how do we actually do the go-to-market motions? Do we even do a beta, since technically it&#x27;s live?\"The unplanned adoption also created a compliance gap. Treasure Data is still in the process of formally certifying Treasure Code under its Trust AI compliance program, a certification it had not completed before the product reached customers.A second problem emerged when Treasure Data opened skill development to non-engineering teams. CSMs and account directors began building and submitting skills without understanding what would get approved and merged, creating significant wasted effort and a backlog of submissions that couldn&#x27;t clear the repository&#x27;s access policies.Enterprise validation and what&#x27;s still missingThomson Reuters is among the early adopters. Flores said that the company had been attempting to build an in-house AI agent platform and struggling to move fast enough. It connected with Treasure Data&#x27;s AI Agent Foundry to accelerate audience segmentation work, then extended into Treasure Code to customize and iterate more rapidly.The feedback, Flores said, has centered on extensibility and flexibility, and the fact that procurement was already done, removing a significant enterprise barrier to adoption.The gap Thomson Reuters has flagged, and that Flores acknowledges the product doesn&#x27;t yet address, is guidance on AI maturity. Treasure Code doesn&#x27;t tell users who should use it, what to tackle first, or how to structure access across different skill levels within an organization.\"AI that allows you to be leveraged, but also tells you how to leverage it, I think that&#x27;s very differentiated,\" Flores said. He sees it as the next meaningful layer to build.What engineering leaders should take from thisFlores has had time to reflect on what the experience actually taught him, and he was direct about what he&#x27;d change. Next time, he said, the release would stay internal first.\"We will release it internally only. I will not release it to anyone outside of the organization,\" he said. \"It will be more of a controlled release so we can actually learn what we&#x27;re actually being exposed to at lower risk.\"On skill development, the lesson was to establish clear criteria for what gets approved and merged before opening the process to teams outside engineering, not after.The common thread in both lessons is the same one that shaped the governance architecture and the three-tier pipeline: speed is only an advantage if the structure around it holds. For engineering leaders evaluating whether agentic coding is ready for production, the Treasure Data experience translates into three practical conclusions.Governance infrastructure has to precede the code, not follow it. The platform-level access controls and permission inheritance were what made it safe to let AI generate freely. Without that foundation, the speed advantage disappears because every output requires exhaustive manual review.A quality gate that doesn&#x27;t depend entirely on humans is not optional at scale. Build a quality gate that doesn&#x27;t depend entirely on humans. AI can review every pull request consistently, without fatigue, and check policy compliance systematically across the entire codebase. Human review remains essential, but as a final check rather than the primary quality mechanism.Plan for organic adoption. If the product works, people will find it before you&#x27;re ready. The compliance and go-to-market gaps Treasure Data is still closing are a direct result of underestimating that.\"Yes, vibe coding can work if done in a safe way and proper guardrails are in place,\" Flores said. \"Embrace it in a way to find means of not replacing the good work you do, but the tedious work that you can probably automate.\"",
          "content": "Every engineering leader watching the agentic coding wave is eventually going to face the same question: if AI can generate production-quality code faster than any team, what does governance look like when the human isn&#x27;t writing the code anymore?Most teams don&#x27;t have a good answer yet. Treasure Data, a SoftBank-backed customer data platform serving more than 450 global brands, now has one, though they learned parts of it the hard way.The company today officially announced Treasure Code, a new AI-native command-line interface that lets data engineers and platform teams operate its full CDP through natural language, with Claude Code handling creation and iteration underneath. It was built by a single engineer. The company says the coding itself took roughly 60 minutes. But that number is almost beside the point. The more important story is what had to be true before those 60 minutes were possible, and what broke after.\"From a planning standpoint, we still have to plan to derisk the business, and that did take a couple of weeks,\" Rafa Flores, Chief Product Officer at Treasure Data, told VentureBeat. \"From an ideation and execution standpoint, that&#x27;s where you kind of just blend the two and you just go, go, go. And it&#x27;s not just prototyping, it&#x27;s rolling things out in production in a safe way.\"Build the governance layer firstBefore even a single line of code was written, Treasure Data had to answer a harder question: what does the system need to be prohibited from doing, and how do you enforce that at the platform level rather than hoping the code respects it?The guardrails Treasure Data built live upstream of the code itself. When any user connects to the CDP through Treasure Code, access control and permission management are inherited directly from the platform. Users can only reach resources they already have permission for. PII cannot be exposed. API keys cannot be surfaced. The system cannot speak disparagingly about a brand or competitor.\"We had to get CISOs involved. I was involved. Our CTO, heads of engineering, just to make sure that this thing didn&#x27;t just go rogue,\" Flores said.This foundation made the next step possible: letting AI generate 100% of the codebase, with a three-tier quality pipeline enforcing production standards throughout.The three-tier pipeline for AI code generation The first tier is an AI-based code reviewer also using Claude Code. The code reviewer sits at the pull request stage and runs a structured review checklist against every proposed merge, checking for architectural alignment, security compliance, proper error handling, test coverage and documentation quality. When all criteria are satisfied it can merge automatically. When they aren&#x27;t, it flags for human intervention.The fact that Treasure Data built the code reviewer in Claude Code is not incidental. It means the tool validating AI-generated code was itself AI-generated, a proof point that the workflow is self-reinforcing rather than dependent on a separate human-written quality layer.The second tier is a standard CI/CD pipeline running automated unit, integration and end-to-end tests, static analysis, linting and security checks against every change. The third is human review, required wherever automated systems flag risk or enterprise policy demands sign-off.The internal principle Treasure Data operates under: AI writes code, but AI does not ship code.Why this isn&#x27;t just Cursor pointed at a databaseThe obvious question for any engineering team is why not just point an existing tool like Cursor at your data platform, or expose it as an MCP server and let Claude Code query it directly.Flores argued the difference is governance depth. A generic connection gives you natural language access to data but inherits none of the platform&#x27;s existing permission structures, meaning every query runs with whatever access the API key allows. Treasure Code inherits Treasure Data&#x27;s full access control and permissioning layer, so what a user can do through natural language is bounded by what they&#x27;re already authorized to do in the platform. The second distinction is orchestration. Because Treasure Code connects directly to Treasure Data&#x27;s AI Agent Foundry, it can coordinate sub-agents and skills across the platform rather than executing single tasks in isolation: the difference between telling an AI to run an analysis and having it orchestrate that analysis across omni-channel activation, segmentation and reporting simultaneously.What broke anywayEven with the governance architecture in place, the launch didn&#x27;t go cleanly, and Flores was candid about it.Treasure Data initially made Treasure Code available to customers without a go-to-market plan. The assumption was that it would stay quiet while the team figured out next steps. Customers found it anyway. More than 100 customers and close to 1,000 users adopted it within two weeks, entirely through organic discovery.\"We didn&#x27;t put any go-to-market motions behind it. We didn&#x27;t think people were going to find it. Well, they did,\" Flores said. \"We were left scrambling with, how do we actually do the go-to-market motions? Do we even do a beta, since technically it&#x27;s live?\"The unplanned adoption also created a compliance gap. Treasure Data is still in the process of formally certifying Treasure Code under its Trust AI compliance program, a certification it had not completed before the product reached customers.A second problem emerged when Treasure Data opened skill development to non-engineering teams. CSMs and account directors began building and submitting skills without understanding what would get approved and merged, creating significant wasted effort and a backlog of submissions that couldn&#x27;t clear the repository&#x27;s access policies.Enterprise validation and what&#x27;s still missingThomson Reuters is among the early adopters. Flores said that the company had been attempting to build an in-house AI agent platform and struggling to move fast enough. It connected with Treasure Data&#x27;s AI Agent Foundry to accelerate audience segmentation work, then extended into Treasure Code to customize and iterate more rapidly.The feedback, Flores said, has centered on extensibility and flexibility, and the fact that procurement was already done, removing a significant enterprise barrier to adoption.The gap Thomson Reuters has flagged, and that Flores acknowledges the product doesn&#x27;t yet address, is guidance on AI maturity. Treasure Code doesn&#x27;t tell users who should use it, what to tackle first, or how to structure access across different skill levels within an organization.\"AI that allows you to be leveraged, but also tells you how to leverage it, I think that&#x27;s very differentiated,\" Flores said. He sees it as the next meaningful layer to build.What engineering leaders should take from thisFlores has had time to reflect on what the experience actually taught him, and he was direct about what he&#x27;d change. Next time, he said, the release would stay internal first.\"We will release it internally only. I will not release it to anyone outside of the organization,\" he said. \"It will be more of a controlled release so we can actually learn what we&#x27;re actually being exposed to at lower risk.\"On skill development, the lesson was to establish clear criteria for what gets approved and merged before opening the process to teams outside engineering, not after.The common thread in both lessons is the same one that shaped the governance architecture and the three-tier pipeline: speed is only an advantage if the structure around it holds. For engineering leaders evaluating whether agentic coding is ready for production, the Treasure Data experience translates into three practical conclusions.Governance infrastructure has to precede the code, not follow it. The platform-level access controls and permission inheritance were what made it safe to let AI generate freely. Without that foundation, the speed advantage disappears because every output requires exhaustive manual review.A quality gate that doesn&#x27;t depend entirely on humans is not optional at scale. Build a quality gate that doesn&#x27;t depend entirely on humans. AI can review every pull request consistently, without fatigue, and check policy compliance systematically across the entire codebase. Human review remains essential, but as a final check rather than the primary quality mechanism.Plan for organic adoption. If the product works, people will find it before you&#x27;re ready. The compliance and go-to-market gaps Treasure Data is still closing are a direct result of underestimating that.\"Yes, vibe coding can work if done in a safe way and proper guardrails are in place,\" Flores said. \"Embrace it in a way to find means of not replacing the good work you do, but the tedious work that you can probably automate.\"",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2LXjt32cwUg6zhRZwofUc7/6771abe5244e9fe9fff86de9ba0cf984/treasure-data-vibe-coding-smk1.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/anthropic-says-deepseek-moonshot-and-minimax-used-24-000-fake-accounts-to",
          "published_at": "Mon, 23 Feb 2026 22:20:00 GMT",
          "title": "Anthropic says DeepSeek, Moonshot, and MiniMax used 24,000 fake accounts to rip off Claude",
          "standfirst": "Anthropic dropped a bombshell on the artificial intelligence industry Monday, publicly accusing three prominent Chinese AI laboratories — DeepSeek, Moonshot AI, and MiniMax — of orchestrating coordinated, industrial-scale campaigns to siphon capabilities from its Claude models using tens of thousands of fraudulent accounts.The San Francisco-based company said the three labs collectively generated more than 16 million exchanges with Claude through approximately 24,000 fake accounts, all in violation of Anthropic&#x27;s terms of service and regional access restrictions. The campaigns, Anthropic said, are the most concrete and detailed public evidence to date of a practice that has haunted Silicon Valley for months: foreign competitors systematically using a technique called distillation to leapfrog years of research and billions of dollars in investment.\"These campaigns are growing in intensity and sophistication,\" Anthropic wrote in a technical blog post published Monday. \"The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.\"The disclosure marks a dramatic escalation in the simmering tensions between American and Chinese AI developers — and it arrives at a moment when Washington is actively debating whether to tighten or loosen export controls on the advanced chips that power AI training. Anthropic, led by CEO Dario Amodei, has been among the most vocal advocates for restricting chip sales to China, and the company explicitly connected Monday&#x27;s revelations to that policy fight.How AI distillation went from obscure research technique to geopolitical flashpointTo understand what Anthropic alleges, it helps to understand what distillation actually is — and how it evolved from an academic curiosity into the most contentious issue in the global AI race.At its core, distillation is a process of extracting knowledge from a larger, more powerful AI model — the \"teacher\" — to create a smaller, more efficient one — the \"student.\" The student model learns not from raw data, but from the teacher&#x27;s outputs: its answers, reasoning patterns, and behaviors. Done correctly, the student can achieve performance remarkably close to the teacher&#x27;s while requiring a fraction of the compute to train.As Anthropic itself acknowledged, distillation is \"a widely used and legitimate training method.\" Frontier AI labs, including Anthropic, routinely distill their own models to create smaller, cheaper versions for customers. But the same technique can be weaponized. A competitor can pose as a legitimate customer, bombard a frontier model with carefully crafted prompts, collect the outputs, and use those outputs to train a rival system — capturing capabilities that took years and hundreds of millions of dollars to develop.The technique burst into public consciousness in January 2025 when DeepSeek released its R1 reasoning model, which appeared to match or approach the performance of leading American models at dramatically lower cost. Databricks CEO Ali Ghodsi captured the industry&#x27;s anxiety at the time, telling CNBC: \"This distillation technique is just so extremely powerful and so extremely cheap, and it&#x27;s just available to anyone.\" He predicted the technique would usher in an era of intense competition for large language models.That prediction proved prescient. In the weeks following DeepSeek&#x27;s release, researchers at UC Berkeley said they recreated OpenAI&#x27;s reasoning model for just $450 in 19 hours. Researchers at Stanford and the University of Washington followed with their own version built in 26 minutes for under $50 in compute credits. The startup Hugging Face replicated OpenAI&#x27;s Deep Research feature as a 24-hour coding challenge. DeepSeek itself openly released a family of distilled models on Hugging Face — including versions built on top of Qwen and Llama architectures — under the permissive MIT license, with the model card explicitly stating that the DeepSeek-R1 series supports commercial use and allows for any modifications and derivative works, \"including, but not limited to, distillation for training other LLMs.\"But what Anthropic described Monday goes far beyond academic replication or open-source experimentation. The company detailed what it characterized as deliberate, covert, and large-scale intellectual property extraction by well-resourced commercial laboratories operating under the jurisdiction of the Chinese government.Anthropic traces 16 million fraudulent exchanges to researchers at DeepSeek, Moonshot, and MiniMaxAnthropic attributed each campaign \"with high confidence\" through IP address correlation, request metadata, infrastructure indicators, and corroboration from unnamed industry partners who observed the same actors on their own platforms. Each campaign specifically targeted what Anthropic described as Claude&#x27;s most differentiated capabilities: agentic reasoning, tool use, and coding.DeepSeek, the company that ignited the distillation debate, conducted what Anthropic described as the most technically sophisticated of the three operations, generating over 150,000 exchanges with Claude. Anthropic said DeepSeek&#x27;s prompts targeted reasoning capabilities, rubric-based grading tasks designed to make Claude function as a reward model for reinforcement learning, and — in a detail likely to draw particular political attention — the creation of \"censorship-safe alternatives to policy sensitive queries.\"Anthropic alleged that DeepSeek \"generated synchronized traffic across accounts\" with \"identical patterns, shared payment methods, and coordinated timing\" that suggested load balancing to maximize throughput while evading detection. In one particularly notable technique, Anthropic said DeepSeek&#x27;s prompts \"asked Claude to imagine and articulate the internal reasoning behind a completed response and write it out step by step — effectively generating chain-of-thought training data at scale.\" The company also alleged it observed tasks in which Claude was used to generate alternatives to politically sensitive queries about \"dissidents, party leaders, or authoritarianism,\" likely to train DeepSeek&#x27;s own models to steer conversations away from censored topics. Anthropic said it was able to trace these accounts to specific researchers at the lab.Moonshot AI, the Beijing-based creator of the Kimi models, ran the second-largest operation by volume at over 3.4 million exchanges. Anthropic said Moonshot targeted agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. The company employed \"hundreds of fraudulent accounts spanning multiple access pathways,\" making the campaign harder to detect as a coordinated operation. Anthropic attributed the campaign through request metadata that \"matched the public profiles of senior Moonshot staff.\" In a later phase, Anthropic said, Moonshot adopted a more targeted approach, \"attempting to extract and reconstruct Claude&#x27;s reasoning traces.\"MiniMax, the least publicly known of the three but the most prolific by volume, generated over 13 million exchanges — more than three-quarters of the total. Anthropic said MiniMax&#x27;s campaign focused on agentic coding, tool use, and orchestration. The company said it detected MiniMax&#x27;s campaign while it was still active, \"before MiniMax released the model it was training,\" giving Anthropic \"unprecedented visibility into the life cycle of distillation attacks, from data generation through to model launch.\" In a detail that underscores the urgency and opportunism Anthropic alleges, the company said that when it released a new model during MiniMax&#x27;s active campaign, MiniMax \"pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system.\"How proxy networks and &#x27;hydra cluster&#x27; architectures helped Chinese labs bypass Anthropic&#x27;s China banAnthropic does not currently offer commercial access to Claude in China, a policy it maintains for national security reasons. So how did these labs access the models at all?The answer, Anthropic said, lies in commercial proxy services that resell access to Claude and other frontier AI models at scale. Anthropic described these services as running what it calls \"hydra cluster\" architectures — sprawling networks of fraudulent accounts that distribute traffic across Anthropic&#x27;s API and third-party cloud platforms. \"The breadth of these networks means that there are no single points of failure,\" Anthropic wrote. \"When one account is banned, a new one takes its place.\" In one case, Anthropic said, a single proxy network managed more than 20,000 fraudulent accounts simultaneously, mixing distillation traffic with unrelated customer requests to make detection harder.The description suggests a mature and well-resourced infrastructure ecosystem dedicated to circumventing access controls — one that may serve many more clients than just the three labs Anthropic named.Why Anthropic framed distillation as a national security crisis, not just an IP disputeAnthropic did not treat this as a mere terms-of-service violation. The company embedded its technical disclosure within an explicit national security argument, warning that \"illicitly distilled models lack necessary safeguards, creating significant national security risks.\"The company argued that models built through illicit distillation are \"unlikely to retain\" the safety guardrails that American companies build into their systems — protections designed to prevent AI from being used to develop bioweapons, carry out cyberattacks, or enable mass surveillance. \"Foreign labs that distill American models can then feed these unprotected capabilities into military, intelligence, and surveillance systems,\" Anthropic wrote, \"enabling authoritarian governments to deploy frontier AI for offensive cyber operations, disinformation campaigns, and mass surveillance.\"This framing directly connects to the chip export control debate that Amodei has made a centerpiece of his public advocacy. In a detailed essay published in January 2025, Amodei argued that export controls are \"the most important determinant of whether we end up in a unipolar or bipolar world\" — a world where either only the U.S. and its allies possess the most powerful AI, or one where China achieves parity. He specifically noted at the time that he was \"not taking any position on reports of distillation from Western models\" and would \"just take DeepSeek at their word that they trained it the way they said in the paper.\"Monday&#x27;s disclosure is a sharp departure from that earlier restraint. Anthropic now argues that distillation attacks \"undermine\" export controls \"by allowing foreign labs, including those subject to the control of the Chinese Communist Party, to close the competitive advantage that export controls are designed to preserve through other means.\" The company went further, asserting that \"without visibility into these attacks, the apparently rapid advancements made by these labs are incorrectly taken as evidence that export controls are ineffective.\" In other words, Anthropic is arguing that what some observers interpreted as proof that Chinese labs can innovate around chip restrictions was actually, in significant part, the result of stealing American capabilities.The murky legal landscape around AI distillation may explain Anthropic&#x27;s political strategyAnthropic&#x27;s decision to frame this as a national security issue rather than a legal dispute may reflect the difficult reality that intellectual property law offers limited recourse against distillation.As a March 2025 analysis by the law firm Winston & Strawn noted, \"the legal landscape surrounding AI distillation is unclear and evolving.\" The firm&#x27;s attorneys observed that proving a copyright claim in this context would be challenging, since it remains unclear whether the outputs of AI models qualify as copyrightable creative expression. The U.S. Copyright Office affirmed in January 2025 that copyright protection requires human authorship, and that \"mere provision of prompts does not render the outputs copyrightable.\"The legal picture is further complicated by the way frontier labs structure output ownership. OpenAI&#x27;s terms of use, for instance, assign ownership of model outputs to the user — meaning that even if a company can prove extraction occurred, it may not hold copyrights over the extracted data. Winston & Strawn noted that this dynamic means \"even if OpenAI can present enough evidence to show that DeepSeek extracted data from its models, OpenAI likely does not have copyrights over the data.\" The same logic would almost certainly apply to Anthropic&#x27;s outputs.Contract law may offer a more promising avenue. Anthropic&#x27;s terms of service prohibit the kind of systematic extraction the company describes, and violation of those terms is a more straightforward legal claim than copyright infringement. But enforcing contractual terms against entities operating through proxy services and fraudulent accounts in a foreign jurisdiction presents its own formidable challenges.This may explain why Anthropic chose the national security frame over a purely legal one. By positioning distillation attacks as threats to export control regimes and democratic security rather than as intellectual property disputes, Anthropic appeals to policymakers and regulators who have tools — sanctions, entity list designations, enhanced export restrictions — that go far beyond what civil litigation could achieve.What Anthropic&#x27;s distillation crackdown means for every company running a frontier AI modelAnthropic outlined a multipronged defensive response. The company said it has built classifiers and behavioral fingerprinting systems designed to identify distillation attack patterns in API traffic, including detection of chain-of-thought elicitation used to construct reasoning training data. It is sharing technical indicators with other AI labs, cloud providers, and relevant authorities to build what it described as a more holistic picture of the distillation landscape. The company has also strengthened verification for educational accounts, security research programs, and startup organizations — the pathways most commonly exploited for setting up fraudulent accounts — and is developing model-level safeguards designed to reduce the usefulness of outputs for illicit distillation without degrading the experience for legitimate customers.But the company acknowledged that \"no company can solve this alone,\" calling for coordinated action across the industry, cloud providers, and policymakers.The disclosure is likely to reverberate through multiple ongoing policy debates. In Congress, the bipartisan No DeepSeek on Government Devices Act has already been introduced. Federal agencies including NASA have banned DeepSeek from employee devices. And the broader question of chip export controls — which the Trump administration has been weighing amid competing pressures from Nvidia and national security hawks — now has a new and vivid data point.For the AI industry&#x27;s technical decision-makers, the implications are immediate and practical. If Anthropic&#x27;s account is accurate, the proxy infrastructure enabling these attacks is vast, sophisticated, and adaptable — and it is not limited to targeting a single company. Every frontier AI lab with an API is a potential target. The era of treating model access as a simple commercial transaction may be coming to an end, replaced by one in which API security is as strategically important as the model weights themselves.Anthropic has now put names, numbers, and forensic detail behind accusations that the industry had only whispered about for months. Whether that evidence galvanizes the coordinated response the company is calling for — or simply accelerates an arms race between distillers and defenders — may depend on a question no classifier can answer: whether Washington sees this as an act of espionage or just the cost of doing business in an era when intelligence itself has become a commodity.",
          "content": "Anthropic dropped a bombshell on the artificial intelligence industry Monday, publicly accusing three prominent Chinese AI laboratories — DeepSeek, Moonshot AI, and MiniMax — of orchestrating coordinated, industrial-scale campaigns to siphon capabilities from its Claude models using tens of thousands of fraudulent accounts.The San Francisco-based company said the three labs collectively generated more than 16 million exchanges with Claude through approximately 24,000 fake accounts, all in violation of Anthropic&#x27;s terms of service and regional access restrictions. The campaigns, Anthropic said, are the most concrete and detailed public evidence to date of a practice that has haunted Silicon Valley for months: foreign competitors systematically using a technique called distillation to leapfrog years of research and billions of dollars in investment.\"These campaigns are growing in intensity and sophistication,\" Anthropic wrote in a technical blog post published Monday. \"The window to act is narrow, and the threat extends beyond any single company or region. Addressing it will require rapid, coordinated action among industry players, policymakers, and the global AI community.\"The disclosure marks a dramatic escalation in the simmering tensions between American and Chinese AI developers — and it arrives at a moment when Washington is actively debating whether to tighten or loosen export controls on the advanced chips that power AI training. Anthropic, led by CEO Dario Amodei, has been among the most vocal advocates for restricting chip sales to China, and the company explicitly connected Monday&#x27;s revelations to that policy fight.How AI distillation went from obscure research technique to geopolitical flashpointTo understand what Anthropic alleges, it helps to understand what distillation actually is — and how it evolved from an academic curiosity into the most contentious issue in the global AI race.At its core, distillation is a process of extracting knowledge from a larger, more powerful AI model — the \"teacher\" — to create a smaller, more efficient one — the \"student.\" The student model learns not from raw data, but from the teacher&#x27;s outputs: its answers, reasoning patterns, and behaviors. Done correctly, the student can achieve performance remarkably close to the teacher&#x27;s while requiring a fraction of the compute to train.As Anthropic itself acknowledged, distillation is \"a widely used and legitimate training method.\" Frontier AI labs, including Anthropic, routinely distill their own models to create smaller, cheaper versions for customers. But the same technique can be weaponized. A competitor can pose as a legitimate customer, bombard a frontier model with carefully crafted prompts, collect the outputs, and use those outputs to train a rival system — capturing capabilities that took years and hundreds of millions of dollars to develop.The technique burst into public consciousness in January 2025 when DeepSeek released its R1 reasoning model, which appeared to match or approach the performance of leading American models at dramatically lower cost. Databricks CEO Ali Ghodsi captured the industry&#x27;s anxiety at the time, telling CNBC: \"This distillation technique is just so extremely powerful and so extremely cheap, and it&#x27;s just available to anyone.\" He predicted the technique would usher in an era of intense competition for large language models.That prediction proved prescient. In the weeks following DeepSeek&#x27;s release, researchers at UC Berkeley said they recreated OpenAI&#x27;s reasoning model for just $450 in 19 hours. Researchers at Stanford and the University of Washington followed with their own version built in 26 minutes for under $50 in compute credits. The startup Hugging Face replicated OpenAI&#x27;s Deep Research feature as a 24-hour coding challenge. DeepSeek itself openly released a family of distilled models on Hugging Face — including versions built on top of Qwen and Llama architectures — under the permissive MIT license, with the model card explicitly stating that the DeepSeek-R1 series supports commercial use and allows for any modifications and derivative works, \"including, but not limited to, distillation for training other LLMs.\"But what Anthropic described Monday goes far beyond academic replication or open-source experimentation. The company detailed what it characterized as deliberate, covert, and large-scale intellectual property extraction by well-resourced commercial laboratories operating under the jurisdiction of the Chinese government.Anthropic traces 16 million fraudulent exchanges to researchers at DeepSeek, Moonshot, and MiniMaxAnthropic attributed each campaign \"with high confidence\" through IP address correlation, request metadata, infrastructure indicators, and corroboration from unnamed industry partners who observed the same actors on their own platforms. Each campaign specifically targeted what Anthropic described as Claude&#x27;s most differentiated capabilities: agentic reasoning, tool use, and coding.DeepSeek, the company that ignited the distillation debate, conducted what Anthropic described as the most technically sophisticated of the three operations, generating over 150,000 exchanges with Claude. Anthropic said DeepSeek&#x27;s prompts targeted reasoning capabilities, rubric-based grading tasks designed to make Claude function as a reward model for reinforcement learning, and — in a detail likely to draw particular political attention — the creation of \"censorship-safe alternatives to policy sensitive queries.\"Anthropic alleged that DeepSeek \"generated synchronized traffic across accounts\" with \"identical patterns, shared payment methods, and coordinated timing\" that suggested load balancing to maximize throughput while evading detection. In one particularly notable technique, Anthropic said DeepSeek&#x27;s prompts \"asked Claude to imagine and articulate the internal reasoning behind a completed response and write it out step by step — effectively generating chain-of-thought training data at scale.\" The company also alleged it observed tasks in which Claude was used to generate alternatives to politically sensitive queries about \"dissidents, party leaders, or authoritarianism,\" likely to train DeepSeek&#x27;s own models to steer conversations away from censored topics. Anthropic said it was able to trace these accounts to specific researchers at the lab.Moonshot AI, the Beijing-based creator of the Kimi models, ran the second-largest operation by volume at over 3.4 million exchanges. Anthropic said Moonshot targeted agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. The company employed \"hundreds of fraudulent accounts spanning multiple access pathways,\" making the campaign harder to detect as a coordinated operation. Anthropic attributed the campaign through request metadata that \"matched the public profiles of senior Moonshot staff.\" In a later phase, Anthropic said, Moonshot adopted a more targeted approach, \"attempting to extract and reconstruct Claude&#x27;s reasoning traces.\"MiniMax, the least publicly known of the three but the most prolific by volume, generated over 13 million exchanges — more than three-quarters of the total. Anthropic said MiniMax&#x27;s campaign focused on agentic coding, tool use, and orchestration. The company said it detected MiniMax&#x27;s campaign while it was still active, \"before MiniMax released the model it was training,\" giving Anthropic \"unprecedented visibility into the life cycle of distillation attacks, from data generation through to model launch.\" In a detail that underscores the urgency and opportunism Anthropic alleges, the company said that when it released a new model during MiniMax&#x27;s active campaign, MiniMax \"pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system.\"How proxy networks and &#x27;hydra cluster&#x27; architectures helped Chinese labs bypass Anthropic&#x27;s China banAnthropic does not currently offer commercial access to Claude in China, a policy it maintains for national security reasons. So how did these labs access the models at all?The answer, Anthropic said, lies in commercial proxy services that resell access to Claude and other frontier AI models at scale. Anthropic described these services as running what it calls \"hydra cluster\" architectures — sprawling networks of fraudulent accounts that distribute traffic across Anthropic&#x27;s API and third-party cloud platforms. \"The breadth of these networks means that there are no single points of failure,\" Anthropic wrote. \"When one account is banned, a new one takes its place.\" In one case, Anthropic said, a single proxy network managed more than 20,000 fraudulent accounts simultaneously, mixing distillation traffic with unrelated customer requests to make detection harder.The description suggests a mature and well-resourced infrastructure ecosystem dedicated to circumventing access controls — one that may serve many more clients than just the three labs Anthropic named.Why Anthropic framed distillation as a national security crisis, not just an IP disputeAnthropic did not treat this as a mere terms-of-service violation. The company embedded its technical disclosure within an explicit national security argument, warning that \"illicitly distilled models lack necessary safeguards, creating significant national security risks.\"The company argued that models built through illicit distillation are \"unlikely to retain\" the safety guardrails that American companies build into their systems — protections designed to prevent AI from being used to develop bioweapons, carry out cyberattacks, or enable mass surveillance. \"Foreign labs that distill American models can then feed these unprotected capabilities into military, intelligence, and surveillance systems,\" Anthropic wrote, \"enabling authoritarian governments to deploy frontier AI for offensive cyber operations, disinformation campaigns, and mass surveillance.\"This framing directly connects to the chip export control debate that Amodei has made a centerpiece of his public advocacy. In a detailed essay published in January 2025, Amodei argued that export controls are \"the most important determinant of whether we end up in a unipolar or bipolar world\" — a world where either only the U.S. and its allies possess the most powerful AI, or one where China achieves parity. He specifically noted at the time that he was \"not taking any position on reports of distillation from Western models\" and would \"just take DeepSeek at their word that they trained it the way they said in the paper.\"Monday&#x27;s disclosure is a sharp departure from that earlier restraint. Anthropic now argues that distillation attacks \"undermine\" export controls \"by allowing foreign labs, including those subject to the control of the Chinese Communist Party, to close the competitive advantage that export controls are designed to preserve through other means.\" The company went further, asserting that \"without visibility into these attacks, the apparently rapid advancements made by these labs are incorrectly taken as evidence that export controls are ineffective.\" In other words, Anthropic is arguing that what some observers interpreted as proof that Chinese labs can innovate around chip restrictions was actually, in significant part, the result of stealing American capabilities.The murky legal landscape around AI distillation may explain Anthropic&#x27;s political strategyAnthropic&#x27;s decision to frame this as a national security issue rather than a legal dispute may reflect the difficult reality that intellectual property law offers limited recourse against distillation.As a March 2025 analysis by the law firm Winston & Strawn noted, \"the legal landscape surrounding AI distillation is unclear and evolving.\" The firm&#x27;s attorneys observed that proving a copyright claim in this context would be challenging, since it remains unclear whether the outputs of AI models qualify as copyrightable creative expression. The U.S. Copyright Office affirmed in January 2025 that copyright protection requires human authorship, and that \"mere provision of prompts does not render the outputs copyrightable.\"The legal picture is further complicated by the way frontier labs structure output ownership. OpenAI&#x27;s terms of use, for instance, assign ownership of model outputs to the user — meaning that even if a company can prove extraction occurred, it may not hold copyrights over the extracted data. Winston & Strawn noted that this dynamic means \"even if OpenAI can present enough evidence to show that DeepSeek extracted data from its models, OpenAI likely does not have copyrights over the data.\" The same logic would almost certainly apply to Anthropic&#x27;s outputs.Contract law may offer a more promising avenue. Anthropic&#x27;s terms of service prohibit the kind of systematic extraction the company describes, and violation of those terms is a more straightforward legal claim than copyright infringement. But enforcing contractual terms against entities operating through proxy services and fraudulent accounts in a foreign jurisdiction presents its own formidable challenges.This may explain why Anthropic chose the national security frame over a purely legal one. By positioning distillation attacks as threats to export control regimes and democratic security rather than as intellectual property disputes, Anthropic appeals to policymakers and regulators who have tools — sanctions, entity list designations, enhanced export restrictions — that go far beyond what civil litigation could achieve.What Anthropic&#x27;s distillation crackdown means for every company running a frontier AI modelAnthropic outlined a multipronged defensive response. The company said it has built classifiers and behavioral fingerprinting systems designed to identify distillation attack patterns in API traffic, including detection of chain-of-thought elicitation used to construct reasoning training data. It is sharing technical indicators with other AI labs, cloud providers, and relevant authorities to build what it described as a more holistic picture of the distillation landscape. The company has also strengthened verification for educational accounts, security research programs, and startup organizations — the pathways most commonly exploited for setting up fraudulent accounts — and is developing model-level safeguards designed to reduce the usefulness of outputs for illicit distillation without degrading the experience for legitimate customers.But the company acknowledged that \"no company can solve this alone,\" calling for coordinated action across the industry, cloud providers, and policymakers.The disclosure is likely to reverberate through multiple ongoing policy debates. In Congress, the bipartisan No DeepSeek on Government Devices Act has already been introduced. Federal agencies including NASA have banned DeepSeek from employee devices. And the broader question of chip export controls — which the Trump administration has been weighing amid competing pressures from Nvidia and national security hawks — now has a new and vivid data point.For the AI industry&#x27;s technical decision-makers, the implications are immediate and practical. If Anthropic&#x27;s account is accurate, the proxy infrastructure enabling these attacks is vast, sophisticated, and adaptable — and it is not limited to targeting a single company. Every frontier AI lab with an API is a potential target. The era of treating model access as a simple commercial transaction may be coming to an end, replaced by one in which API security is as strategically important as the model weights themselves.Anthropic has now put names, numbers, and forensic detail behind accusations that the industry had only whispered about for months. Whether that evidence galvanizes the coordinated response the company is calling for — or simply accelerates an arms race between distillers and defenders — may depend on a question no classifier can answer: whether Washington sees this as an act of espionage or just the cost of doing business in an era when intelligence itself has become a commodity.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/vgR8Wyc97pPTG90zTzQGX/9988675ce7156041adcf19bf19895fde/nuneybits_Vector_art_of_giant_syringe_siphoning_circuit-lines_i_60db17a8-17a3-4f0a-bece-ed1cfb79d116.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/bungie-says-no-second-chances-if-youre-caught-cheating-in-marathon-191633998.html",
          "published_at": "Mon, 23 Feb 2026 19:16:33 +0000",
          "title": "Bungie says 'no second chances' if you're caught cheating in Marathon",
          "standfirst": "Bungie isn't taking any prisoners when it comes to cheating on its upcoming extraction shooter, Marathon. In a detailed blog post explaining its anti-cheat measures, Bungie took a very declarative position against those caught trying to gain an unfair advantage. \"We are taking a strong stance against cheating and anyone found to be cheating or developing cheats will be permanently banned from playing Marathon forever, no second chances,\" the blog post read, adding that there will be an appeals system in place. However, Bungie's anti-cheat standards go beyond punishment. In the blog post, Bungie detailed that Marathon's dedicated servers have full authority on movement, shooting, actions, and inventory. Since these key actions rely on the server, it will translate to smoother gunplay for players as well as the prevention of cheats related to teleportation, unlimited ammo or damage manipulation. Bungie is also incorporating a \"Fog of War\" system that limits an individual player's client to see only certain regions of a map, which should prevent wall hacks, ESP cheats or loot revealers. On top of these robust regulations, Bungie is utilizing BattlEye, a kernel-level anticheat that's seen with other popular multiplayer shooters like Fortnite, Rainbow Six Siege and Destiny 2. Bungie added that in the event of disconnecting, you'll be able to reconnect to your run without any hitches. If players can't reconnect due to an issue with the servers, Bungie said it will \"attempt to return the starting gear to all impacted players.\" Marathon isn't out until March 5, but Bungie is doing a preview weekend with the Server Slam event starting February 26. Still, it's obvious that Bungie already wants to get ahead of the competition, since Arc Raiders, another recently released extraction shooter, has been dealing with its own cheating problem. To address the rise in cheating, the game's developer, Embark Studios, implemented a three-strike system, which some players have criticized as too lenient.This article originally appeared on Engadget at https://www.engadget.com/gaming/bungie-says-no-second-chances-if-youre-caught-cheating-in-marathon-191633998.html?src=rss",
          "content": "Bungie isn't taking any prisoners when it comes to cheating on its upcoming extraction shooter, Marathon. In a detailed blog post explaining its anti-cheat measures, Bungie took a very declarative position against those caught trying to gain an unfair advantage. \"We are taking a strong stance against cheating and anyone found to be cheating or developing cheats will be permanently banned from playing Marathon forever, no second chances,\" the blog post read, adding that there will be an appeals system in place. However, Bungie's anti-cheat standards go beyond punishment. In the blog post, Bungie detailed that Marathon's dedicated servers have full authority on movement, shooting, actions, and inventory. Since these key actions rely on the server, it will translate to smoother gunplay for players as well as the prevention of cheats related to teleportation, unlimited ammo or damage manipulation. Bungie is also incorporating a \"Fog of War\" system that limits an individual player's client to see only certain regions of a map, which should prevent wall hacks, ESP cheats or loot revealers. On top of these robust regulations, Bungie is utilizing BattlEye, a kernel-level anticheat that's seen with other popular multiplayer shooters like Fortnite, Rainbow Six Siege and Destiny 2. Bungie added that in the event of disconnecting, you'll be able to reconnect to your run without any hitches. If players can't reconnect due to an issue with the servers, Bungie said it will \"attempt to return the starting gear to all impacted players.\" Marathon isn't out until March 5, but Bungie is doing a preview weekend with the Server Slam event starting February 26. Still, it's obvious that Bungie already wants to get ahead of the competition, since Arc Raiders, another recently released extraction shooter, has been dealing with its own cheating problem. To address the rise in cheating, the game's developer, Embark Studios, implemented a three-strike system, which some players have criticized as too lenient.This article originally appeared on Engadget at https://www.engadget.com/gaming/bungie-says-no-second-chances-if-youre-caught-cheating-in-marathon-191633998.html?src=rss",
          "feed_position": 25
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/falcon-northwest-fragbox-review-a-compact-gaming-rig-that-does-everything-right-130000837.html",
          "published_at": "Mon, 23 Feb 2026 18:51:26 +0000",
          "title": "Falcon Northwest FragBox review: A compact gaming rig that does everything right",
          "standfirst": "Mafia: The Old Country demands to be played on an enormous screen. As much as I love my 32-inch Alienware OLED gaming monitor, it doesn't do justice to Mafia's cinematic vistas of Sicily. But, I also wanted to play that game in its full 4K glory, with none of the compromises of today's game consoles. So why not just shove a tiny gaming desktop under my home theater? Enter the Fragbox, Falcon Northwest's revamped small form factor gaming PC. While it's very expensive, starting at $3,997, it's incredibly powerful and gives you the freedom to easily upgrade the hardware down the line. I know what you're thinking: \"A $4,000 desktop, in this economy?\" That pricing also doesn't include upgrading from the stock NVIDIA's RTX 5070 GPU, as well as adding more RAM and larger SSDs, all of which could drive the price up thousands more. I initially planned to review the FragBox back in early December 2025, before the AI-induced RAMaggedon made memory, storage and other components dramatically more expensive. Falcon Northwest is mainly known as a boutique and high-end system builder, so its wealthier clientele can likely weather the pricing storm. If you're looking for a deal, though, you won't find it here. So what, exactly, is a FragBox? Imagine a typical mid-tower desktop squashed down to a system that's only 10.2-inches tall, 10.5-inches wide and 15.9-inches deep. When Falcon initially debuted the FragBox in 2003, it was notable for being a genuinely small PC that used full-sized parts. That's still a main selling point today: It can still fit in large NVIDIA GPUs, including the beefy RTX 5090, as well as either Intel's latest Core Ultra chips or AMD's Ryzen 9000 CPUs. A huge 280mm radiator sits at the top pulling out hot air, and it also serves as an All-in-One (AIO) liquid cooler for the CPU. At 25 pounds, the FragBox isn't exactly light, but its sturdy metal handle makes it easy to move around. Most mid-tower desktops usually weigh between 20 and 35 pounds, depending on their case material. But they're also much larger and harder to squeeze into tight spaces. The FragBox's relatively squat size makes it easy to shove into a home entertainment center, or just sit on the corner of your desk. If you need a bit more height clearance, you can also remove the handle from the top panel. Just be sure there’s enough room for some airflow — all of that heat has to go somewhere, right? Falcon Northwest FragBox Devindra Hardawar for Engadget Despite its density, the FragBox's elegant design makes it a cinch to access to all of the system's components. Just unscrew the side and top panels and you can easily remove the GPU, RAM, storage and other major components. There are three slots of M.2 SSDs, as well as two locations for 2.5-inch drives and a spot for a large 3.5-inch HDD. The system is bundled with a 1,200W power supply, which should be more than enough to handle future GPUs and CPUs. Ports are plentiful as well: There are two USB-A and one USB-C connections right up front, alongside a headphone jack. On the rear, you've got your typical assortment of mid-tower connections, including four USB-A 2.0 connections, seven USB-A 3 ports, one 20G USB-C 3.2 port, 2.5G Ethernet, HDMI and DisplayPort. Our RTX 5090 review unit also included three DisplayPort jacks and one HDMI connection (which you'll see on most GPUs). Wi-Fi 6E was also built into our unit, but Falcon says that Wi-Fi 7 is now standard with new builds. Falcon Northwest FragBox Devindra Hardawar for Engadget The FragBox, thankfully, lacks the garish LEDs and cheesy thermal glass you find on more ostentatious gaming rigs. Falcon Northwest's aluminum case looks and feels stately, like an old-school luxury car. If you want something flashier, you can shell out an additional $400 for a custom UV printed case or $149 for a UV-printed front panel. Our review unit was equipped with AMD's Ryzen 9950X3D CPU, NVIDIA's RTX 5090, 96GB of DDR5 RAM and a 2TB SSD, which adds up to a whopping $7,995. Five months ago, it would have cost $7,047 —- you can thank the RAM shortage for the price jump. Even before benchmarking or running any games, I expected it to be a beast. In PCMark 10, the FragBox scored a whopping 13,810, which is around 500 points higher than my mid-tower system with the same CPU and GPU. It also scored the highest 3DMark Speedway and Port Royal ray tracing scores I've ever seen. Even more impressive, the FragBox's fans were barely audible under load, and the CPU and GPU sat at a chill 52C and 65C, respectively CPU GeekBench 6 CPU GeekBench 6 GPU Cinebench 2024 Falcon Northwest FragBox 3,445/22,787 390,148 N/A Desktop with AMD Ryzen 9 9950X3D, RTX 5090 3,366/18,950 381,400 134/2,124 Desktop with AMD Ryzen 9 7900X, RTX 5090 2,822/14,216 358,253 113/1,103 Apple Mac Studio M4 Max 4,090/26,394 116,028 190/2066 To get back to my initial point, it ran Mafia: The Old Country in 4K flawlessly, with every graphics setting cranked all the way up. While playing on my 120-inch projector home theater setup, the game reached 62 fps natively, and flipping on DLSS upscaling and frame generation bumped that up to 120 fps. Not that you need a super higher framerate for a slow-paced, mostly cinematic action game. I was just happy to be playing without any compromises — even the PS5 Pro can't reach the same level of graphical fidelity as the monstrously powerful RTX 5090. Falcon Northwest FragBox Devindra Hardawar for Engadget I'm no stranger to big-screen PC gaming, but previously I've had to run a laughably long HDMI cable from my desktop to make it work. I'm just too old for that mess now. And it also doesn't work consistently, especially at higher framerates, thanks to the massive bandwidth required to pump out 4K at high refresh rates. In-home game streaming is also an option, but that's not great when you're blowing games up to an enormous TV or projector screen. It's just too hard to ignore the imperfections of streaming compression. (Admittedly, I need to test newer high-bandwidth options, especially after I was impressed by NVIDIA's GeForce Now upgrade last year.) The FragBox also made it easy to jump into all of my recent Steam titles, including Mewgeneics and Arc Raiders on a big screen. Unfortunately, Windows itself remains a key stumbling block for home theater PC gaming. You'll still need to keep a keyboard and PC around to deal with the initial OS configuration. And even once I enabled Steam's Big Picture mode, which offers excellent controller options, I still occasionally had to deal with Windows Updates and other annoyances. Falcon Northwest FragBox Devindra Hardawar for Engadget Microsoft is currently trying to optimize Windows for gaming handhelds, and it's reportedly doing even more to make a future PC-powered Xbox feel more console-like. For now, though, using a Windows PC in your home theater doesn't feel much different than it did a decade ago. Steam is your savior, Windows is your enemy. Or you could just save thousands of dollars and buy a $500 PlayStation 5 or $700 PS5 Pro, instead. The latter will still get you smooth framerates and a healthy dose of ray tracing, without the annoyance of Windows, keyboards and mice. But if you just want a compact and insanely powerful gaming desktop, and you don't mind spending a premium, it's hard to deny that the FragBox gets everything right. Update 2/23, 1:48PM: Added updated information about Wi-Fi 7, handle removability and pricing.This article originally appeared on Engadget at https://www.engadget.com/computing/falcon-northwest-fragbox-review-a-compact-gaming-rig-that-does-everything-right-130000837.html?src=rss",
          "content": "Mafia: The Old Country demands to be played on an enormous screen. As much as I love my 32-inch Alienware OLED gaming monitor, it doesn't do justice to Mafia's cinematic vistas of Sicily. But, I also wanted to play that game in its full 4K glory, with none of the compromises of today's game consoles. So why not just shove a tiny gaming desktop under my home theater? Enter the Fragbox, Falcon Northwest's revamped small form factor gaming PC. While it's very expensive, starting at $3,997, it's incredibly powerful and gives you the freedom to easily upgrade the hardware down the line. I know what you're thinking: \"A $4,000 desktop, in this economy?\" That pricing also doesn't include upgrading from the stock NVIDIA's RTX 5070 GPU, as well as adding more RAM and larger SSDs, all of which could drive the price up thousands more. I initially planned to review the FragBox back in early December 2025, before the AI-induced RAMaggedon made memory, storage and other components dramatically more expensive. Falcon Northwest is mainly known as a boutique and high-end system builder, so its wealthier clientele can likely weather the pricing storm. If you're looking for a deal, though, you won't find it here. So what, exactly, is a FragBox? Imagine a typical mid-tower desktop squashed down to a system that's only 10.2-inches tall, 10.5-inches wide and 15.9-inches deep. When Falcon initially debuted the FragBox in 2003, it was notable for being a genuinely small PC that used full-sized parts. That's still a main selling point today: It can still fit in large NVIDIA GPUs, including the beefy RTX 5090, as well as either Intel's latest Core Ultra chips or AMD's Ryzen 9000 CPUs. A huge 280mm radiator sits at the top pulling out hot air, and it also serves as an All-in-One (AIO) liquid cooler for the CPU. At 25 pounds, the FragBox isn't exactly light, but its sturdy metal handle makes it easy to move around. Most mid-tower desktops usually weigh between 20 and 35 pounds, depending on their case material. But they're also much larger and harder to squeeze into tight spaces. The FragBox's relatively squat size makes it easy to shove into a home entertainment center, or just sit on the corner of your desk. If you need a bit more height clearance, you can also remove the handle from the top panel. Just be sure there’s enough room for some airflow — all of that heat has to go somewhere, right? Falcon Northwest FragBox Devindra Hardawar for Engadget Despite its density, the FragBox's elegant design makes it a cinch to access to all of the system's components. Just unscrew the side and top panels and you can easily remove the GPU, RAM, storage and other major components. There are three slots of M.2 SSDs, as well as two locations for 2.5-inch drives and a spot for a large 3.5-inch HDD. The system is bundled with a 1,200W power supply, which should be more than enough to handle future GPUs and CPUs. Ports are plentiful as well: There are two USB-A and one USB-C connections right up front, alongside a headphone jack. On the rear, you've got your typical assortment of mid-tower connections, including four USB-A 2.0 connections, seven USB-A 3 ports, one 20G USB-C 3.2 port, 2.5G Ethernet, HDMI and DisplayPort. Our RTX 5090 review unit also included three DisplayPort jacks and one HDMI connection (which you'll see on most GPUs). Wi-Fi 6E was also built into our unit, but Falcon says that Wi-Fi 7 is now standard with new builds. Falcon Northwest FragBox Devindra Hardawar for Engadget The FragBox, thankfully, lacks the garish LEDs and cheesy thermal glass you find on more ostentatious gaming rigs. Falcon Northwest's aluminum case looks and feels stately, like an old-school luxury car. If you want something flashier, you can shell out an additional $400 for a custom UV printed case or $149 for a UV-printed front panel. Our review unit was equipped with AMD's Ryzen 9950X3D CPU, NVIDIA's RTX 5090, 96GB of DDR5 RAM and a 2TB SSD, which adds up to a whopping $7,995. Five months ago, it would have cost $7,047 —- you can thank the RAM shortage for the price jump. Even before benchmarking or running any games, I expected it to be a beast. In PCMark 10, the FragBox scored a whopping 13,810, which is around 500 points higher than my mid-tower system with the same CPU and GPU. It also scored the highest 3DMark Speedway and Port Royal ray tracing scores I've ever seen. Even more impressive, the FragBox's fans were barely audible under load, and the CPU and GPU sat at a chill 52C and 65C, respectively CPU GeekBench 6 CPU GeekBench 6 GPU Cinebench 2024 Falcon Northwest FragBox 3,445/22,787 390,148 N/A Desktop with AMD Ryzen 9 9950X3D, RTX 5090 3,366/18,950 381,400 134/2,124 Desktop with AMD Ryzen 9 7900X, RTX 5090 2,822/14,216 358,253 113/1,103 Apple Mac Studio M4 Max 4,090/26,394 116,028 190/2066 To get back to my initial point, it ran Mafia: The Old Country in 4K flawlessly, with every graphics setting cranked all the way up. While playing on my 120-inch projector home theater setup, the game reached 62 fps natively, and flipping on DLSS upscaling and frame generation bumped that up to 120 fps. Not that you need a super higher framerate for a slow-paced, mostly cinematic action game. I was just happy to be playing without any compromises — even the PS5 Pro can't reach the same level of graphical fidelity as the monstrously powerful RTX 5090. Falcon Northwest FragBox Devindra Hardawar for Engadget I'm no stranger to big-screen PC gaming, but previously I've had to run a laughably long HDMI cable from my desktop to make it work. I'm just too old for that mess now. And it also doesn't work consistently, especially at higher framerates, thanks to the massive bandwidth required to pump out 4K at high refresh rates. In-home game streaming is also an option, but that's not great when you're blowing games up to an enormous TV or projector screen. It's just too hard to ignore the imperfections of streaming compression. (Admittedly, I need to test newer high-bandwidth options, especially after I was impressed by NVIDIA's GeForce Now upgrade last year.) The FragBox also made it easy to jump into all of my recent Steam titles, including Mewgeneics and Arc Raiders on a big screen. Unfortunately, Windows itself remains a key stumbling block for home theater PC gaming. You'll still need to keep a keyboard and PC around to deal with the initial OS configuration. And even once I enabled Steam's Big Picture mode, which offers excellent controller options, I still occasionally had to deal with Windows Updates and other annoyances. Falcon Northwest FragBox Devindra Hardawar for Engadget Microsoft is currently trying to optimize Windows for gaming handhelds, and it's reportedly doing even more to make a future PC-powered Xbox feel more console-like. For now, though, using a Windows PC in your home theater doesn't feel much different than it did a decade ago. Steam is your savior, Windows is your enemy. Or you could just save thousands of dollars and buy a $500 PlayStation 5 or $700 PS5 Pro, instead. The latter will still get you smooth framerates and a healthy dose of ray tracing, without the annoyance of Windows, keyboards and mice. But if you just want a compact and insanely powerful gaming desktop, and you don't mind spending a premium, it's hard to deny that the FragBox gets everything right. Update 2/23, 1:48PM: Added updated information about Wi-Fi 7, handle removability and pricing.This article originally appeared on Engadget at https://www.engadget.com/computing/falcon-northwest-fragbox-review-a-compact-gaming-rig-that-does-everything-right-130000837.html?src=rss",
          "feed_position": 26,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/falcon_northwest_fragbox_2.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/tv-movies/a-new-evangelion-series-is-coming-from-studio-khara-and-yoko-taro-creator-of-nier-170916543.html",
          "published_at": "Mon, 23 Feb 2026 17:09:16 +0000",
          "title": "A new Evangelion series is coming from Studio Khara and Yoko Taro, creator of NieR",
          "standfirst": "Anime fans rejoice, as there's a new Neon Genesis Evangelion series on the horizon. This was announced during a 30th anniversary event held in Japan. The bad news? Franchise creator Hideaki Anno won't be writing the scripts. However, his replacement will be Yoko Taro, the guy who created the video game NieR. He also wears a giant and rather unsettling moon mask for some reason. The NieR franchise is known for rich and complex lore, with a story spanning thousands of years that occasionally dips into a parallel universe. Wikimedia Commons Evangelion veteran Kazuya Tsurumaki will be on hand to direct episodes, which is nice for long-time fans. He directed the Rebuild of Evangelion films and the recent Mobile Suit Gundam GQuuuuuuX anime. Composer Keiichi Okabe, from the NieR franchise, is scoring the new show. The new series will be produced by Studio Khara and Cloverworks. While we know a fair bit about who's behind the scenes of the upcoming show, we don't know anything about the plot. We don't know if it's yet another remake of the original story, a sequel or some kind of spin-off like the chibi-inspired Petit Eva: Evangelion@School. There's a trailer, but it's light on details. New \"Neon Genesis Evangelion\" ANIME SERIES NEW TRAILER Written by Yoko Taro Directors: Kazuya Tsurumaki & Toko Yatabe Music: Keiichi Okabe Animation Production: CloverWorks x Khara pic.twitter.com/jnJZ12XSRb— Captain Melvin Seahorse⚘️ (@sshiroux19) February 23, 2026 With Taro on board, it could really go in any direction. It's worth remembering, after all, that NieR is actually a spin-off of a PS2 game called Drakengard. In one of the multiple endings of that game, a final boss is transported from a fantasy realm to modern-day Tokyo. Slaying this beast releases a virus that plagues humankind, which is what eventually leads to the post-apocalyptic setting of NieR. This is sort of like if the events of a Dragon Quest game somehow led to the world of Resident Evil. If there's anyone who can breathe fresh life into the Evangelion franchise, it's Taro. Did I mention he wears a gigantic moon mask? Also, this isn't his first time penning TV scripts. He co-wrote the NieR: Automata anime spinoff.This article originally appeared on Engadget at https://www.engadget.com/entertainment/tv-movies/a-new-evangelion-series-is-coming-from-studio-khara-and-yoko-taro-creator-of-nier-170916543.html?src=rss",
          "content": "Anime fans rejoice, as there's a new Neon Genesis Evangelion series on the horizon. This was announced during a 30th anniversary event held in Japan. The bad news? Franchise creator Hideaki Anno won't be writing the scripts. However, his replacement will be Yoko Taro, the guy who created the video game NieR. He also wears a giant and rather unsettling moon mask for some reason. The NieR franchise is known for rich and complex lore, with a story spanning thousands of years that occasionally dips into a parallel universe. Wikimedia Commons Evangelion veteran Kazuya Tsurumaki will be on hand to direct episodes, which is nice for long-time fans. He directed the Rebuild of Evangelion films and the recent Mobile Suit Gundam GQuuuuuuX anime. Composer Keiichi Okabe, from the NieR franchise, is scoring the new show. The new series will be produced by Studio Khara and Cloverworks. While we know a fair bit about who's behind the scenes of the upcoming show, we don't know anything about the plot. We don't know if it's yet another remake of the original story, a sequel or some kind of spin-off like the chibi-inspired Petit Eva: Evangelion@School. There's a trailer, but it's light on details. New \"Neon Genesis Evangelion\" ANIME SERIES NEW TRAILER Written by Yoko Taro Directors: Kazuya Tsurumaki & Toko Yatabe Music: Keiichi Okabe Animation Production: CloverWorks x Khara pic.twitter.com/jnJZ12XSRb— Captain Melvin Seahorse⚘️ (@sshiroux19) February 23, 2026 With Taro on board, it could really go in any direction. It's worth remembering, after all, that NieR is actually a spin-off of a PS2 game called Drakengard. In one of the multiple endings of that game, a final boss is transported from a fantasy realm to modern-day Tokyo. Slaying this beast releases a virus that plagues humankind, which is what eventually leads to the post-apocalyptic setting of NieR. This is sort of like if the events of a Dragon Quest game somehow led to the world of Resident Evil. If there's anyone who can breathe fresh life into the Evangelion franchise, it's Taro. Did I mention he wears a gigantic moon mask? Also, this isn't his first time penning TV scripts. He co-wrote the NieR: Automata anime spinoff.This article originally appeared on Engadget at https://www.engadget.com/entertainment/tv-movies/a-new-evangelion-series-is-coming-from-studio-khara-and-yoko-taro-creator-of-nier-170916543.html?src=rss",
          "feed_position": 30,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2026-02/45209e00-10d5-11f1-93df-540dd239e30e"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/researchers-baked-3x-inference-speedups-directly-into-llm-weights-without",
          "published_at": "Mon, 23 Feb 2026 17:00:00 GMT",
          "title": "Researchers baked 3x inference speedups directly into LLM weights — without speculative decoding",
          "standfirst": "As agentic AI workflows multiply the cost and latency of long reasoning chains, a team from the University of Maryland, Lawrence Livermore National Labs, Columbia University and TogetherAI has found a way to bake 3x throughput gains directly into a model&#x27;s weights.Unlike speculative decoding, which requires a separate drafting model, this approach requires no additional infrastructure — just a single special token added to the model&#x27;s existing architecture.The limits of next-token predictionNext-token prediction — generating text one token per forward pass — creates a throughput ceiling that becomes painfully expensive when models need to produce thousands of tokens. This bottleneck is especially problematic in reasoning models, which frequently generate thousands of “chain of thought” tokens before producing the final response, leading to a slow and expensive user experience.Multi-token prediction (MTP) offers an alternative training paradigm that allows a language model to produce multiple tokens simultaneously in a single forward pass. For example, the model can be trained to predict a block of tokens all at once instead of just the immediate next token.John Kirchenbauer, a doctorate candidate in computer science at the University of Maryland and co-author of the paper, told VentureBeat that as we move toward agentic workflows, the focus is shifting from overall throughput to single-user speed. \"Today, with ultra-long thinking traces being the norm and agentic outer loops multiplying out those costs even further, latency is becoming as equally important a dimension of overall serving efficiency as gross tokens per second per hardware unit (tps/GPU),\" Kirchenbauer said. He said that while standard batched next-token prediction is already optimal for overall throughput, the new approach \"strive[s] to saturate the GPU with just a single user&#x27;s query to decrease latency for that single user.\"Other methods exist, but they come with drawbacks. \"It&#x27;s worth noting that speculative decoding, and diffusion LLMs as an efficiency focused alternative to next token prediction (NTP), are both latency focused acceleration techniques,\" Kirchenbauer said. But speculative decoding requires deploying and managing an auxiliary \"drafting\" model, which spends more absolute compute to draft and verify. MTP, on the other hand, \"leverages a similar sort of tradeoff, it&#x27;s just simpler to serve and scientifically interesting in its own right.\"Current MTP paradigms have limitations, however. The standard objective for training a language model for MTP involves comparing its predictions against ground-truth text from a dataset. The pitfall is that this standard training teaches the model to predict the probability of a token at a specific position independently, rather than caring about the joint relationship between a sequence of tokens.If a model tries to predict multiple tokens at once using this standard method, two major problems occur. The first is grammatical mismatch. For example, if a model predicts two words following the prefix \"The zookeeper fed the,\" it might sample independently and produce a mismatched phrase like \"panda meat\" or \"lion bamboo\" instead of \"panda bamboo\" and “lion meat.”The second issue is degenerate repetition. Because typical text is unpredictable, a model trying to predict a token 100 positions into the future against a standard dataset will just predict \"the,\" since it is the most common word in English. This results in the model outputting nonsense like \"...the the the...\" for far-future positions.Multi-token prediction via self-distillationTo solve the issues of generating multiple tokens, the researchers propose a novel training technique that uses a student-teacher scheme. A student model, which is the model learning to predict multiple tokens, generates a deterministic multi-token block. A teacher model, acting as a strong standard next-token prediction language model, evaluates that block. The teacher acts as a critic, calculating how likely and coherent the student&#x27;s proposed sequence is. If the student proposes a mismatched phrase like \"lion bamboo,\" the teacher assigns it a high loss, teaching the student to avoid that construction.The paradigm is inspired by on-policy reinforcement learning because the student model is not simply memorizing static text. It generates a full rollout (sequence of actions in RL parlance) instantly in parallel on a single forward pass and receives a reward based on how good the teacher thinks it is. Unlike static supervised methods where training pairs are fixed in advance, the feedback here is dynamic, generated from the student&#x27;s own outputs in real time. The strong teacher also verifies the coherence of the tokens, which prevents the student model from learning degenerate outputs like repeated words.For developers, the beauty of this approach lies in its simplicity. \"There are truly no modifications to the architecture except for the addition of a special token,\" Kirchenbauer said. By co-opting an unused slot in a model&#x27;s existing embedding matrix to act as an <MTP> mask token, the technique converts sequential operations into parallel ones. \"Any standard next token prediction language model can be adapted in this way... the internal implementation — MoE, windowed attention, SSM layers, etc. — are left untouched and present no barrier to adaptation.\"For engineering teams, this means the adaptation can be applied to models already in production without rebuilding pipelines. Generating multiple tokens at the same time can still hurt the accuracy of the response at inference time. To maximize generation speed without sacrificing the quality of the output, the authors introduce an adaptive decoding strategy called ConfAdapt.ConfAdapt evaluates a confidence threshold, such as 90%, at each step. The model generates a block of tokens, but it only keeps the tokens that meet or exceed this high-confidence threshold. When the upcoming text is highly predictable or structural, the model&#x27;s confidence is very high. It will accept and output a large chunk of tokens all at once, saving significant computational time on easy tokens. It then focuses its costly single-token passes on harder tokens that require more computational effort.Putting multi-token prediction to the testTo see how the training paradigm performed in practice, the researchers applied their method to popular open-weight instruction-tuned models. They tested the strong general-purpose model Llama-3.1-8B-Magpie and the smaller, efficient Qwen3-4B-Instruct-2507, which is often chosen for cost-sensitive enterprise deployments. Both models were tuned on MetaMathQA, a dataset of synthetic grade school math problems that rely heavily on reasoning traces.The experiments revealed a clear sweet spot between speed and accuracy. Using the ConfAdapt strategy, the Llama-3.1-8B model achieved a 3x speedup with less than a 3% drop in accuracy on math benchmarks. The Qwen3-4B model achieved the same 3x speedup with a slightly higher 7% drop in accuracy. More aggressive settings could hit 5x speedups, though they came with steeper accuracy penalties.How this translates to real-world tasks depends on predictability. \"As the ConfAdapt approach naturally tailors the acceleration to the inherent entropy in the domain, when the model &#x27;knows&#x27; exactly what comes next it can emit it in a single pass,\" he noted, leading to massive acceleration on predictable tasks, while using more steps for uncertain outputs.The speedups also transferred across domains that were not included in the multi-token prediction training phase. This included tasks within the same domain as the training data, like math and reasoning, as well as open-ended tasks such as creative writing and summarization.Despite this transfer learning, enterprises deploying these models for specialized tasks shouldn&#x27;t rely on it entirely. \"Our recommendation would be to tune/adapt the model for MTP using samples from the special industrial domain,\" Kirchenbauer said. \"The best performance is likely achieved if the MTP adaptation is performed using prompts from the deployment domain.\"Serving compatibility and the road aheadThe research team released their trained models on Hugging Face and will soon release the code for their MTP framework. Infrastructure teams integrating these models into vLLM or SGLang will need to account for changes in how batching and KV caching are handled — but that&#x27;s a one-time engineering investment, not an ongoing burden. However, Kirchenbauer sees \"no clear barriers to integration\" and confirmed the team is \"working with some systems experts to identify the shortest path to integration.\"Kirchenbauer&#x27;s advice for teams wanting to test the released models: start with toy prompts like counting or repeating a phrase to see ConfAdapt&#x27;s gains in action, then adapt the model using samples from your specific deployment domain for best results. \"Overall we do expect that a production-ready implementation of our approach could simplify the lifecycle of building and deploying low-latency agentic models,\" Kirchenbauer concluded. \"While existing acceleration techniques for NTP models focus almost solely on inference harnesses and logic, our approach just bakes some of the complexity into the model itself making it largely complementary to existing work.\"",
          "content": "As agentic AI workflows multiply the cost and latency of long reasoning chains, a team from the University of Maryland, Lawrence Livermore National Labs, Columbia University and TogetherAI has found a way to bake 3x throughput gains directly into a model&#x27;s weights.Unlike speculative decoding, which requires a separate drafting model, this approach requires no additional infrastructure — just a single special token added to the model&#x27;s existing architecture.The limits of next-token predictionNext-token prediction — generating text one token per forward pass — creates a throughput ceiling that becomes painfully expensive when models need to produce thousands of tokens. This bottleneck is especially problematic in reasoning models, which frequently generate thousands of “chain of thought” tokens before producing the final response, leading to a slow and expensive user experience.Multi-token prediction (MTP) offers an alternative training paradigm that allows a language model to produce multiple tokens simultaneously in a single forward pass. For example, the model can be trained to predict a block of tokens all at once instead of just the immediate next token.John Kirchenbauer, a doctorate candidate in computer science at the University of Maryland and co-author of the paper, told VentureBeat that as we move toward agentic workflows, the focus is shifting from overall throughput to single-user speed. \"Today, with ultra-long thinking traces being the norm and agentic outer loops multiplying out those costs even further, latency is becoming as equally important a dimension of overall serving efficiency as gross tokens per second per hardware unit (tps/GPU),\" Kirchenbauer said. He said that while standard batched next-token prediction is already optimal for overall throughput, the new approach \"strive[s] to saturate the GPU with just a single user&#x27;s query to decrease latency for that single user.\"Other methods exist, but they come with drawbacks. \"It&#x27;s worth noting that speculative decoding, and diffusion LLMs as an efficiency focused alternative to next token prediction (NTP), are both latency focused acceleration techniques,\" Kirchenbauer said. But speculative decoding requires deploying and managing an auxiliary \"drafting\" model, which spends more absolute compute to draft and verify. MTP, on the other hand, \"leverages a similar sort of tradeoff, it&#x27;s just simpler to serve and scientifically interesting in its own right.\"Current MTP paradigms have limitations, however. The standard objective for training a language model for MTP involves comparing its predictions against ground-truth text from a dataset. The pitfall is that this standard training teaches the model to predict the probability of a token at a specific position independently, rather than caring about the joint relationship between a sequence of tokens.If a model tries to predict multiple tokens at once using this standard method, two major problems occur. The first is grammatical mismatch. For example, if a model predicts two words following the prefix \"The zookeeper fed the,\" it might sample independently and produce a mismatched phrase like \"panda meat\" or \"lion bamboo\" instead of \"panda bamboo\" and “lion meat.”The second issue is degenerate repetition. Because typical text is unpredictable, a model trying to predict a token 100 positions into the future against a standard dataset will just predict \"the,\" since it is the most common word in English. This results in the model outputting nonsense like \"...the the the...\" for far-future positions.Multi-token prediction via self-distillationTo solve the issues of generating multiple tokens, the researchers propose a novel training technique that uses a student-teacher scheme. A student model, which is the model learning to predict multiple tokens, generates a deterministic multi-token block. A teacher model, acting as a strong standard next-token prediction language model, evaluates that block. The teacher acts as a critic, calculating how likely and coherent the student&#x27;s proposed sequence is. If the student proposes a mismatched phrase like \"lion bamboo,\" the teacher assigns it a high loss, teaching the student to avoid that construction.The paradigm is inspired by on-policy reinforcement learning because the student model is not simply memorizing static text. It generates a full rollout (sequence of actions in RL parlance) instantly in parallel on a single forward pass and receives a reward based on how good the teacher thinks it is. Unlike static supervised methods where training pairs are fixed in advance, the feedback here is dynamic, generated from the student&#x27;s own outputs in real time. The strong teacher also verifies the coherence of the tokens, which prevents the student model from learning degenerate outputs like repeated words.For developers, the beauty of this approach lies in its simplicity. \"There are truly no modifications to the architecture except for the addition of a special token,\" Kirchenbauer said. By co-opting an unused slot in a model&#x27;s existing embedding matrix to act as an <MTP> mask token, the technique converts sequential operations into parallel ones. \"Any standard next token prediction language model can be adapted in this way... the internal implementation — MoE, windowed attention, SSM layers, etc. — are left untouched and present no barrier to adaptation.\"For engineering teams, this means the adaptation can be applied to models already in production without rebuilding pipelines. Generating multiple tokens at the same time can still hurt the accuracy of the response at inference time. To maximize generation speed without sacrificing the quality of the output, the authors introduce an adaptive decoding strategy called ConfAdapt.ConfAdapt evaluates a confidence threshold, such as 90%, at each step. The model generates a block of tokens, but it only keeps the tokens that meet or exceed this high-confidence threshold. When the upcoming text is highly predictable or structural, the model&#x27;s confidence is very high. It will accept and output a large chunk of tokens all at once, saving significant computational time on easy tokens. It then focuses its costly single-token passes on harder tokens that require more computational effort.Putting multi-token prediction to the testTo see how the training paradigm performed in practice, the researchers applied their method to popular open-weight instruction-tuned models. They tested the strong general-purpose model Llama-3.1-8B-Magpie and the smaller, efficient Qwen3-4B-Instruct-2507, which is often chosen for cost-sensitive enterprise deployments. Both models were tuned on MetaMathQA, a dataset of synthetic grade school math problems that rely heavily on reasoning traces.The experiments revealed a clear sweet spot between speed and accuracy. Using the ConfAdapt strategy, the Llama-3.1-8B model achieved a 3x speedup with less than a 3% drop in accuracy on math benchmarks. The Qwen3-4B model achieved the same 3x speedup with a slightly higher 7% drop in accuracy. More aggressive settings could hit 5x speedups, though they came with steeper accuracy penalties.How this translates to real-world tasks depends on predictability. \"As the ConfAdapt approach naturally tailors the acceleration to the inherent entropy in the domain, when the model &#x27;knows&#x27; exactly what comes next it can emit it in a single pass,\" he noted, leading to massive acceleration on predictable tasks, while using more steps for uncertain outputs.The speedups also transferred across domains that were not included in the multi-token prediction training phase. This included tasks within the same domain as the training data, like math and reasoning, as well as open-ended tasks such as creative writing and summarization.Despite this transfer learning, enterprises deploying these models for specialized tasks shouldn&#x27;t rely on it entirely. \"Our recommendation would be to tune/adapt the model for MTP using samples from the special industrial domain,\" Kirchenbauer said. \"The best performance is likely achieved if the MTP adaptation is performed using prompts from the deployment domain.\"Serving compatibility and the road aheadThe research team released their trained models on Hugging Face and will soon release the code for their MTP framework. Infrastructure teams integrating these models into vLLM or SGLang will need to account for changes in how batching and KV caching are handled — but that&#x27;s a one-time engineering investment, not an ongoing burden. However, Kirchenbauer sees \"no clear barriers to integration\" and confirmed the team is \"working with some systems experts to identify the shortest path to integration.\"Kirchenbauer&#x27;s advice for teams wanting to test the released models: start with toy prompts like counting or repeating a phrase to see ConfAdapt&#x27;s gains in action, then adapt the model using samples from your specific deployment domain for best results. \"Overall we do expect that a production-ready implementation of our approach could simplify the lifecycle of building and deploying low-latency agentic models,\" Kirchenbauer concluded. \"While existing acceleration techniques for NTP models focus almost solely on inference harnesses and logic, our approach just bakes some of the complexity into the model itself making it largely complementary to existing work.\"",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7lB8UaMsO4LhTkCf5Tf7Yf/40524ebbaf93e89ba37322f9753fdd03/multi-token_prediction.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/anthropic-claude-code-security-reasoning-vulnerability-hunting",
          "published_at": "Mon, 23 Feb 2026 15:44:00 GMT",
          "title": "Anthropic's Claude Code Security is available now after finding 500+ vulnerabilities: how security leaders should respond",
          "standfirst": "Anthropic pointed its most advanced AI model, Claude Opus 4.6, at production open-source codebases and found a plethora of security holes: more than 500 high-severity vulnerabilities that had survived decades of expert review and millions of hours of fuzzing, with each candidate vetted through internal and external security review before disclosure. Fifteen days later, the company productized the capability and launched Claude Code Security.Security directors responsible for seven-figure vulnerability management stacks should expect a common question from their boards in the next review cycle. VentureBeat anticipates the emails and conversations will start with, \"How do we add reasoning-based scanning before attackers get there first?\", because as Anthropic&#x27;s review found, simply pointing an AI model at exposed code can be enough to identify — and in the case of malicious actors, exploit — security lapses in production code. The answer matters more than the number, and it is primarily structural: how your tooling and processes allocate work between pattern-based scanners and reasoning-based analysis. CodeQL and the tools built on it match code against known patterns. Claude Code Security, which Anthropic launched February 20 as a limited research preview, reasons about code the way a human security researcher would. It follows how data moves through an application and catches flaws in business logic and access control that no rule set covers.The board conversation security leaders need to have this weekFive hundred newly discovered zero-days is less a scare statistic than a standing budget justification for rethinking how you fund code security. The reasoning capability Claude Code Security represents, and its inevitable competitors, need to drive the procurement conversation. Static application security testing (SAST) catches known vulnerability classes. Reasoning-based scanners find what pattern-matching was never designed to detect. Both have a role.Anthropic published the zero-day research on February 5. Fifteen days later, they shipped the product. While it&#x27;s the same model and capabilities, it is now available to Enterprise and Team customers.What Claude does that CodeQL couldn&#x27;tGitHub has offered CodeQL-based scanning through Advanced Security for years, and added Copilot Autofix in August 2024 to generate LLM-suggested fixes for alerts. Security teams rely on it. But the detection boundary is the CodeQL rule set, and everything outside that boundary stays invisible.Claude Code Security extends that boundary by generating and testing its own hypotheses about how data and control flow through an application, including cases where no existing rule set describes. CodeQL solves the problem it was built to solve: data-flow analysis within predefined queries. It tells you whether tainted input reaches a dangerous function.CodeQL is not designed to autonomously read a project&#x27;s commit history, infer an incomplete patch, trace that logic into another file, and then assemble a working proof-of-concept exploit end to end. Claude did exactly that on GhostScript, OpenSC, and CGIF, each time using a different reasoning strategy.\"The real shift is from pattern-matching to hypothesis generation,\" said Merritt Baer, CSO at Enkrypt AI, advisor to Andesite and AppOmni, and former Deputy CISO at AWS, in an exclusive interview with VentureBeat. \"That&#x27;s a step-function increase in discovery power, and it demands equally strong human and technical controls.\"Three proof points from Anthropic&#x27;s published methodology show where pattern-matching ends and hypothesis generation begins.Commit history analysis across files. GhostScript is a widely deployed utility for processing PostScript and PDF files. Fuzzing turned up nothing, and neither did manual analysis. Then Claude pulled the Git commit history, found a patch that added stack bounds checking for font handling in gstype1.c, and reversed the logic: if the fix was needed there, every other call to that function without the fix was still vulnerable. In gdevpsfx.c, a completely different file, the call to the same function lacked the bounds checking patched elsewhere. Claude built a working proof-of-concept crash. No CodeQL rule describes that bug today. The maintainers have since patched it.Reasoning about preconditions that fuzzers can&#x27;t reach. OpenSC processes smart card data. Standard approaches failed here, too, so Claude searched the repository for function calls that are frequently vulnerable and found a location where multiple strcat operations ran in succession without length checking on the output buffer. Fuzzers rarely reached that code path because too many preconditions stood in the way. Claude reasoned about which code fragments looked interesting, constructed a buffer overflow, and proved the vulnerability.Algorithm-level edge cases that no coverage metric catches. CGIF is a library for processing GIF files. This vulnerability required understanding how LZW compression builds a dictionary of tokens. CGIF assumed compressed output would always be smaller than uncompressed input, which is almost always true. Claude recognized that if the LZW dictionary filled up and triggered resets, the compressed output could exceed the uncompressed size, overflowing the buffer. Even 100% branch coverage wouldn&#x27;t catch this. The flaw demands a particular sequence of operations that exercises an edge case in the compression algorithm itself. Random input generation almost never produces it. Claude did.Baer sees something broader in that progression. \"The challenge with reasoning isn&#x27;t accuracy, it&#x27;s agency,\" she told VentureBeat. \"Once a system can form hypotheses and pursue them, you&#x27;ve shifted from a lookup tool to something that can explore your environment in ways that are harder to predict and constrain.\"How Anthropic validated 500+ findingsAnthropic placed Claude inside a sandboxed virtual machine with standard utilities and vulnerability analysis tools. The red team didn&#x27;t provide any specialized instructions, custom harnesses, or task-specific prompting. Just the model and the code.The red team focused on memory corruption vulnerabilities because they&#x27;re the easiest to confirm objectively. Crash monitoring and address sanitizers don&#x27;t leave room for debate. Claude filtered its own output, deduplicating and reprioritizing before human researchers touched anything. When the confirmed count kept climbing, Anthropic brought in external security professionals to validate findings and write patches.Every target was an open-source project underpinning enterprise systems and critical infrastructure. Small teams maintain many of them, staffed by volunteers, not security professionals. When a vulnerability sits in one of these projects for a decade, every product that pulls from it inherits the risk.Anthropic didn&#x27;t start with the product launch. The defensive research spans more than a year. The company entered Claude in competitive Capture-the-Flag events where it ranked in the top 3% of PicoCTF globally, solved 19 of 20 challenges in the HackTheBox AI vs Human CTF, and placed 6th out of 9 teams defending live networks against human red team attacks at Western Regional CCDC. Anthropic also partnered with Pacific Northwest National Laboratory to test Claude against a simulated water treatment plant. PNNL&#x27;s researchers estimated that the model completed adversary emulation in three hours. The traditional process takes multiple weeks.The dual-use question security leaders can&#x27;t avoidThe same reasoning that finds a vulnerability can help an attacker exploit one. Frontier Red Team leader Logan Graham acknowledged this directly to Fortune&#x27;s Sharon Goldman. He told Fortune the models can now explore codebases autonomously and follow investigative leads faster than a junior security researcher.Gabby Curtis, Anthropic&#x27;s communications lead, told VentureBeat in an exclusive interview the company built Claude Code Security to make defensive capabilities more widely available, \"tipping the scales towards defenders.\" She was equally direct about the tension: \"The same reasoning that helps Claude find and fix a vulnerability could help an attacker exploit it, so we&#x27;re being deliberate about how we release this.\"In interviews with more than 40 CISOs across industries, VentureBeat found that formal governance frameworks for reasoning-based scanning tools are the exception, not the norm. The most common responses are that the area was considered so nascent that many CISOs didn&#x27;t think this capability would arrive so early in 2026.The question every security director has to answer before deploying this: if I give my team a tool that finds zero-days through reasoning, have I unintentionally expanded my internal threat surface?\"You didn&#x27;t weaponize your internal surface, you revealed it,\" Baer told VentureBeat. \"These tools can be helpful, but they also may surface latent risk faster and more scalably. The same tool that finds zero-days for defense can expose gaps in your threat model. Keep in mind that most intrusions don&#x27;t come from zero-days, they come from misconfigurations.\"\"In addition to the access and attack path risk, there is IP risk,\" she said. \"Not just exfiltration, but transformation. Reasoning models can internalize and re-express proprietary insights in ways that blur the line between use and leakage.\"The release is deliberately constrained. Enterprise and Team customers only, through a limited research preview. Open-source maintainers apply for free expedited access. Findings go through multi-stage self-verification before reaching an analyst, with severity ratings and confidence scores attached. Every patch requires human approval.Anthropic also built detection into the model itself. In a blog post detailing the safeguards, the company described deploying probes that measure activations within the model as it generates responses, with new cyber-specific probes designed to track potential misuse. On the enforcement side, Anthropic is expanding its response capabilities to include real-time intervention, including blocking traffic it detects as malicious.Graham was direct with Axios: the models are extremely good at finding vulnerabilities, and he expects them to get much better still. VentureBeat asked Anthropic for the false-positive rate before and after self-verification, the number of disclosed vulnerabilities with patches landed versus still in triage, and the specific safeguards that distinguish attacker use from defender use. The lead researcher on the 500-vulnerability project was unavailable, and the company declined to share specific attacker-detection mechanisms to avoid tipping off threat actors.\"Offense and defense are converging in capability,\" Baer said. \"The differentiator is oversight. If you can&#x27;t audit and bound how the tool is used, you&#x27;ve created another risk.\"That speed advantage doesn&#x27;t favor defenders by default. It favors whoever adopts it first. Security directors who move early set the terms.Anthropic isn&#x27;t alone. The pattern is repeating.Security researcher Sean Heelan used OpenAI&#x27;s o3 model with no custom tooling and no agentic framework to discover CVE-2025-37899, a previously unknown use-after-free vulnerability in the Linux kernel&#x27;s SMB implementation. The model analyzed over 12,000 lines of code and identified a race condition that traditional static analysis tools consistently missed because detecting it requires understanding concurrent thread interactions across connections.Separately, AI security startup AISLE discovered all 12 zero-day vulnerabilities announced in OpenSSL&#x27;s January 2026 security patch, including a rare high-severity finding (CVE-2025-15467, a stack buffer overflow in CMS message parsing that is potentially remotely exploitable without valid key material). AISLE co-founder and chief scientist Stanislav Fort reported that his team&#x27;s AI system accounted for 13 of the 14 total OpenSSL CVEs assigned in 2025. OpenSSL is among the most scrutinized cryptographic libraries on the planet. Fuzzers have run against it for years. The AI found what they were not designed to find.The window is already openThose 500 vulnerabilities live in open-source projects that enterprise applications depend on. Anthropic is disclosing and patching, but the window between discovery and adoption of those patches is where attackers operate today.The same model improvements behind Claude Code Security are available to anyone with API access.If your team is evaluating these capabilities, the limited research preview is the right place to start, with clearly defined data handling rules, audit logging, and success criteria agreed up front.",
          "content": "Anthropic pointed its most advanced AI model, Claude Opus 4.6, at production open-source codebases and found a plethora of security holes: more than 500 high-severity vulnerabilities that had survived decades of expert review and millions of hours of fuzzing, with each candidate vetted through internal and external security review before disclosure. Fifteen days later, the company productized the capability and launched Claude Code Security.Security directors responsible for seven-figure vulnerability management stacks should expect a common question from their boards in the next review cycle. VentureBeat anticipates the emails and conversations will start with, \"How do we add reasoning-based scanning before attackers get there first?\", because as Anthropic&#x27;s review found, simply pointing an AI model at exposed code can be enough to identify — and in the case of malicious actors, exploit — security lapses in production code. The answer matters more than the number, and it is primarily structural: how your tooling and processes allocate work between pattern-based scanners and reasoning-based analysis. CodeQL and the tools built on it match code against known patterns. Claude Code Security, which Anthropic launched February 20 as a limited research preview, reasons about code the way a human security researcher would. It follows how data moves through an application and catches flaws in business logic and access control that no rule set covers.The board conversation security leaders need to have this weekFive hundred newly discovered zero-days is less a scare statistic than a standing budget justification for rethinking how you fund code security. The reasoning capability Claude Code Security represents, and its inevitable competitors, need to drive the procurement conversation. Static application security testing (SAST) catches known vulnerability classes. Reasoning-based scanners find what pattern-matching was never designed to detect. Both have a role.Anthropic published the zero-day research on February 5. Fifteen days later, they shipped the product. While it&#x27;s the same model and capabilities, it is now available to Enterprise and Team customers.What Claude does that CodeQL couldn&#x27;tGitHub has offered CodeQL-based scanning through Advanced Security for years, and added Copilot Autofix in August 2024 to generate LLM-suggested fixes for alerts. Security teams rely on it. But the detection boundary is the CodeQL rule set, and everything outside that boundary stays invisible.Claude Code Security extends that boundary by generating and testing its own hypotheses about how data and control flow through an application, including cases where no existing rule set describes. CodeQL solves the problem it was built to solve: data-flow analysis within predefined queries. It tells you whether tainted input reaches a dangerous function.CodeQL is not designed to autonomously read a project&#x27;s commit history, infer an incomplete patch, trace that logic into another file, and then assemble a working proof-of-concept exploit end to end. Claude did exactly that on GhostScript, OpenSC, and CGIF, each time using a different reasoning strategy.\"The real shift is from pattern-matching to hypothesis generation,\" said Merritt Baer, CSO at Enkrypt AI, advisor to Andesite and AppOmni, and former Deputy CISO at AWS, in an exclusive interview with VentureBeat. \"That&#x27;s a step-function increase in discovery power, and it demands equally strong human and technical controls.\"Three proof points from Anthropic&#x27;s published methodology show where pattern-matching ends and hypothesis generation begins.Commit history analysis across files. GhostScript is a widely deployed utility for processing PostScript and PDF files. Fuzzing turned up nothing, and neither did manual analysis. Then Claude pulled the Git commit history, found a patch that added stack bounds checking for font handling in gstype1.c, and reversed the logic: if the fix was needed there, every other call to that function without the fix was still vulnerable. In gdevpsfx.c, a completely different file, the call to the same function lacked the bounds checking patched elsewhere. Claude built a working proof-of-concept crash. No CodeQL rule describes that bug today. The maintainers have since patched it.Reasoning about preconditions that fuzzers can&#x27;t reach. OpenSC processes smart card data. Standard approaches failed here, too, so Claude searched the repository for function calls that are frequently vulnerable and found a location where multiple strcat operations ran in succession without length checking on the output buffer. Fuzzers rarely reached that code path because too many preconditions stood in the way. Claude reasoned about which code fragments looked interesting, constructed a buffer overflow, and proved the vulnerability.Algorithm-level edge cases that no coverage metric catches. CGIF is a library for processing GIF files. This vulnerability required understanding how LZW compression builds a dictionary of tokens. CGIF assumed compressed output would always be smaller than uncompressed input, which is almost always true. Claude recognized that if the LZW dictionary filled up and triggered resets, the compressed output could exceed the uncompressed size, overflowing the buffer. Even 100% branch coverage wouldn&#x27;t catch this. The flaw demands a particular sequence of operations that exercises an edge case in the compression algorithm itself. Random input generation almost never produces it. Claude did.Baer sees something broader in that progression. \"The challenge with reasoning isn&#x27;t accuracy, it&#x27;s agency,\" she told VentureBeat. \"Once a system can form hypotheses and pursue them, you&#x27;ve shifted from a lookup tool to something that can explore your environment in ways that are harder to predict and constrain.\"How Anthropic validated 500+ findingsAnthropic placed Claude inside a sandboxed virtual machine with standard utilities and vulnerability analysis tools. The red team didn&#x27;t provide any specialized instructions, custom harnesses, or task-specific prompting. Just the model and the code.The red team focused on memory corruption vulnerabilities because they&#x27;re the easiest to confirm objectively. Crash monitoring and address sanitizers don&#x27;t leave room for debate. Claude filtered its own output, deduplicating and reprioritizing before human researchers touched anything. When the confirmed count kept climbing, Anthropic brought in external security professionals to validate findings and write patches.Every target was an open-source project underpinning enterprise systems and critical infrastructure. Small teams maintain many of them, staffed by volunteers, not security professionals. When a vulnerability sits in one of these projects for a decade, every product that pulls from it inherits the risk.Anthropic didn&#x27;t start with the product launch. The defensive research spans more than a year. The company entered Claude in competitive Capture-the-Flag events where it ranked in the top 3% of PicoCTF globally, solved 19 of 20 challenges in the HackTheBox AI vs Human CTF, and placed 6th out of 9 teams defending live networks against human red team attacks at Western Regional CCDC. Anthropic also partnered with Pacific Northwest National Laboratory to test Claude against a simulated water treatment plant. PNNL&#x27;s researchers estimated that the model completed adversary emulation in three hours. The traditional process takes multiple weeks.The dual-use question security leaders can&#x27;t avoidThe same reasoning that finds a vulnerability can help an attacker exploit one. Frontier Red Team leader Logan Graham acknowledged this directly to Fortune&#x27;s Sharon Goldman. He told Fortune the models can now explore codebases autonomously and follow investigative leads faster than a junior security researcher.Gabby Curtis, Anthropic&#x27;s communications lead, told VentureBeat in an exclusive interview the company built Claude Code Security to make defensive capabilities more widely available, \"tipping the scales towards defenders.\" She was equally direct about the tension: \"The same reasoning that helps Claude find and fix a vulnerability could help an attacker exploit it, so we&#x27;re being deliberate about how we release this.\"In interviews with more than 40 CISOs across industries, VentureBeat found that formal governance frameworks for reasoning-based scanning tools are the exception, not the norm. The most common responses are that the area was considered so nascent that many CISOs didn&#x27;t think this capability would arrive so early in 2026.The question every security director has to answer before deploying this: if I give my team a tool that finds zero-days through reasoning, have I unintentionally expanded my internal threat surface?\"You didn&#x27;t weaponize your internal surface, you revealed it,\" Baer told VentureBeat. \"These tools can be helpful, but they also may surface latent risk faster and more scalably. The same tool that finds zero-days for defense can expose gaps in your threat model. Keep in mind that most intrusions don&#x27;t come from zero-days, they come from misconfigurations.\"\"In addition to the access and attack path risk, there is IP risk,\" she said. \"Not just exfiltration, but transformation. Reasoning models can internalize and re-express proprietary insights in ways that blur the line between use and leakage.\"The release is deliberately constrained. Enterprise and Team customers only, through a limited research preview. Open-source maintainers apply for free expedited access. Findings go through multi-stage self-verification before reaching an analyst, with severity ratings and confidence scores attached. Every patch requires human approval.Anthropic also built detection into the model itself. In a blog post detailing the safeguards, the company described deploying probes that measure activations within the model as it generates responses, with new cyber-specific probes designed to track potential misuse. On the enforcement side, Anthropic is expanding its response capabilities to include real-time intervention, including blocking traffic it detects as malicious.Graham was direct with Axios: the models are extremely good at finding vulnerabilities, and he expects them to get much better still. VentureBeat asked Anthropic for the false-positive rate before and after self-verification, the number of disclosed vulnerabilities with patches landed versus still in triage, and the specific safeguards that distinguish attacker use from defender use. The lead researcher on the 500-vulnerability project was unavailable, and the company declined to share specific attacker-detection mechanisms to avoid tipping off threat actors.\"Offense and defense are converging in capability,\" Baer said. \"The differentiator is oversight. If you can&#x27;t audit and bound how the tool is used, you&#x27;ve created another risk.\"That speed advantage doesn&#x27;t favor defenders by default. It favors whoever adopts it first. Security directors who move early set the terms.Anthropic isn&#x27;t alone. The pattern is repeating.Security researcher Sean Heelan used OpenAI&#x27;s o3 model with no custom tooling and no agentic framework to discover CVE-2025-37899, a previously unknown use-after-free vulnerability in the Linux kernel&#x27;s SMB implementation. The model analyzed over 12,000 lines of code and identified a race condition that traditional static analysis tools consistently missed because detecting it requires understanding concurrent thread interactions across connections.Separately, AI security startup AISLE discovered all 12 zero-day vulnerabilities announced in OpenSSL&#x27;s January 2026 security patch, including a rare high-severity finding (CVE-2025-15467, a stack buffer overflow in CMS message parsing that is potentially remotely exploitable without valid key material). AISLE co-founder and chief scientist Stanislav Fort reported that his team&#x27;s AI system accounted for 13 of the 14 total OpenSSL CVEs assigned in 2025. OpenSSL is among the most scrutinized cryptographic libraries on the planet. Fuzzers have run against it for years. The AI found what they were not designed to find.The window is already openThose 500 vulnerabilities live in open-source projects that enterprise applications depend on. Anthropic is disclosing and patching, but the window between discovery and adoption of those patches is where attackers operate today.The same model improvements behind Claude Code Security are available to anyone with API access.If your team is evaluating these capabilities, the limited research preview is the right place to start, with clearly defined data handling rules, audit logging, and success criteria agreed up front.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4yR2yRPMyVDENsc3LePH0g/470f944f258db5254dc0ee056e52b1c9/hero_anthropic_story.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/best-affordable-windows-laptops-123000512.html",
          "published_at": "Mon, 23 Feb 2026 10:01:25 +0000",
          "title": "The best cheap Windows laptops for 2026",
          "standfirst": "You don’t need to spend a fortune to get a capable Windows laptop. For everyday tasks like web browsing, writing documents, streaming video or handling schoolwork, a well-chosen budget machine can still deliver a smooth, reliable experience. The challenge is cutting through the noise to find affordable options that balance performance, build quality and battery life without serious compromises.For many buyers, timing is no longer optional. With Windows 10 support now officially over, upgrading has become a necessity rather than a nice-to-have. The picks below focus on cheap Windows laptops that can handle day-to-day workloads comfortably while keeping you current on software and security updates. If you’re open to spending more for extra power or premium features, our broader guide to the best Windows laptops covers higher-end alternatives as well. What to look for in a budget-friendly Windows laptop While you can do a lot even when spending little on a Windows laptop, you must set your expectations accordingly. The biggest downside when purchasing a budget laptop (of any kind, really) is limited power. You’ll want to carefully consider a few specs, the most important among them being the processor (CPU). Many Windows laptops under $500 run on Intel Celeron or Pentium chipsets, but you can find some with Core i3/i5 and AMD Ryzen 3/5 CPUs at the higher end of the price spectrum. We recommend getting the most powerful CPU you can afford because it will dictate how fast the computer will feel overall. Memory (RAM) is also important because, the more you have, the easier it will be for the laptop to manage things like a dozen browser tabs while you edit a Word document and stream music in the background. When it comes to storage, consider how much you want to save locally. If you primarily work in Google Docs or save most things in the cloud, you may not need a machine with a ton of onboard storage. Just remember that your digital space will also be taken up by apps, so it may be worth getting a little extra storage than you think you need if you know you’ll be downloading big programs. A final side note: solid state drives (SSDs) are ubiquitous at this point, not to mention faster and more efficient than hard drives (HDDs), so we recommend getting a laptop with that type of storage. As for screens, there’s a healthy mix of HD (720p resolution) and FHD (1080p) options in this price range and we recommend springing for a notebook with a 1080p display if you can. Touchscreens aren’t as common in the budget space as standard panels, but you’ll only really miss one if you get a 2-in-1 laptop. Before we get to our recommended specs for a cheap Windows laptop, it’s worth mentioning that Microsoft clearly lays out the true minimum requirements for any Windows 11 machine. Those include a 1GHz or faster processor that includes two or more cores, at least 4GB of RAM and 64GB of available storage space. That’s the bare minimum to run Windows 11; we recommend giving yourself some wiggle room by choosing a machine that will perform well now and for years to come. Specs to look for in an affordable Windows laptop CPU: Intel Core i3 or AMD Ryzen 3 processors, at minimum RAM: At least 8GB Storage: At least 128GB SSD Screen: At least 1080p FHD It’s essential to prioritize what’s important to you. But at the lower end of the budget, a good laptop may not offer everything you need, whereas a great one might. Although most machines come with features like Bluetooth, built-in Wi-Fi and additional ports, you might find not all of them come with the specifics you require, like an SD card slot, webcam, charger, and so on. Be sure to check the spec list of any laptop you’re considering before you buy, especially if you need specific connectors and capabilities. See Also: Best Laptops for 2026 Best Gaming Laptops Best 2-in-1 Laptops for 2026 Best Chromebooks Best Laptops for College Students As for Copilot+, don’t expect to see much of it on truly affordable Windows laptops just yet. Microsoft’s AI features and Copilot assistant require certain specs to run, namely a powerful neural processing unit (NPU), 16GB of RAM and 256GB of storage. Currently, the cheapest Copilot+ AI PCs will run you about $700, so if you’re willing to pay more for those perks, check out our best laptops guide for more options. If you’re looking for either a gaming laptop or a “Windows on Arm” laptop, both categories will require you to spend more money than we’re discussing here. Best cheap Windows laptops for 2026 The cheap Windows laptop market moves fast, and — unlike nearly all of our other buying guides — we haven't necessarily tested each specific configuration listed below. However, the combination of these technical specifications and familiar brands represent exactly the sort of entry-level laptops we'd recommend to shoppers in this price range based on our thorough research and expert knowledge. What to know about the budget Windows laptop market The best cheap laptop models change all the time. Unlike more expensive, flagship machines, these notebooks can be updated a couple times each year. That can make it hard to track down a specific model at Amazon, Best Buy, Walmart or any other retailer. Also, we’ve seen prices vary widely depending on the configuration and retailer you’re looking at. You can ensure you’re getting a quality laptop by doing a few things. First and foremost, make sure you get a machine that follows the recommended specs we list above. Also, make sure you’re buying from a reputable retailer, including big-box stores like Walmart, Best Buy and Costco, online shops like Amazon or direct manufacturers like Dell, HP, Lenovo and others. If you have a physical store near you (likely a Best Buy in the US), it’s never a bad idea to go play around with some laptops in person before choosing one. If you decide to shop online from the likes of Amazon or Walmart, double check the seller of the laptop you’re considering. For example, many items on Amazon are “shipped and sold” by Amazon and those are typically the best options. You’ll see that information on Amazon on the right sidebar on a product page, under the Add to Cart and Buy Now buttons. Third-party sellers are common in the affordable laptop space. Amazon sometimes classifies laptop manufacturers as third-party sellers, so you may see a laptop shipped and sold by HP or Dell — that’s a good thing, since it’s coming directly from the manufacturer. However, there are other third-party electronics sellers out there. We recommend clicking on the third-party seller’s name on Amazon or Walmart (yes, Walmart has them, too) to see how much positive feedback and how many five-star ratings they’ve received from buyers. What about Chromebooks and tablets? You may be inclined to recommend a Chromebook or a tablet to anyone considering a budget Windows laptop computer. Those instincts aren’t wrong, but Chromebooks and tablets aren’t the best buy for everyone. Tablets have the most portability, but they will only work for the most mobile-competent users like kids who have been grabbing smartphones out of their parents’ hands since they’ve been dexterous enough to do so. Tablets can also be just as expensive as some of the cheapest Windows laptops, and that’s without a mouse or keyboard. Chromebooks are a good alternative for those that basically live in a browser, the trade-off being you must give up the “traditional desktop.” And Chrome OS is a more limited operating system than Windows when it comes to the programs you can install and run. What Windows laptops do well What can you realistically accomplish on a cheap Windows laptop? Quite a bit, especially if you’re doing one thing (or a limited number of things) at a time. They’re great for everyday tasks like web browsing, checking email, video streaming and more. All of those things can be done on Chromebooks as well, but Windows laptops have a big advantage in Microsoft Office. While yes, there is a browser based version, the native, desktop apps are considered a must have for many and will run smoothly on even the most bare-bones budget laptop. The only caveat is that you may run into some slowdown on low-powered devices if you’re multitasking or working with large data sets in Excel or a lot of photos and graphics in Powerpoint. When it comes to specs, a bright spot for Windows laptops is storage. Even the most affordable devices tend to have at least a 128GB solid state drive. That will come in handy if you prefer to keep your most important files saved locally on your laptop's hard drive. In contrast, cheaper Chromebooks often have less storage because they’re built on the assumption that you’ll save all of your documents in the cloud. Not only is that less convenient when you need to work offline, but it also limits the size of programs and files that you can download. So, Chromebooks aren't the best for hoarding Netflix shows before a long trip or for use as a gaming laptop. Windows also has thousands of apps that you can download from its app store. Chromebooks have some Chrome apps, numerous browser extensions and the ability to download Android apps, but quality control is… inconsistent. Android apps, in particular, often haven’t been optimized for Chrome OS, which makes for a wonky user experience. Windows may not have as many apps as Android, but at least the experience is fairly standard across the board. Windows also gives you the ability to download and use programs from other sources, like direct from the developer. You can run things like Adobe Creative Suite, certain VPNs and programs like GIMP, Audacity and ClipMate on a Windows device, which just isn’t possible on Chrome OS. Chromebooks limit you to the apps and programs in The Play Store and the Chrome Extensions store, reducing any others to unusable, space-sucking icons in your Downloads folder.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-affordable-windows-laptops-123000512.html?src=rss",
          "content": "You don’t need to spend a fortune to get a capable Windows laptop. For everyday tasks like web browsing, writing documents, streaming video or handling schoolwork, a well-chosen budget machine can still deliver a smooth, reliable experience. The challenge is cutting through the noise to find affordable options that balance performance, build quality and battery life without serious compromises.For many buyers, timing is no longer optional. With Windows 10 support now officially over, upgrading has become a necessity rather than a nice-to-have. The picks below focus on cheap Windows laptops that can handle day-to-day workloads comfortably while keeping you current on software and security updates. If you’re open to spending more for extra power or premium features, our broader guide to the best Windows laptops covers higher-end alternatives as well. What to look for in a budget-friendly Windows laptop While you can do a lot even when spending little on a Windows laptop, you must set your expectations accordingly. The biggest downside when purchasing a budget laptop (of any kind, really) is limited power. You’ll want to carefully consider a few specs, the most important among them being the processor (CPU). Many Windows laptops under $500 run on Intel Celeron or Pentium chipsets, but you can find some with Core i3/i5 and AMD Ryzen 3/5 CPUs at the higher end of the price spectrum. We recommend getting the most powerful CPU you can afford because it will dictate how fast the computer will feel overall. Memory (RAM) is also important because, the more you have, the easier it will be for the laptop to manage things like a dozen browser tabs while you edit a Word document and stream music in the background. When it comes to storage, consider how much you want to save locally. If you primarily work in Google Docs or save most things in the cloud, you may not need a machine with a ton of onboard storage. Just remember that your digital space will also be taken up by apps, so it may be worth getting a little extra storage than you think you need if you know you’ll be downloading big programs. A final side note: solid state drives (SSDs) are ubiquitous at this point, not to mention faster and more efficient than hard drives (HDDs), so we recommend getting a laptop with that type of storage. As for screens, there’s a healthy mix of HD (720p resolution) and FHD (1080p) options in this price range and we recommend springing for a notebook with a 1080p display if you can. Touchscreens aren’t as common in the budget space as standard panels, but you’ll only really miss one if you get a 2-in-1 laptop. Before we get to our recommended specs for a cheap Windows laptop, it’s worth mentioning that Microsoft clearly lays out the true minimum requirements for any Windows 11 machine. Those include a 1GHz or faster processor that includes two or more cores, at least 4GB of RAM and 64GB of available storage space. That’s the bare minimum to run Windows 11; we recommend giving yourself some wiggle room by choosing a machine that will perform well now and for years to come. Specs to look for in an affordable Windows laptop CPU: Intel Core i3 or AMD Ryzen 3 processors, at minimum RAM: At least 8GB Storage: At least 128GB SSD Screen: At least 1080p FHD It’s essential to prioritize what’s important to you. But at the lower end of the budget, a good laptop may not offer everything you need, whereas a great one might. Although most machines come with features like Bluetooth, built-in Wi-Fi and additional ports, you might find not all of them come with the specifics you require, like an SD card slot, webcam, charger, and so on. Be sure to check the spec list of any laptop you’re considering before you buy, especially if you need specific connectors and capabilities. See Also: Best Laptops for 2026 Best Gaming Laptops Best 2-in-1 Laptops for 2026 Best Chromebooks Best Laptops for College Students As for Copilot+, don’t expect to see much of it on truly affordable Windows laptops just yet. Microsoft’s AI features and Copilot assistant require certain specs to run, namely a powerful neural processing unit (NPU), 16GB of RAM and 256GB of storage. Currently, the cheapest Copilot+ AI PCs will run you about $700, so if you’re willing to pay more for those perks, check out our best laptops guide for more options. If you’re looking for either a gaming laptop or a “Windows on Arm” laptop, both categories will require you to spend more money than we’re discussing here. Best cheap Windows laptops for 2026 The cheap Windows laptop market moves fast, and — unlike nearly all of our other buying guides — we haven't necessarily tested each specific configuration listed below. However, the combination of these technical specifications and familiar brands represent exactly the sort of entry-level laptops we'd recommend to shoppers in this price range based on our thorough research and expert knowledge. What to know about the budget Windows laptop market The best cheap laptop models change all the time. Unlike more expensive, flagship machines, these notebooks can be updated a couple times each year. That can make it hard to track down a specific model at Amazon, Best Buy, Walmart or any other retailer. Also, we’ve seen prices vary widely depending on the configuration and retailer you’re looking at. You can ensure you’re getting a quality laptop by doing a few things. First and foremost, make sure you get a machine that follows the recommended specs we list above. Also, make sure you’re buying from a reputable retailer, including big-box stores like Walmart, Best Buy and Costco, online shops like Amazon or direct manufacturers like Dell, HP, Lenovo and others. If you have a physical store near you (likely a Best Buy in the US), it’s never a bad idea to go play around with some laptops in person before choosing one. If you decide to shop online from the likes of Amazon or Walmart, double check the seller of the laptop you’re considering. For example, many items on Amazon are “shipped and sold” by Amazon and those are typically the best options. You’ll see that information on Amazon on the right sidebar on a product page, under the Add to Cart and Buy Now buttons. Third-party sellers are common in the affordable laptop space. Amazon sometimes classifies laptop manufacturers as third-party sellers, so you may see a laptop shipped and sold by HP or Dell — that’s a good thing, since it’s coming directly from the manufacturer. However, there are other third-party electronics sellers out there. We recommend clicking on the third-party seller’s name on Amazon or Walmart (yes, Walmart has them, too) to see how much positive feedback and how many five-star ratings they’ve received from buyers. What about Chromebooks and tablets? You may be inclined to recommend a Chromebook or a tablet to anyone considering a budget Windows laptop computer. Those instincts aren’t wrong, but Chromebooks and tablets aren’t the best buy for everyone. Tablets have the most portability, but they will only work for the most mobile-competent users like kids who have been grabbing smartphones out of their parents’ hands since they’ve been dexterous enough to do so. Tablets can also be just as expensive as some of the cheapest Windows laptops, and that’s without a mouse or keyboard. Chromebooks are a good alternative for those that basically live in a browser, the trade-off being you must give up the “traditional desktop.” And Chrome OS is a more limited operating system than Windows when it comes to the programs you can install and run. What Windows laptops do well What can you realistically accomplish on a cheap Windows laptop? Quite a bit, especially if you’re doing one thing (or a limited number of things) at a time. They’re great for everyday tasks like web browsing, checking email, video streaming and more. All of those things can be done on Chromebooks as well, but Windows laptops have a big advantage in Microsoft Office. While yes, there is a browser based version, the native, desktop apps are considered a must have for many and will run smoothly on even the most bare-bones budget laptop. The only caveat is that you may run into some slowdown on low-powered devices if you’re multitasking or working with large data sets in Excel or a lot of photos and graphics in Powerpoint. When it comes to specs, a bright spot for Windows laptops is storage. Even the most affordable devices tend to have at least a 128GB solid state drive. That will come in handy if you prefer to keep your most important files saved locally on your laptop's hard drive. In contrast, cheaper Chromebooks often have less storage because they’re built on the assumption that you’ll save all of your documents in the cloud. Not only is that less convenient when you need to work offline, but it also limits the size of programs and files that you can download. So, Chromebooks aren't the best for hoarding Netflix shows before a long trip or for use as a gaming laptop. Windows also has thousands of apps that you can download from its app store. Chromebooks have some Chrome apps, numerous browser extensions and the ability to download Android apps, but quality control is… inconsistent. Android apps, in particular, often haven’t been optimized for Chrome OS, which makes for a wonky user experience. Windows may not have as many apps as Android, but at least the experience is fairly standard across the board. Windows also gives you the ability to download and use programs from other sources, like direct from the developer. You can run things like Adobe Creative Suite, certain VPNs and programs like GIMP, Audacity and ClipMate on a Windows device, which just isn’t possible on Chrome OS. Chromebooks limit you to the apps and programs in The Play Store and the Chrome Extensions store, reducing any others to unusable, space-sucking icons in your Downloads folder.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-affordable-windows-laptops-123000512.html?src=rss",
          "feed_position": 34,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2021-05/843ff380-b255-11eb-bddf-b4000d3cf728"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html",
          "published_at": "Mon, 23 Feb 2026 08:00:38 +0000",
          "title": "The best earbuds for Android devices in 2026",
          "standfirst": "If you’re using an Android phone, finding the right pair of wireless earbuds can take a little more work than it does for iPhone owners. Apple’s AirPods are tightly woven into iOS, but that same level of seamless integration doesn’t automatically carry over to Android. The good news is there are plenty of earbuds that play just as nicely with Android devices, and in some cases offer features AirPods simply don’t.From earbuds designed to pair especially well with Samsung Galaxy and Google Pixel phones to models that prioritize strong noise cancellation, long battery life or workout-friendly durability, the Android ecosystem has no shortage of solid options. We’ve tested a wide range of wireless earbuds to find the best picks for Android users, whether you’re after premium sound, reliable everyday performance or a more affordable alternative. Best Android earbuds for 2026 What to look for in wireless earbuds for Android devices Photo by Jeff Dunn / Engadget For the most part, the features you want from a set of “Android earbuds” are the same as what you want from any headphones. Great sound quality, a comfortable fit and sufficient battery life are still the foundations. Adequate water resistance is good for workouts, and nobody wants a crummy mic for making calls. Once you approach the $100 range, features like active noise cancellation (ANC), wireless charging, an ambient sound mode (which lets you better hear outside noise without turning off your music) and multipoint connectivity (the ability to pair with multiple devices simultaneously) should be expected. For Android devices specifically, there are a few extras to consider. A dedicated app that makes it easy to switch sound modes, customize the audio profile, locate your earbuds if they ever get misplaced or adjust other settings is strongly preferred. Features like Google Fast Pair or NFC-based pairing, which can help you avoid having to dig through your Bluetooth menu to connect your earbuds for the first time, are also nice perks. Some Android devices can also utilize higher-quality Bluetooth codecs such as aptX Adaptive or Sony’s LDAC — these aren’t nearly as important to audio quality as the actual architecture of your earbuds, but they can help wring out a little more detail if the buds are capable enough and you’re streaming lossless files. AptX Adaptive can also help reduce latency, which is good for streaming video or gaming. Diversity is Android’s greatest strength, but it also means that some wireless earbuds play nicer with certain devices, typically those made by the same company. Recent Samsung earbuds, for instance, come with a few perks that are only available if you use a Galaxy phone. We have a couple of recommendations related to this idea above. How we test Android earbuds Photo by Billy Steele/Engadget The best way to test earphones is simply to wear them as much as possible, so that’s what we do. We typically do this over a one- to two-week period, though embargo times occasionally force us to finish our review process a bit faster. We listen to a test playlist that includes several musical genres and podcasts, paying close attention to how each pair approaches the bass, mid and treble frequencies to get an accurate sense of its sound profile. We also test at high and low volumes to check for consistency in the tuning. We do not have access to a dummy head to take more objective measurements, but we’ll sometimes look to sites like Rtings, SoundGuys and others that do just to ensure our impressions are not wildly off-base. If a model supports custom EQ, we’ll tinker with that and use the available EQ presets to see if one sounds dramatically better than the others — though in general we base most of our impressions on the stock tuning each pair uses by default. To assess microphone quality, we record our own audio samples and take multiple calls with a partner both indoors and outside. For battery life, we play our test playlist on a loop with the volume around 75 percent and measure how long it takes for each set to drain. Where applicable, we do a thorough review of a pair’s companion app and test each available feature. While comfort is ultimately subjective, we take note of how secure each pair feels while we’re on the move. We also use certain pairs in especially crowded public spaces to get a better sense of their passive and active noise cancellation, as well as their ability to maintain a consistent Bluetooth connection. Recent updates February 2026: Updated to include new top picks. November 2025: The lightly updated Beats Powerbeats Fit replace the older Beats Fit Pro as our top pick for working out. We’ve also noted the new Google Pixel Buds 2a as a cheaper alternative to the Pixel Buds Pro 2, which remain our recommendation for Pixel phone users. August 2025: We’ve taken another sweep to ensure our advice is still up-to-date. May 2025: We’ve checked this guide to ensure our top picks still stand and noted a couple alternatives to the Noble Fokus Rex5, since that pair has had stock issues of late. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of our recommendations (and the consumer tech industry as a whole). All of our picks are still available in their normal price ranges today, but we’ll update this guide if that changes. February 2025: The Noble FoKus Rex5 is our new \"best for sound quality\" pick, replacing the Sennheiser Momentum True Wireless 4. Our other recommendations remain unchanged. December 2024: We’ve lightly edited this guide for clarity and ensured that our current picks are still accurate.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html?src=rss",
          "content": "If you’re using an Android phone, finding the right pair of wireless earbuds can take a little more work than it does for iPhone owners. Apple’s AirPods are tightly woven into iOS, but that same level of seamless integration doesn’t automatically carry over to Android. The good news is there are plenty of earbuds that play just as nicely with Android devices, and in some cases offer features AirPods simply don’t.From earbuds designed to pair especially well with Samsung Galaxy and Google Pixel phones to models that prioritize strong noise cancellation, long battery life or workout-friendly durability, the Android ecosystem has no shortage of solid options. We’ve tested a wide range of wireless earbuds to find the best picks for Android users, whether you’re after premium sound, reliable everyday performance or a more affordable alternative. Best Android earbuds for 2026 What to look for in wireless earbuds for Android devices Photo by Jeff Dunn / Engadget For the most part, the features you want from a set of “Android earbuds” are the same as what you want from any headphones. Great sound quality, a comfortable fit and sufficient battery life are still the foundations. Adequate water resistance is good for workouts, and nobody wants a crummy mic for making calls. Once you approach the $100 range, features like active noise cancellation (ANC), wireless charging, an ambient sound mode (which lets you better hear outside noise without turning off your music) and multipoint connectivity (the ability to pair with multiple devices simultaneously) should be expected. For Android devices specifically, there are a few extras to consider. A dedicated app that makes it easy to switch sound modes, customize the audio profile, locate your earbuds if they ever get misplaced or adjust other settings is strongly preferred. Features like Google Fast Pair or NFC-based pairing, which can help you avoid having to dig through your Bluetooth menu to connect your earbuds for the first time, are also nice perks. Some Android devices can also utilize higher-quality Bluetooth codecs such as aptX Adaptive or Sony’s LDAC — these aren’t nearly as important to audio quality as the actual architecture of your earbuds, but they can help wring out a little more detail if the buds are capable enough and you’re streaming lossless files. AptX Adaptive can also help reduce latency, which is good for streaming video or gaming. Diversity is Android’s greatest strength, but it also means that some wireless earbuds play nicer with certain devices, typically those made by the same company. Recent Samsung earbuds, for instance, come with a few perks that are only available if you use a Galaxy phone. We have a couple of recommendations related to this idea above. How we test Android earbuds Photo by Billy Steele/Engadget The best way to test earphones is simply to wear them as much as possible, so that’s what we do. We typically do this over a one- to two-week period, though embargo times occasionally force us to finish our review process a bit faster. We listen to a test playlist that includes several musical genres and podcasts, paying close attention to how each pair approaches the bass, mid and treble frequencies to get an accurate sense of its sound profile. We also test at high and low volumes to check for consistency in the tuning. We do not have access to a dummy head to take more objective measurements, but we’ll sometimes look to sites like Rtings, SoundGuys and others that do just to ensure our impressions are not wildly off-base. If a model supports custom EQ, we’ll tinker with that and use the available EQ presets to see if one sounds dramatically better than the others — though in general we base most of our impressions on the stock tuning each pair uses by default. To assess microphone quality, we record our own audio samples and take multiple calls with a partner both indoors and outside. For battery life, we play our test playlist on a loop with the volume around 75 percent and measure how long it takes for each set to drain. Where applicable, we do a thorough review of a pair’s companion app and test each available feature. While comfort is ultimately subjective, we take note of how secure each pair feels while we’re on the move. We also use certain pairs in especially crowded public spaces to get a better sense of their passive and active noise cancellation, as well as their ability to maintain a consistent Bluetooth connection. Recent updates February 2026: Updated to include new top picks. November 2025: The lightly updated Beats Powerbeats Fit replace the older Beats Fit Pro as our top pick for working out. We’ve also noted the new Google Pixel Buds 2a as a cheaper alternative to the Pixel Buds Pro 2, which remain our recommendation for Pixel phone users. August 2025: We’ve taken another sweep to ensure our advice is still up-to-date. May 2025: We’ve checked this guide to ensure our top picks still stand and noted a couple alternatives to the Noble Fokus Rex5, since that pair has had stock issues of late. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of our recommendations (and the consumer tech industry as a whole). All of our picks are still available in their normal price ranges today, but we’ll update this guide if that changes. February 2025: The Noble FoKus Rex5 is our new \"best for sound quality\" pick, replacing the Sennheiser Momentum True Wireless 4. Our other recommendations remain unchanged. December 2024: We’ve lightly edited this guide for clarity and ensured that our current picks are still accurate.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html?src=rss",
          "feed_position": 35,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-06/f9e8ef90-0c55-11ee-97db-1ddd4c19f474"
        }
      ],
      "featured_image": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-09/a476c2e0-9780-11f0-bd4b-d87caa240702",
      "popularity_score": 2018.5236997222223
    },
    {
      "id": "cluster_96",
      "coverage": 2,
      "updated_at": "Tue, 24 Feb 2026 11:00:49 +0000",
      "title": "The US military will reportedly use Elon Musk's Grok AI in its classified systems",
      "neutral_headline": "US AI giant accuses Chinese rivals of mass data theft",
      "bullet_summary": [
        "The US Department of Defense has reportedly reached a deal to use Elon Musk's Grok in its classified systems, according to Axios",
        "Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife",
        "Anthropic reportedly refused to offer its tech for those things, even with a \"safety stack\" built into that model",
        "The Pentagon is reportedly also negotiating deals with OpenAI and Gemini, both of which it considers to be on par with Anthropic"
      ],
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/the-us-military-will-reportedly-use-elon-musks-grok-ai-in-its-classified-systems-110049021.html",
          "published_at": "Tue, 24 Feb 2026 11:00:49 +0000",
          "title": "The US military will reportedly use Elon Musk's Grok AI in its classified systems",
          "standfirst": "The US Department of Defense has reportedly reached a deal to use Elon Musk's Grok in its classified systems, according to Axios. That follows news that the Pentagon is currently in a dispute with another AI company, Anthropic, over limits on its technology for things like mass surveillance. Last year, the White ordered Grok, along with ChatGPT, Gemini and Anthropic's Claude to be approved for government use. Up until now, though, only Anthropic's model has been allowed for the military's most sensitive tasks in intelligence, weapons development and battlefield operations. Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife. However, the Pentagon demanded that Anthropic make Claude available for \"all lawful purposes\" including mass surveillance and the development of fully autonomous weapons. Anthropic reportedly refused to offer its tech for those things, even with a \"safety stack\" built into that model. xAI, by contrast, agreed to a standard that would allow the DoD to employ its AI for any purpose it deems \"lawful.\" However, the xAI model is not considered by officials to be as cutting-edge or reliable as Anthropic's Claude, and they admit that replacing Claude with Grok would be a challenge. The Pentagon is reportedly also negotiating deals with OpenAI and Gemini, both of which it considers to be on par with Anthropic. xAI had announced a version of Grok for US government agencies in July 2025. Shortly before that, though, the chatbot started spouting fascist propaganda and antisemitic rhetoric while dubbing itself \"MechaHitler.\" All of that followed a public spat between Musk and Trump over the president's spending bill, after which GSA approval of Grok seemed to stall. Earlier this week, Anthropic accused three Chinese AI labs of abusing Claude's AI with \"distillation attacks\" to improve their own models. This article originally appeared on Engadget at https://www.engadget.com/ai/the-us-military-will-reportedly-use-elon-musks-grok-ai-in-its-classified-systems-110049021.html?src=rss",
          "content": "The US Department of Defense has reportedly reached a deal to use Elon Musk's Grok in its classified systems, according to Axios. That follows news that the Pentagon is currently in a dispute with another AI company, Anthropic, over limits on its technology for things like mass surveillance. Last year, the White ordered Grok, along with ChatGPT, Gemini and Anthropic's Claude to be approved for government use. Up until now, though, only Anthropic's model has been allowed for the military's most sensitive tasks in intelligence, weapons development and battlefield operations. Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife. However, the Pentagon demanded that Anthropic make Claude available for \"all lawful purposes\" including mass surveillance and the development of fully autonomous weapons. Anthropic reportedly refused to offer its tech for those things, even with a \"safety stack\" built into that model. xAI, by contrast, agreed to a standard that would allow the DoD to employ its AI for any purpose it deems \"lawful.\" However, the xAI model is not considered by officials to be as cutting-edge or reliable as Anthropic's Claude, and they admit that replacing Claude with Grok would be a challenge. The Pentagon is reportedly also negotiating deals with OpenAI and Gemini, both of which it considers to be on par with Anthropic. xAI had announced a version of Grok for US government agencies in July 2025. Shortly before that, though, the chatbot started spouting fascist propaganda and antisemitic rhetoric while dubbing itself \"MechaHitler.\" All of that followed a public spat between Musk and Trump over the president's spending bill, after which GSA approval of Grok seemed to stall. Earlier this week, Anthropic accused three Chinese AI labs of abusing Claude's AI with \"distillation attacks\" to improve their own models. This article originally appeared on Engadget at https://www.engadget.com/ai/the-us-military-will-reportedly-use-elon-musks-grok-ai-in-its-classified-systems-110049021.html?src=rss",
          "feed_position": 19
        },
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/technology/2026/feb/23/us-ai-anthropic-china",
          "published_at": "Mon, 23 Feb 2026 23:15:50 GMT",
          "title": "US AI giant accuses Chinese rivals of mass data theft",
          "standfirst": "Anthropic says three Chinese firms used ‘distillation’ technique to extract information from its Claude chatbotUS artificial intelligence company Anthropic said on Monday it had uncovered campaigns by three Chinese AI firms to illicitly extract capabilities from its Claude chatbot, in what it described as industrial-scale intellectual property theft. OpenAI leveled similar charges last month.Anthropic said DeepSeek, Moonshot AI and MiniMax used a technique known as “distillation” – using outputs from a more powerful AI system to rapidly boost the performance of a less capable one. Continue reading...",
          "content": "Anthropic says three Chinese firms used ‘distillation’ technique to extract information from its Claude chatbotUS artificial intelligence company Anthropic said on Monday it had uncovered campaigns by three Chinese AI firms to illicitly extract capabilities from its Claude chatbot, in what it described as industrial-scale intellectual property theft. OpenAI leveled similar charges last month.Anthropic said DeepSeek, Moonshot AI and MiniMax used a technique known as “distillation” – using outputs from a more powerful AI system to rapidly boost the performance of a less capable one. Continue reading...",
          "feed_position": 4
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/anthropic-accuses-three-chinese-ai-labs-of-abusing-claude-to-improve-their-own-models-205210613.html",
          "published_at": "Mon, 23 Feb 2026 20:52:10 +0000",
          "title": "Anthropic accuses three Chinese AI labs of abusing Claude to improve their own models",
          "standfirst": "Anthropic is issuing a call to action against AI \"distillation attacks,\" after accusing three AI companies of misusing its Claude chatbot. On its website, Anthropic claimed that DeepSeek, Moonshot and MiniMax have been conducting \"industrial-scale campaigns…to illicitly extract Claude’s capabilities to improve their own models.\" Distillation in the AI world refers to when less capable models lean on the responses of more powerful ones to train themselves. While distillation isn't a bad thing across the board, Anthropic said that these types of attacks can be used in a more nefarious way. According to Anthropic, these three Chinese AI firms were responsible for more than \"16 million exchanges with Claude through approximately 24,000 fraudulent accounts.\" From Anthropic's perspective, these competing companies were using Claude as a shortcut to develop more advanced AI models, which could also lead to circumventing certain safeguards. Anthropic said in its post that it was able to link each of these distilling attack campaigns to the specific companies with \"high confidence\" thanks to IP address correlation, metadata requests and infrastructure indicators, along with corroborating with others in the AI industry who have noticed similar behaviors. Early last year, OpenAI made similar claims of rival firms distilling its models and banned suspected accounts in response. As for Anthropic, the company behind Claude said it would upgrade its system to make distillation attacks harder to do and easier to identify. While Anthropic is pointing fingers at these other firms, it's also facing a lawsuit from music publishers who accused the AI company of using illegal copies of songs to train its Claude chatbot.This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-accuses-three-chinese-ai-labs-of-abusing-claude-to-improve-their-own-models-205210613.html?src=rss",
          "content": "Anthropic is issuing a call to action against AI \"distillation attacks,\" after accusing three AI companies of misusing its Claude chatbot. On its website, Anthropic claimed that DeepSeek, Moonshot and MiniMax have been conducting \"industrial-scale campaigns…to illicitly extract Claude’s capabilities to improve their own models.\" Distillation in the AI world refers to when less capable models lean on the responses of more powerful ones to train themselves. While distillation isn't a bad thing across the board, Anthropic said that these types of attacks can be used in a more nefarious way. According to Anthropic, these three Chinese AI firms were responsible for more than \"16 million exchanges with Claude through approximately 24,000 fraudulent accounts.\" From Anthropic's perspective, these competing companies were using Claude as a shortcut to develop more advanced AI models, which could also lead to circumventing certain safeguards. Anthropic said in its post that it was able to link each of these distilling attack campaigns to the specific companies with \"high confidence\" thanks to IP address correlation, metadata requests and infrastructure indicators, along with corroborating with others in the AI industry who have noticed similar behaviors. Early last year, OpenAI made similar claims of rival firms distilling its models and banned suspected accounts in response. As for Anthropic, the company behind Claude said it would upgrade its system to make distillation attacks harder to do and easier to identify. While Anthropic is pointing fingers at these other firms, it's also facing a lawsuit from music publishers who accused the AI company of using illegal copies of songs to train its Claude chatbot.This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-accuses-three-chinese-ai-labs-of-abusing-claude-to-improve-their-own-models-205210613.html?src=rss",
          "feed_position": 23
        }
      ],
      "popularity_score": 2011.2586997222222
    },
    {
      "id": "cluster_17",
      "coverage": 1,
      "updated_at": "Tue, 24 Feb 2026 18:46:03 +0000",
      "title": "In a replay of 2019, Apple says a single desktop Mac will be manufactured in the US",
      "neutral_headline": "In a replay of 2019, Apple says a single desktop Mac will be manufactured in the US",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2026/02/in-a-replay-of-2019-apple-says-a-single-desktop-mac-will-be-manufactured-in-the-us/",
          "published_at": "Tue, 24 Feb 2026 18:46:03 +0000",
          "title": "In a replay of 2019, Apple says a single desktop Mac will be manufactured in the US",
          "standfirst": "Apple is still working to get favorable tariff treatment from the Trump administration.",
          "content": "Apple plans to start manufacturing the Mac mini in the United States later this year, the company announced today, as part of its $600 billion commitment to expand its domestic manufacturing operation. The Macs will be made in a facility in Houston, the same facility Apple uses for \"advanced AI server manufacturing.\" CEO Tim Cook says these AI servers are shipping \"ahead of schedule.\" The facility will also eventually provide \"hands-on training in advanced manufacturing techniques\" for students, Apple employees, \"and American businesses of all sizes.\" Apple and many other US tech companies have announced plans to expand their domestic manufacturing operations, just one element of a multi-prong strategy to secure favorable treatment from a Trump administration that has been happy to threaten Apple and others with steep tariffs to get what it wants. Today's Mac mini announcement is more subtle than the time Tim Cook delivered Trump a signed gold statue, but the goal is likely the same.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/IMG_2326-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/IMG_2326-1152x648.jpeg",
      "popularity_score": 364.01258861111114
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Tue, 24 Feb 2026 17:13:14 +0000",
      "title": "Inside the quixotic team trying to build an entire world in a 20-year-old game",
      "neutral_headline": "Inside the quixotic team trying to build an entire world in a 20-year-old game",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/02/inside-the-quixotic-team-trying-to-build-an-entire-world-in-a-20-year-old-game/",
          "published_at": "Tue, 24 Feb 2026 17:13:14 +0000",
          "title": "Inside the quixotic team trying to build an entire world in a 20-year-old game",
          "standfirst": "Stories and lesson learned from an impossibly large community modding project.",
          "content": "Despite being regarded as one of the greatest role-playing games of all time, The Elder Scrolls III: Morrowind disappointed some fans upon its release in 2002 because it didn't match the colossal scope of its predecessor, The Elder Scrolls II: Daggerfall. Almost immediately, fans began modding the remaining parts of the series’ fictional continent, Tamriel, into the game. Over 20 years later, thousands of volunteers have collaborated on the mod projects Tamriel Rebuilt and Project Tamriel, building a space comparable in size to a small country. Such projects often sputter out, but these have endured, thanks in part to a steady stream of small, manageable updates instead of larger, less frequent ones. A tale of (at least two) mods It's true that Daggerfall included an entire continent’s worth of content, but it was mostly composed of procedurally generated liminal space. By contrast, Morrowind contained just a single island—not even the entire province after which the game was named. The difference was that it was handcrafted.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Anvil-1152x648-1769206004.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Anvil-1152x648-1769206004.jpg",
      "popularity_score": 340.46564416666666
    },
    {
      "id": "cluster_49",
      "coverage": 1,
      "updated_at": "Tue, 24 Feb 2026 16:43:27 +0000",
      "title": "50 mpg in a Nissan crossover? Testing the new E-Power hybrid system.",
      "neutral_headline": "50 mpg in a Nissan crossover? Testing the new E-Power hybrid system.",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/02/on-the-road-with-nissans-new-e-power-hybrid-coming-to-the-2027-rogue/",
          "published_at": "Tue, 24 Feb 2026 16:43:27 +0000",
          "title": "50 mpg in a Nissan crossover? Testing the new E-Power hybrid system.",
          "standfirst": "Nissan imported some Qashqais from Europe so we could sample the hybrid system.",
          "content": "While Toyota and Honda's showrooms are littered with electrified offerings, Nissan hasn't had much to counter. Globally, Nissan offers a series hybrid system called E-Power, but the company has been reluctant to offer it Stateside. If you ask anyone at the company about it, they'll tell you that while it makes sense in Europe, Japan, and other parts of Asia, it is not optimized for the type of driving we do this side of the pond. Nissan's hybrid offerings in North America have been lackluster at best. There was the Altima that borrowed Toyota's hybrid system from the Camry, and there was the Rogue hybrid that failed to deliver noticeably better fuel economy. And that's really it. That, however, is about to change with the company's third-generation system.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/IMG_0763-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/IMG_0763-1152x648.jpeg",
      "popularity_score": 334.9692552777778
    },
    {
      "id": "cluster_68",
      "coverage": 1,
      "updated_at": "Tue, 24 Feb 2026 15:24:02 +0000",
      "title": "Lamborghini cancels electric Lanzador as supercar buyers reject EVs",
      "neutral_headline": "Lamborghini cancels electric Lanzador as supercar buyers reject EVs",
      "bullet_summary": [
        "Investing heavily in battery EVs would be \"financially irresponsible,\" CEO said",
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/02/lamborghini-drops-ev-plan-in-favor-of-future-plug-in-hybrids/",
          "published_at": "Tue, 24 Feb 2026 15:24:02 +0000",
          "title": "Lamborghini cancels electric Lanzador as supercar buyers reject EVs",
          "standfirst": "Investing heavily in battery EVs would be \"financially irresponsible,\" CEO said.",
          "content": "For the last few years, Lamborghini has been in a quandary: What to do about an electric vehicle? Among the supercar brands, Lamborghini has always stood out as favoring drama over lap times. And while electric motors and their instant torque can make a car accelerate very quickly indeed, other than the G-forces, it happens with such little fuss. Working out how to imbue an EV with enough \"wow\" factor to wear the famous bull badge has proved so difficult that the company has thrown in the towel in favor of developing more plug-in hybrids. As part of Volkswagen Group, Lamborghini has access to the EV platforms used by fellow VW Group brands Audi and Porsche, so it's not a question of access to technology. Rather, the company just doesn't think it can sell the cars. As Tim Stevens found out for Ars last year, in this rarefied end of the car market, the customers just aren't interested in EVs. People paying six or even seven figures for a supercar, especially a Lamborghini, are not exercising restraint, and they don't want the car to do that, either. Speaking to the Sunday Times this weekend, Lamborghini CEO Stephan Winkelmann revealed that the Lanzador, an electric SUV under development for the past few years, was canceled in late 2025. \"Investing heavily in full-EV development when the market and customer base are not ready would be an expensive hobby, and financially irresponsible towards shareholders, customers [and] to our employees and their families,\" he told the paper.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1610564490-1152x648-1771945747.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1610564490-1152x648-1771945747.jpg",
      "popularity_score": 324.64564416666667
    },
    {
      "id": "cluster_78",
      "coverage": 1,
      "updated_at": "Tue, 24 Feb 2026 14:10:13 +0000",
      "title": "Meta could end up owning 10% of AMD in new chip deal",
      "neutral_headline": "Meta could end up owning 10% of AMD in new chip deal",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/meta-could-end-up-owning-of-10-amd-in-new-chip-deal/",
          "published_at": "Tue, 24 Feb 2026 14:10:13 +0000",
          "title": "Meta could end up owning 10% of AMD in new chip deal",
          "standfirst": "AMD will supply 6 gigawatts' worth of chips to buttress Meta's AI efforts.",
          "content": "Meta has struck a multi-billion dollar chip deal with AMD that could lead to the Facebook owner taking a 10 percent stake in the group, sending shares in the US chipmaker surging on Tuesday. The social media giant said it would acquire customized chips with a total capacity of 6 gigawatts from AMD as it races to develop and deploy its AI models. AMD’s chief executive Lisa Su said that “each gigawatt of compute is worth double-digit billions” under the deal.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/07/meta-ai-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/07/meta-ai-1152x648.jpg",
      "popularity_score": 307.4153663888889
    },
    {
      "id": "cluster_81",
      "coverage": 1,
      "updated_at": "Tue, 24 Feb 2026 14:00:17 +0000",
      "title": "Scientists crack the case of \"screeching\" Scotch tape",
      "neutral_headline": "Scientists crack the case of \"screeching\" Scotch tape",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/02/heres-why-scotch-tape-screeches-when-its-peeled/",
          "published_at": "Tue, 24 Feb 2026 14:00:17 +0000",
          "title": "Scientists crack the case of \"screeching\" Scotch tape",
          "standfirst": "Micro-cracks travel along the peeling tape at supersonic speeds, producing shock waves and sound pulses.",
          "content": "Scotch tape has been a household mainstay for nearly a century, but it still holds some scientific surprises. Researchers have discovered that the screeching sound emitted when one rapidly peels Scotch tape—akin to the screech of fingernails on a chalkboard—is the result of shock waves produced by micro-cracks propagating along the tape at supersonic speeds, according to a new paper published in the journal Physical Review E. It was a 3M engineer named Richard Drew who developed the first transparent sticky tape in 1930. The impetus came from car manufacturing, specifically two-color designs, where the adhesives used were so sticky they often removed the paint when peeled off; the paint then needed to be manually touched up. Drew found a sandpaper adhesive with just the right amount of stickiness and used it to coat a roll of cellophane tape. (Fun fact: Drew also co-invented the snail-style dispenser for the tape with his 3M colleague, John Borden.) The tape was hugely popular during the Great Depression; consumers used it to repair everyday items rather than replace them. That popularity has never waned. Scotch tape has also generated considerable interest among physicists. Back in 1939, scientists noticed that peeling tape could produce light—specifically, a glowing line where the tape end pulls away from the roll. The phenomenon was first recorded in the 17th century and is known as triboluminescence: the generation of light when a material is crushed, ripped, rubbed, or scratched. Diamonds, for instance, sometimes glow blue or red during the cutting process, while ceramics emit yellow-orange light when being cut by abrasive water jets.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/scotch1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/scotch1-1152x648.jpg",
      "popularity_score": 297.24981083333336
    },
    {
      "id": "cluster_109",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 22:14:28 +0000",
      "title": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
      "neutral_headline": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/02/pentagon-buyer-were-happy-with-our-launch-industry-but-payloads-are-lagging/",
          "published_at": "Mon, 23 Feb 2026 22:14:28 +0000",
          "title": "Pentagon buyer: We're happy with our launch industry, but payloads are lagging",
          "standfirst": "\"The point is to get missions out the door as fast as possible. Two to three years is too slow.\"",
          "content": "DALLAS—The Space Force officer tasked with overseeing more than $24 billion in research and development spending says the Pentagon is more interested in supporting startups building new space sensors and payloads than adding yet another rocket company to its portfolio. The statement, made at a space finance conference in Dallas last week, was one of several points Maj. Gen. Stephen Purdy wanted to get across to a room full of investors and commercial space executives. The other points on Purdy's agenda were that the Space Force is more interested in high-volume production than spending money to develop the latest technologies, and that the military has, at least for now, lost one of its most important tools for supporting and diversifying the space industrial base.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1348666509-1152x648-1771883779.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1348666509-1152x648-1771883779.jpg",
      "popularity_score": 273
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 17:45:29 +0000",
      "title": "New Microsoft gaming chief has \"no tolerance for bad AI\"",
      "neutral_headline": "New Microsoft gaming chief has \"no tolerance for bad AI\"",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/02/new-microsoft-gaming-chief-has-no-tolerance-for-bad-ai/",
          "published_at": "Mon, 23 Feb 2026 17:45:29 +0000",
          "title": "New Microsoft gaming chief has \"no tolerance for bad AI\"",
          "standfirst": "But Asha Sharma faces scrutiny for lack of gaming experience.",
          "content": "Last week's surprise departure of Phil Spencer from Microsoft led to the promotion of Asha Sharma, who comes to head Microsoft's gaming division after two years as president of the company's CoreAI Product group. Despite that recent history, Sharma says in a new interview that she has \"no tolerance for bad AI\" in game development. Speaking with Variety, Sharma noted that \"AI has long been part of gaming and will continue to be,\" before adding that \"great stories are created by humans.\" The interview comes after Sharma promised in an introductory memo: \"We will not chase short-term efficiency or flood our ecosystem with soulless AI slop. Games are and always will be art, crafted by humans, and created with the most innovative technology provided by us.\" Those statements seem like a clear line in the sand from Sharma against the use of AI tools in Microsoft's first-party game development, at the very least. But what separates \"bad AI\" and \"soulless AI slop\" from \"innovative technology\" that humans can use to create artful games is a matter of some significant debate in the gaming world.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-1152x648.jpg",
      "popularity_score": 270
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 21:48:44 +0000",
      "title": "Data center builders thought farmers would willingly sell land, learn otherwise",
      "neutral_headline": "Data center builders thought farmers would willingly sell land, learn otherwise",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/",
          "published_at": "Mon, 23 Feb 2026 21:48:44 +0000",
          "title": "Data center builders thought farmers would willingly sell land, learn otherwise",
          "standfirst": "Even in a fragile farm economy, million-dollar offers can't sway dedicated farmers.",
          "content": "It seems that tech giants eyeing rural zones for data center development have underestimated how attached American farmers have grown to their lands in the decades they've been nurturing them. Across the country, several farmers have firmly rejected eye-popping offers—sometimes in the tens of millions. These offers dwarf the value of their properties, but farmers have refused to put a price on the lands that they love most. In a report on Monday, The Guardian highlighted a handful of cases nationwide where farmers' refusals have frustrated plans to build data centers in areas long deemed rural.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg",
      "popularity_score": 263
    },
    {
      "id": "cluster_111",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 21:16:20 +0000",
      "title": "Panasonic, the former plasma king, will no longer make its own TVs",
      "neutral_headline": "Panasonic, the former plasma king, will no longer make its own TVs",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/panasonic-the-former-plasma-king-will-no-longer-make-its-own-tvs/",
          "published_at": "Mon, 23 Feb 2026 21:16:20 +0000",
          "title": "Panasonic, the former plasma king, will no longer make its own TVs",
          "standfirst": "Panasonic was one of the last Japanese companies still manufacturing TVs.",
          "content": "Panasonic, once revered for its plasma TVs, is giving up on making its own TV sets. Today, it announced that Chinese company Skyworth will take over manufacturing, marketing, and selling Panasonic-branded TVs. Skyworth is a Shenzhen-headquartered TV brand. The company claims to be “a top three global provider of the Android TV platform.” In July, research firm Omdia reported that Skyworth was one of the top-five TV brands by sales revenue in Q1 2025; however, Skyworth hasn’t been able to maintain that position regularly. Panasonic made its announcement at a \"launch event,” FlatpanelsHD reported today. During the event, a Panasonic representative reportedly said:Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-56528381-1152x648-1771879994.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-56528381-1152x648-1771879994.jpg",
      "popularity_score": 253
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 15:38:00 +0000",
      "title": "AIs can generate near-verbatim copies of novels from training data",
      "neutral_headline": "AIs can generate near-verbatim copies of novels from training data",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/ais-can-generate-near-verbatim-copies-of-novels-from-training-data/",
          "published_at": "Mon, 23 Feb 2026 15:38:00 +0000",
          "title": "AIs can generate near-verbatim copies of novels from training data",
          "standfirst": "LLMs memorize more training data than previously thought.",
          "content": "The world’s top AI models can be prompted to generate near-verbatim copies of bestselling novels, raising fresh questions about the industry’s claim that its systems do not store copyrighted works. A series of recent studies has shown that large language models from OpenAI, Google, Meta, Anthropic, and xAI memorize far more of their training data than previously thought. AI and legal experts told the FT this “memorization” ability could have serious ramifications on AI groups’ battle against dozens of copyright lawsuits around the world, as it undermines their core defense that LLMs “learn” from copyrighted works but do not store copies.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-1152x648-1768598730.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-1152x648-1768598730.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_127",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 13:51:36 +0000",
      "title": "Review: Knight of the Seven Kingdoms brings back that Westeros magic",
      "neutral_headline": "Review: Knight of the Seven Kingdoms brings back that Westeros magic",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/02/review-knight-of-the-seven-kingdoms-brings-back-that-westeros-magic/",
          "published_at": "Mon, 23 Feb 2026 13:51:36 +0000",
          "title": "Review: Knight of the Seven Kingdoms brings back that Westeros magic",
          "standfirst": "Prequel series is just great storytelling, reminding GoT fans why they loved the original so much.",
          "content": "HBO has another critically acclaimed hit with A Knight of the Seven Kingdoms, based on George R.R. Martin’s Tales of Dunk and Egg novellas, and it deserves every bit of the praise heaped upon it. The immensely satisfying first season wrapped with last night's finale, dealing with the tragedy of the penultimate episode and setting the stage for the further adventures of Dunk and Egg. House of the Dragon is a solid series, but Knight of the Seven Kingdoms has reminded staunch GoT fans of everything they loved about the original series in the first place. (Spoilers below, but no major reveals until after the second gallery. We'll give you a heads up when we get there.) A Knight of the Seven Kingdoms adapts the first novella in the series, The Hedge Knight, and is set more than 50 years after the events of House of the Dragon. Dunk (Peter Claffey) is a lowly hedge knight who has just buried his aged mentor, Ser Arlan of Pennytree (Danny Webb). Ser Arlan was perhaps not the kindest of mentors and often stone drunk, but at least he was hung like the proverbial horse—as viewers discovered in a full-frontal moment that instantly went viral. Lacking any good employment options, Dunk decides to enter a local tournament, since he has inherited Ser Arlan's sword, shield, and three horses.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/dunkTOP-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/dunkTOP-1152x648.jpg",
      "popularity_score": 144
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 17:00:02 +0000",
      "title": "The 2026 Mazda CX-5, driven: It got bigger; plus, radical tech upgrade",
      "neutral_headline": "The 2026 Mazda CX-5, driven: It got bigger; plus, radical tech upgrade",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/02/the-2026-mazda-cx-5-driven-it-got-bigger-plus-radical-tech-upgrade/",
          "published_at": "Mon, 23 Feb 2026 17:00:02 +0000",
          "title": "The 2026 Mazda CX-5, driven: It got bigger; plus, radical tech upgrade",
          "standfirst": "Starting at $29,990, there's a lot to like about the all-new Mazda, but it's not perfect.",
          "content": "Mazda provided flights from Washington, DC, to San Diego and accommodation so Ars could drive the CX-5. Ars does not accept paid editorial content. ENCINITAS, Calif.—Its sales may have been buoyed of late by the big CX-90 and CX-70 SUVs, but for Mazda, the CX-5 is still where most of the action is. Unlike the similar-sized, similar-priced CX-50, which was designed just for North America, the all-new CX-5 is a global car, and it's also Mazda's standard-bearer for a range of new technologies. Gone is the basic but effective infotainment system, replaced by an all-new Google-based experience as Mazda starts its journey toward software-defined vehicles. There's even an in-house hybrid on the way, albeit not until next year. And it starts at a competitive $29,990. The new CX-5 is bigger than the car it replaces, 4.5 inches (114.5 mm) longer and half an inch (13 mm) wider than before, at 184.6 inches (4,689 mm) long, 73.2 inches (1,859 mm) wide, and 66.7 inches (1,694 mm) tall. Much of that extra space is between the axles—the wheelbase is now 110 inches (2,794 mm) long, which translates to more interior space. From the outside, there's a new light signature, and the way the bodywork curves around the front and wraps down the fenders gives me strong Range Rover vibes, even if I could never adequately capture what I'm talking about with a camera. As ever, Mazda's arresting Soul Red Crystal metallic paint (a $595 option) sparkles, even on a day when the sun remained hidden from view. The last time that Mazda evolved this compact crossover, it did so with a new upmarket interior. Since then, the brand has staked out that space across its model lineup, with cabins that punch well above their price tags. Happily, the company's designers haven't lost much mojo since then, with a restrained approach that looks good across the five different trim levels, each of which is a $2,000 step up from the one that precedes it. But if you're a current CX-5 driver, you'll find much has changed, perhaps not entirely for the better.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/2026-Mazda-CX-5-1-1152x648-1771861550.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/2026-Mazda-CX-5-1-1152x648-1771861550.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_129",
      "coverage": 1,
      "updated_at": "Mon, 23 Feb 2026 12:00:45 +0000",
      "title": "The first cars bold enough to drive themselves",
      "neutral_headline": "The first cars bold enough to drive themselves",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/features/2026/02/the-first-cars-bold-enough-to-drive-themselves/",
          "published_at": "Mon, 23 Feb 2026 12:00:45 +0000",
          "title": "The first cars bold enough to drive themselves",
          "standfirst": "Quevedo's telekino of 1904 was the first step on the road to autonomous Waymos.",
          "content": "No one knows exactly when the vehicles we drive will finally wrest the steering wheel from us. But the age of the autonomous automobile isn’t some sudden Big Bang. It’s more of a slow crawl, one that started during the Roosevelt administration. And that’s Theodore, not Franklin. And not in America, but in Spain, by someone you’ve probably never heard of. His name was Leonardo Torres Quevedo, a Spanish engineer born in Santa Cruz, Spain, in 1852. Smart? In 1914, he developed a mechanical chess machine that autonomously played against humans. But more than a decade earlier, he pioneered the development of remote-control systems. What he wrought was brilliant, if crude—and certainly ahead of its time. The first wireless control It was called the Telekino, a name drawn from the Greek “tele,” meaning at a distance, and “kino,” meaning movement. Patented in Spain, France, and the United States, it was conceived as a way to prevent airship accidents. The Telekino transmitted wireless signals to a small receiver known as a coherer, which detected electromagnetic waves and transformed them into an electrical current. This current was amplified and sent on to electromagnets that slowly rotated a switch controlling the proper servomotor. Quevedo could issue 19 distinct commands to the systems of an airship without ever touching a control cable.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/radio-controlled-vintage-cars-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/radio-controlled-vintage-cars-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}