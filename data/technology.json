{
  "updated_at": "2026-02-28T07:20:45.748Z",
  "clusters": [
    {
      "id": "cluster_0",
      "coverage": 2,
      "updated_at": "Sat, 28 Feb 2026 02:15:02 -0500",
      "title": "US Congressional Joint Economic Committee report: US consumers lost $20.9B nominally to identity theft from four major data broker breaches over the past decade (Dell Cameron/Wired)",
      "neutral_headline": "Data Broker Breaches Fueled Nearly $21 Billion in Identity-Theft Losses",
      "bullet_summary": [
        "Reported by TechMeme, Wired Tech"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260228/p4#a260228p4",
          "published_at": "Sat, 28 Feb 2026 02:15:02 -0500",
          "title": "US Congressional Joint Economic Committee report: US consumers lost $20.9B nominally to identity theft from four major data broker breaches over the past decade (Dell Cameron/Wired)",
          "standfirst": "Dell Cameron / Wired: US Congressional Joint Economic Committee report: US consumers lost $20.9B nominally to identity theft from four major data broker breaches over the past decade &mdash; A report copublished by WIRED sparked a probe into opt-out pages hidden by data brokers. Now congressional Democrats &hellip;",
          "content": "Dell Cameron / Wired: US Congressional Joint Economic Committee report: US consumers lost $20.9B nominally to identity theft from four major data broker breaches over the past decade &mdash; A report copublished by WIRED sparked a probe into opt-out pages hidden by data brokers. Now congressional Democrats &hellip;",
          "feed_position": 0,
          "image_url": "http://www.techmeme.com/260228/i4.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/data-broker-breaches-fueled-dollar209-billion-in-identity-theft-losses/",
          "published_at": "Fri, 27 Feb 2026 10:00:00 +0000",
          "title": "Data Broker Breaches Fueled Nearly $21 Billion in Identity-Theft Losses",
          "standfirst": "A report copublished by WIRED sparked a probe into opt-out pages hidden by data brokers. Now congressional Democrats say breaches tied to the industry have cost people tens of billions of dollars.",
          "content": "A report copublished by WIRED sparked a probe into opt-out pages hidden by data brokers. Now congressional Democrats say breaches tied to the industry have cost people tens of billions of dollars.",
          "feed_position": 15,
          "image_url": "https://media.wired.com/photos/699f3b7fc95722828c3f077b/master/pass/Data-Brokers-Stop-Hiding-Opt-Out-Pages-From-Google-After-US-Senators-Probe-Security-2245425437.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260228/i4.jpg",
      "popularity_score": 2019.9045144444444
    },
    {
      "id": "cluster_2",
      "coverage": 2,
      "updated_at": "Sat, 28 Feb 2026 06:16:00 GMT",
      "title": "Anthropic vs. The Pentagon: what enterprises should do",
      "neutral_headline": "Anthropic vs. The Pentagon: what enterprises should do",
      "bullet_summary": [
        "OpenAI CEO Sam Altman just announced a deal with the Pentagon that includes two similar sounding \"safety principles,\" though whether they are the same type of contractual language is still not clear",
        "Earlier in the day, OpenAI announced a staggering $110 billion investment round led by Amazon, Nvidia, and SoftBank",
        "OpenAI has reached an agreement with the Defense Department to deploy its models in the agency’s network, company chief Sam Altman has revealed on X",
        "He said it was the same “compromise that Anthropic was offered, and rejected"
      ],
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/anthropic-vs-the-pentagon-what-enterprises-should-do",
          "published_at": "Sat, 28 Feb 2026 06:16:00 GMT",
          "title": "Anthropic vs. The Pentagon: what enterprises should do",
          "standfirst": "The relationship between one of Silicon Valley&#x27;s most lucrative and powerful AI model makers, Anthropic, and the U.S. government reached a breaking point on Friday, February 27, 2026.President Donald J. Trump and the White House posted on social media ordering all federal agencies to immediately cease using technology from Anthropic, the maker of the powerful Claude family of AI models, after reportedly months of renegotiating a less than two-year-old contract. Following the President’s lead, Secretary of War Pete Hegseth said he was directing the Department of War to designate Anthropic a \"Supply-Chain Risk to National Security,\" a blacklisting traditionally reserved for foreign adversaries like Huawei or Kaspersky Lab. The move effectively terminates Anthropic&#x27;s $200 million military contract and sets a hard six-month deadline for the Department of War to scrub Claude from its systems.But Anthropic&#x27;s business has been booming lately, with its Claude Code service alone taking off into a $2.5+ billion ARR division less than a year after launch, and it just announced a $30 billion Series G at $380 billion valuation earlier this month and has, more or less singlehandedly spurred massive stock dives in the SaaS sector by releasing plugins and skills for specific enterprise and verticalized industry functions including HR, design, engineering, operations, financial analysis, investment banking, equity research, private equity, and wealth management. Ironically, SaaS companies across industries and sectors such as Salesforce, Spotify, Novo Nordisk, Thompson Reuters and more are reporting some of the biggest benefits in productivity and performance thanks to Anthropic&#x27;s top benchmark-scoring, highly capable and effective Claude AI models. It&#x27;s not a stretch to say Anthropic is among the most successful AI labs in the U.S. and globally. So why is it now being considered a \"Supply-Chain Risk to National Security?\"Why is the Pentagon designating Anthropic a &#x27;Supply-Chain Risk to National Security&#x27; and why now?The rupture stems from a fundamental dispute over \"all lawful use.\" The Pentagon demanded unrestricted access to Claude for any mission deemed legal, while Anthropic CEO Dario Amodei refused to budge on two specific \"red lines\": the use of its models for mass surveillance of American citizens and fully autonomous lethal weaponry.Hegseth characterized the refusal as \"arrogance and betrayal,\" while Amodei maintained that such guardrails are essential to prevent \"unintended escalation or mission failure.\" The fallout is immediate; the Department of War has ordered all contractors and partners to stop conducting commercial activity with Anthropic effectively at once, though the Pentagon itself has a 180-day window to transition to \"more patriotic\" providers.The vacuum left by Anthropic is already being filled by its primary rivals. OpenAI CEO Sam Altman just announced a deal with the Pentagon that includes two similar sounding \"safety principles,\" though whether they are the same type of contractual language is still not clear. Earlier in the day, OpenAI announced a staggering $110 billion investment round led by Amazon, Nvidia, and SoftBank. Elon Musk’s xAI has also reportedly signed a deal to allow its Grok model to be used in highly classified systems, having agreed to the \"all lawful use\" standard that Anthropic rejected, but is said to rate poorly among government and military workers already using it. Meanwhile, Anthropic has stated its intention to fight the designation in court and has encouraged its commercial customers to continue usage of its products and services with the exception of military work. What it means for enterprises: the interoperability imperativeFor enterprise technical decision-makers, the \"Anthropic Ban\" is a clarion call that transcends the specific politics of the Trump administration. Regardless of whether you agree with Anthropic’s ethical stance (as I do) or the Pentagon&#x27;s position, the core takeaway is the same: model interoperability is more important than ever. If your entire agentic workflow or customer-facing stack is hard-coded to a single provider&#x27;s API, you aren&#x27;t going to be nimble or flexible enough to meet the demands of a marketplace where some potential customers, such as the U.S. military or government, want you to use or avoid specific models as conditions of your contracts with them. The most prudent move right now isn&#x27;t necessarily to hit the \"delete\" button on Claude—which remains a best-in-class model for coding and nuanced reasoning—but to ensure you have a \"warm standby.\" This means utilizing orchestration layers and standardized prompting formats that allow you to toggle between Claude, GPT-4o, and Gemini 1.5 Pro without massive performance degradation. If you can’t switch providers in a 24-hour sprint, your supply chain is brittle.Diversify your AI supplyWhile the U.S. giants scramble for the Pentagon&#x27;s favor, the market is fragmenting in ways that offer surprising hedges. Google Gemini saw its stock spike following the news, and OpenAI&#x27;s massive new cash infusion from Amazon (formerly a staunch Anthropic ally) signals a consolidation of power. However, don&#x27;t overlook the \"open\" and international alternatives. U.S. firms like Airbnb have already made waves by pivoting to lower cost, Chinese open-source models like Alibaba’s Qwen for certain customer service functions, citing cost and flexibility. While Chinese models carry their own set of arguably greater geopolitical risks, for some enterprises, they serve as a viable hedge against the current volatility of the U.S. domestic market.More realistically for most, the move toward in-house hosting via domestic brews like OpenAI&#x27;s GPT-OSS series, IBM&#x27;s Granite, Meta’s Llama, Arcee&#x27;s Trinity models, AI2&#x27;s Olmo, Liquid AI&#x27;s smaller LFM2 models, or other high-performing open-source weights is the ultimate insurance policy. Third-party benchmarking tools like Artificial Analysis and Pinchbench can help enterprises decide which models meet their cost and performance criteria in the tasks and workloads they are being deployed.By running models locally or in a private cloud and fine-tuning them on your proprietary data, you insulate your business from the \"Terms of Service\" wars and federal blacklists.Even if a secondary model is slightly inferior in benchmark performance, having it ready to scale up prevents a total blackout if your primary provider is suddenly \"besieged\" by government reprisal. It’s just good business: you need to diversify your supply. The new due diligenceAs an enterprise leader, your due diligence checklist has just expanded thanks to a volatile federal vs. private sector fight. The takeaway is clear: if you plan to maintain business with federal agencies, you must be able to certify to them that your products aren&#x27;t built on any single prohibited model provider — however sudden that designation may come down. Ultimately, this is a lesson in strategic redundancy. The AI era was supposed to be about the democratization of intelligence, but it’s currently looking like a classic battle over defense procurement and executive power. Secure your backup and diversified suppliers, build for portability, and don&#x27;t let your \"agents\" become collateral damage in the war between the government and any specific company. Whether you’re motivated by ideological support for Anthropic or cold-blooded bottom-line protection, the path forward is the same: diversify, decouple, and be ready to swap in and out fast.Model interoperability just became the new enterprise \"must-have.\"",
          "content": "The relationship between one of Silicon Valley&#x27;s most lucrative and powerful AI model makers, Anthropic, and the U.S. government reached a breaking point on Friday, February 27, 2026.President Donald J. Trump and the White House posted on social media ordering all federal agencies to immediately cease using technology from Anthropic, the maker of the powerful Claude family of AI models, after reportedly months of renegotiating a less than two-year-old contract. Following the President’s lead, Secretary of War Pete Hegseth said he was directing the Department of War to designate Anthropic a \"Supply-Chain Risk to National Security,\" a blacklisting traditionally reserved for foreign adversaries like Huawei or Kaspersky Lab. The move effectively terminates Anthropic&#x27;s $200 million military contract and sets a hard six-month deadline for the Department of War to scrub Claude from its systems.But Anthropic&#x27;s business has been booming lately, with its Claude Code service alone taking off into a $2.5+ billion ARR division less than a year after launch, and it just announced a $30 billion Series G at $380 billion valuation earlier this month and has, more or less singlehandedly spurred massive stock dives in the SaaS sector by releasing plugins and skills for specific enterprise and verticalized industry functions including HR, design, engineering, operations, financial analysis, investment banking, equity research, private equity, and wealth management. Ironically, SaaS companies across industries and sectors such as Salesforce, Spotify, Novo Nordisk, Thompson Reuters and more are reporting some of the biggest benefits in productivity and performance thanks to Anthropic&#x27;s top benchmark-scoring, highly capable and effective Claude AI models. It&#x27;s not a stretch to say Anthropic is among the most successful AI labs in the U.S. and globally. So why is it now being considered a \"Supply-Chain Risk to National Security?\"Why is the Pentagon designating Anthropic a &#x27;Supply-Chain Risk to National Security&#x27; and why now?The rupture stems from a fundamental dispute over \"all lawful use.\" The Pentagon demanded unrestricted access to Claude for any mission deemed legal, while Anthropic CEO Dario Amodei refused to budge on two specific \"red lines\": the use of its models for mass surveillance of American citizens and fully autonomous lethal weaponry.Hegseth characterized the refusal as \"arrogance and betrayal,\" while Amodei maintained that such guardrails are essential to prevent \"unintended escalation or mission failure.\" The fallout is immediate; the Department of War has ordered all contractors and partners to stop conducting commercial activity with Anthropic effectively at once, though the Pentagon itself has a 180-day window to transition to \"more patriotic\" providers.The vacuum left by Anthropic is already being filled by its primary rivals. OpenAI CEO Sam Altman just announced a deal with the Pentagon that includes two similar sounding \"safety principles,\" though whether they are the same type of contractual language is still not clear. Earlier in the day, OpenAI announced a staggering $110 billion investment round led by Amazon, Nvidia, and SoftBank. Elon Musk’s xAI has also reportedly signed a deal to allow its Grok model to be used in highly classified systems, having agreed to the \"all lawful use\" standard that Anthropic rejected, but is said to rate poorly among government and military workers already using it. Meanwhile, Anthropic has stated its intention to fight the designation in court and has encouraged its commercial customers to continue usage of its products and services with the exception of military work. What it means for enterprises: the interoperability imperativeFor enterprise technical decision-makers, the \"Anthropic Ban\" is a clarion call that transcends the specific politics of the Trump administration. Regardless of whether you agree with Anthropic’s ethical stance (as I do) or the Pentagon&#x27;s position, the core takeaway is the same: model interoperability is more important than ever. If your entire agentic workflow or customer-facing stack is hard-coded to a single provider&#x27;s API, you aren&#x27;t going to be nimble or flexible enough to meet the demands of a marketplace where some potential customers, such as the U.S. military or government, want you to use or avoid specific models as conditions of your contracts with them. The most prudent move right now isn&#x27;t necessarily to hit the \"delete\" button on Claude—which remains a best-in-class model for coding and nuanced reasoning—but to ensure you have a \"warm standby.\" This means utilizing orchestration layers and standardized prompting formats that allow you to toggle between Claude, GPT-4o, and Gemini 1.5 Pro without massive performance degradation. If you can’t switch providers in a 24-hour sprint, your supply chain is brittle.Diversify your AI supplyWhile the U.S. giants scramble for the Pentagon&#x27;s favor, the market is fragmenting in ways that offer surprising hedges. Google Gemini saw its stock spike following the news, and OpenAI&#x27;s massive new cash infusion from Amazon (formerly a staunch Anthropic ally) signals a consolidation of power. However, don&#x27;t overlook the \"open\" and international alternatives. U.S. firms like Airbnb have already made waves by pivoting to lower cost, Chinese open-source models like Alibaba’s Qwen for certain customer service functions, citing cost and flexibility. While Chinese models carry their own set of arguably greater geopolitical risks, for some enterprises, they serve as a viable hedge against the current volatility of the U.S. domestic market.More realistically for most, the move toward in-house hosting via domestic brews like OpenAI&#x27;s GPT-OSS series, IBM&#x27;s Granite, Meta’s Llama, Arcee&#x27;s Trinity models, AI2&#x27;s Olmo, Liquid AI&#x27;s smaller LFM2 models, or other high-performing open-source weights is the ultimate insurance policy. Third-party benchmarking tools like Artificial Analysis and Pinchbench can help enterprises decide which models meet their cost and performance criteria in the tasks and workloads they are being deployed.By running models locally or in a private cloud and fine-tuning them on your proprietary data, you insulate your business from the \"Terms of Service\" wars and federal blacklists.Even if a secondary model is slightly inferior in benchmark performance, having it ready to scale up prevents a total blackout if your primary provider is suddenly \"besieged\" by government reprisal. It’s just good business: you need to diversify your supply. The new due diligenceAs an enterprise leader, your due diligence checklist has just expanded thanks to a volatile federal vs. private sector fight. The takeaway is clear: if you plan to maintain business with federal agencies, you must be able to certify to them that your products aren&#x27;t built on any single prohibited model provider — however sudden that designation may come down. Ultimately, this is a lesson in strategic redundancy. The AI era was supposed to be about the democratization of intelligence, but it’s currently looking like a classic battle over defense procurement and executive power. Secure your backup and diversified suppliers, build for portability, and don&#x27;t let your \"agents\" become collateral damage in the war between the government and any specific company. Whether you’re motivated by ideological support for Anthropic or cold-blooded bottom-line protection, the path forward is the same: diversify, decouple, and be ready to swap in and out fast.Model interoperability just became the new enterprise \"must-have.\"",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4VfkSUNLbEJKeO0m863euh/30d0739f8ff325650151b76c51fae3ac/Gemini_Generated_Image_gt46srgt46srgt46.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/openai-strikes-a-deal-with-the-defense-department-to-deploy-its-ai-models-054441785.html",
          "published_at": "Sat, 28 Feb 2026 05:44:41 +0000",
          "title": "OpenAI strikes a deal with the Defense Department to deploy its AI models",
          "standfirst": "OpenAI has reached an agreement with the Defense Department to deploy its models in the agency’s network, company chief Sam Altman has revealed on X. In his post, he said two of OpenAI’s most important safety principles are “prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems.” Altman claimed the company put those principles in its agreement with the agency, which he called by the government’s preferred name of Department of War (DoW), and that it had agreed to honor them. The agency has closed the deal with OpenAI, shortly after President Donald Trump ordered all government agencies to stop using Claude and any other Anthropic services. If you’ll recall, US Defense Secretary Pete Hegseth previously threatened to label Anthropic “supply chain risk” if it continues refusing to remove the guardrails on its AI, which are preventing the technology to be used for mass surveillance against Americans and in fully autonomous weapons. It’s unclear why the government agreed to team up with OpenAI if its models also have the same guardrails, but Altman said it’s asking the government to offer the same terms to all the AI companies it works with. Jeremy Lewin, the Senior Official Under Secretary for Foreign Assistance, Humanitarian Affairs, and Religious Freedom, said on X that DoW “references certain existing legal authorities and includes certain mutually agreed upon safety mechanisms” in its contracts. Both OpenAI and xAI, which had also previously signed a deal to deploy Grok in the DoW’s classified systems, agreed to those terms. He said it was the same “compromise that Anthropic was offered, and rejected.”Anthropic, which started working with the US government in 2024, refused to bow down to Hegseth. In its latest statement, published just hours before Altman announced OpenAI’s agreement, it repeated its stance. “No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons,” Anthropic wrote. “We will challenge any supply chain risk designation in court.” Altman added in his post on X that OpenAI will build technical safeguards to ensure the company’s models behave as they should, claiming that’s also what the DoW wanted. It’s sending engineers to work with the agency to “ensure [its models’] safety,” and it will only deploy on cloud networks. As The New York Times notes, OpenAI is not yet on Amazon cloud, which the government uses. But that could change soon, as company has also just announced forming a partnership with Amazon to run its models on Amazon Web Services (AWS) for enterprise customers. Tonight, we reached an agreement with the Department of War to deploy our models in their classified network.In all of our interactions, the DoW displayed a deep respect for safety and a desire to partner to achieve the best possible outcome.AI safety and wide distribution of…— Sam Altman (@sama) February 28, 2026 This article originally appeared on Engadget at https://www.engadget.com/ai/openai-strikes-a-deal-with-the-defense-department-to-deploy-its-ai-models-054441785.html?src=rss",
          "content": "OpenAI has reached an agreement with the Defense Department to deploy its models in the agency’s network, company chief Sam Altman has revealed on X. In his post, he said two of OpenAI’s most important safety principles are “prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems.” Altman claimed the company put those principles in its agreement with the agency, which he called by the government’s preferred name of Department of War (DoW), and that it had agreed to honor them. The agency has closed the deal with OpenAI, shortly after President Donald Trump ordered all government agencies to stop using Claude and any other Anthropic services. If you’ll recall, US Defense Secretary Pete Hegseth previously threatened to label Anthropic “supply chain risk” if it continues refusing to remove the guardrails on its AI, which are preventing the technology to be used for mass surveillance against Americans and in fully autonomous weapons. It’s unclear why the government agreed to team up with OpenAI if its models also have the same guardrails, but Altman said it’s asking the government to offer the same terms to all the AI companies it works with. Jeremy Lewin, the Senior Official Under Secretary for Foreign Assistance, Humanitarian Affairs, and Religious Freedom, said on X that DoW “references certain existing legal authorities and includes certain mutually agreed upon safety mechanisms” in its contracts. Both OpenAI and xAI, which had also previously signed a deal to deploy Grok in the DoW’s classified systems, agreed to those terms. He said it was the same “compromise that Anthropic was offered, and rejected.”Anthropic, which started working with the US government in 2024, refused to bow down to Hegseth. In its latest statement, published just hours before Altman announced OpenAI’s agreement, it repeated its stance. “No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons,” Anthropic wrote. “We will challenge any supply chain risk designation in court.” Altman added in his post on X that OpenAI will build technical safeguards to ensure the company’s models behave as they should, claiming that’s also what the DoW wanted. It’s sending engineers to work with the agency to “ensure [its models’] safety,” and it will only deploy on cloud networks. As The New York Times notes, OpenAI is not yet on Amazon cloud, which the government uses. But that could change soon, as company has also just announced forming a partnership with Amazon to run its models on Amazon Web Services (AWS) for enterprise customers. Tonight, we reached an agreement with the Department of War to deploy our models in their classified network.In all of our interactions, the DoW displayed a deep respect for safety and a desire to partner to achieve the best possible outcome.AI safety and wide distribution of…— Sam Altman (@sama) February 28, 2026 This article originally appeared on Engadget at https://www.engadget.com/ai/openai-strikes-a-deal-with-the-defense-department-to-deploy-its-ai-models-054441785.html?src=rss",
          "feed_position": 0
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/trump-orders-federal-agencies-to-drop-anthropic-services-amid-pentagon-feud-222029306.html",
          "published_at": "Sat, 28 Feb 2026 02:08:49 +0000",
          "title": "Trump orders federal agencies to drop Anthropic services amid Pentagon feud",
          "standfirst": "President Donald Trump has ordered all US government agencies to stop using Claude and other Anthropic services, escalating an already volatile feud between the Department of Defense and company over AI safeguards. Taking to Truth Social on Friday afternoon, the president said there would be a six-month phase out period for federal agencies, including the Defense Department, to migrate off of Anthropic's products. “The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution,” the president wrote. “Anthropic better get their act together, and be helpful during this phase out period, or I will use the Full Power of the Presidency to make them comply, with major civil and criminal consequences to follow.” Before today, US Defense Secretary Pete Hegseth had threatened to label Anthropic a “supply chain risk” if it did not agree to withdraw safeguards that insist Claude not be used for mass surveillance against Americans or in fully autonomous weapons. In a post on X published after President Trump’s statement, Hegseth said he was “directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic.”Anthropic did not immediately respond to Engadget's comment request. Earlier in the day, a spokesperson for the company said the contract Anthropic received after CEO Dario Amodei outlined Anthropic's position made “virtually no progress” on preventing the outlined misuses. \"New language framed as a compromise was paired with legalese that would allow those safeguards to be disregarded at will. Despite DOW's recent public statements, these narrow safeguards have been the crux of our negotiations for months,\" the spokesperson said. \"We remain ready to continue talks and committed to operational continuity for the Department and America's warfighters.\" Advocacy groups like the Center for Democracy and Technology (CDT) quickly came out against the president’s threats. “This action sets a dangerous precedent. It chills private companies’ ability to engage frankly with the government about appropriate uses of their technology, which is especially important in national security settings that so often have reduced public visibility,” said CDT President and CEO Alexandra Givens, in a statement shared with Engadget. “These threats undermine the integrity of the innovation ecosystem, distort market incentives and normalize an expansive view of executive power that should worry Americans all across the political spectrum.”For now, it appears the AI industry is united behind Anthropic. On Friday, hundreds of Google and OpenAI employees signed an open letter urging their companies to stand in \"solidarity\" with the lab. According to an internal memo seen by Axios, OpenAI CEO Sam Altman said the ChatGPT maker would draw the same red line as Anthropic. In a blog post published late on Friday, Anthropic vowed to “challenge any supply chain risk designation in court,” and assured its customers that only work related to the Defense Department would be affected. The company's full statement is available here, an excerpt is below:Designating Anthropic as a supply chain risk would be an unprecedented action—one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US government’s classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so.We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government.No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons. We will challenge any supply chain risk designation in court.Update, February 27, 9PM ET: This story was updated twice after publish. First at 6PM ET to include a link to and quotes from Hegseth about the designation of Anthropic as a supply chain risk. Later, a quote from Anthropic was added, along with a link to the company’s blog post on the subject.This article originally appeared on Engadget at https://www.engadget.com/ai/trump-orders-federal-agencies-to-drop-anthropic-services-amid-pentagon-feud-222029306.html?src=rss",
          "content": "President Donald Trump has ordered all US government agencies to stop using Claude and other Anthropic services, escalating an already volatile feud between the Department of Defense and company over AI safeguards. Taking to Truth Social on Friday afternoon, the president said there would be a six-month phase out period for federal agencies, including the Defense Department, to migrate off of Anthropic's products. “The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution,” the president wrote. “Anthropic better get their act together, and be helpful during this phase out period, or I will use the Full Power of the Presidency to make them comply, with major civil and criminal consequences to follow.” Before today, US Defense Secretary Pete Hegseth had threatened to label Anthropic a “supply chain risk” if it did not agree to withdraw safeguards that insist Claude not be used for mass surveillance against Americans or in fully autonomous weapons. In a post on X published after President Trump’s statement, Hegseth said he was “directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic.”Anthropic did not immediately respond to Engadget's comment request. Earlier in the day, a spokesperson for the company said the contract Anthropic received after CEO Dario Amodei outlined Anthropic's position made “virtually no progress” on preventing the outlined misuses. \"New language framed as a compromise was paired with legalese that would allow those safeguards to be disregarded at will. Despite DOW's recent public statements, these narrow safeguards have been the crux of our negotiations for months,\" the spokesperson said. \"We remain ready to continue talks and committed to operational continuity for the Department and America's warfighters.\" Advocacy groups like the Center for Democracy and Technology (CDT) quickly came out against the president’s threats. “This action sets a dangerous precedent. It chills private companies’ ability to engage frankly with the government about appropriate uses of their technology, which is especially important in national security settings that so often have reduced public visibility,” said CDT President and CEO Alexandra Givens, in a statement shared with Engadget. “These threats undermine the integrity of the innovation ecosystem, distort market incentives and normalize an expansive view of executive power that should worry Americans all across the political spectrum.”For now, it appears the AI industry is united behind Anthropic. On Friday, hundreds of Google and OpenAI employees signed an open letter urging their companies to stand in \"solidarity\" with the lab. According to an internal memo seen by Axios, OpenAI CEO Sam Altman said the ChatGPT maker would draw the same red line as Anthropic. In a blog post published late on Friday, Anthropic vowed to “challenge any supply chain risk designation in court,” and assured its customers that only work related to the Defense Department would be affected. The company's full statement is available here, an excerpt is below:Designating Anthropic as a supply chain risk would be an unprecedented action—one historically reserved for US adversaries, never before publicly applied to an American company. We are deeply saddened by these developments. As the first frontier AI company to deploy models in the US government’s classified networks, Anthropic has supported American warfighters since June 2024 and has every intention of continuing to do so.We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government.No amount of intimidation or punishment from the Department of War will change our position on mass domestic surveillance or fully autonomous weapons. We will challenge any supply chain risk designation in court.Update, February 27, 9PM ET: This story was updated twice after publish. First at 6PM ET to include a link to and quotes from Hegseth about the designation of Anthropic as a supply chain risk. Later, a quote from Anthropic was added, along with a link to the company’s blog post on the subject.This article originally appeared on Engadget at https://www.engadget.com/ai/trump-orders-federal-agencies-to-drop-anthropic-services-amid-pentagon-feud-222029306.html?src=rss",
          "feed_position": 1
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/googles-opal-just-quietly-showed-enterprise-teams-the-new-blueprint-for",
          "published_at": "Fri, 27 Feb 2026 23:25:00 GMT",
          "title": "Google's Opal just quietly showed enterprise teams the new blueprint for building AI agents",
          "standfirst": "For the past year, the enterprise AI community has been locked in a debate about how much freedom to give AI agents. Too little, and you get expensive workflow automation that barely justifies the \"agent\" label. Too much, and you get the kind of data-wiping disasters that plagued early adopters of tools like OpenClaw. This week, Google Labs released an update to Opal, its no-code visual agent builder, that quietly lands on an answer — and it carries lessons that every IT leader planning an agent strategy should study carefully.The update introduces what Google calls an \"agent step\" that transforms Opal&#x27;s previously static, drag-and-drop workflows into dynamic, interactive experiences. Instead of manually specifying which model or tool to call and in what order, builders can now define a goal and let the agent determine the best path to reach it — selecting tools, triggering models like Gemini 3 Flash or Veo for video generation, and even initiating conversations with users when it needs more information.It sounds like a modest product update. It is not. What Google has shipped is a working reference architecture for the three capabilities that will define enterprise agents in 2026:Adaptive routingPersistent memoryHuman-in-the-loop orchestration...and it&#x27;s all made possible by the rapidly improving reasoning abilities of frontier models like the Gemini 3 series.The &#x27;off the rails&#x27; inflection point: Why better models change everything about agent designTo understand why the Opal update matters, you need to understand a shift that has been building across the agent ecosystem for months.The first wave of enterprise agent frameworks — tools like the early versions of CrewAI and the initial releases of LangGraph — were defined by a tension between autonomy and control. Early models simply were not reliable enough to be trusted with open-ended decision-making. The result was what practitioners began calling \"agents on rails\": tightly constrained workflows where every decision point, every tool call, and every branching path had to be pre-defined by a human developer.This approach worked, but it was limited. Building an agent on rails meant anticipating every possible state the system might encounter — a combinatorial nightmare for anything beyond simple, linear tasks. Worse, it meant that agents could not adapt to novel situations, the very capability that makes agentic AI valuable in the first place.The Gemini 3 series, along with recent releases like Anthropic&#x27;s Claude Opus 4.6 and Sonnet 4.6, represents a threshold where models have become reliable enough at planning, reasoning, and self-correction that the rails can start coming off. Google&#x27;s own Opal update is an acknowledgment of this shift. The new agent step does not require builders to pre-define every path through a workflow. Instead, it trusts the underlying model to evaluate the user&#x27;s goal, assess available tools, and determine the optimal sequence of actions dynamically.This is the same pattern that made Claude Code&#x27;s agentic workflows and tool calling viable: the models are good enough to decide the agent’s next step and often even to self-correct without a human manually re-prompting every error. The difference compared to Claude Code is that Google is now packaging this capability into a consumer-grade, no-code product — a strong signal that the underlying technology has matured past the experimental phase.For enterprise teams, the implication is direct: if you are still designing agent architectures that require pre-defined paths for every contingency, you are likely over-engineering. The new generation of models supports a design pattern where you define goals and constraints, provide tools, and let the model handle routing — a shift from programming agents to managing them.Memory across sessions: The feature that separates demos from production agentsThe second major addition in the Opal update is persistent memory. Google now allows Opals to remember information across sessions — user preferences, prior interactions, accumulated context — making agents that improve with use rather than starting from zero each time.Google has not disclosed the technical implementation behind Opal&#x27;s memory system. But the pattern itself is well-established in the agent-building community. Tools like OpenClaw handle memory primarily through markdown and JSON files, a simple approach that works well for single-user systems. Enterprise deployments face a harder problem: maintaining memory across multiple users, sessions, and security boundaries without leaking sensitive context between them.This single-user versus multi-user memory divide is one of the most under-discussed challenges in enterprise agent deployment. A personal coding assistant that remembers your project structure is fundamentally different from a customer-facing agent that must maintain separate memory states for thousands of concurrent users while complying with data retention policies.What the Opal update signals is that Google considers memory a core feature of agent architecture, not an optional add-on. For IT decision-makers evaluating agent platforms, this should inform procurement criteria. An agent framework without a clear memory strategy is a framework that will produce impressive demos but struggle in production, where the value of an agent compounds over repeated interactions with the same users and datasets.Human-in-the-loop is not a fallback — it is a design patternThe third pillar of the Opal update is what Google calls \"interactive chat\" — the ability for an agent to pause execution, ask the user a follow-up question, gather missing information, or present choices before proceeding. In agent architecture terminology, this is human-in-the-loop orchestration, and its inclusion in a consumer product is telling.The most effective agents in production today are not fully autonomous. They are systems that know when they have reached the limits of their confidence and can gracefully hand control back to a human. This is the pattern that separates reliable enterprise agents from the kind of runaway autonomous systems that have generated cautionary tales across the industry.In frameworks like LangGraph, human-in-the-loop has traditionally been implemented as an explicit node in the graph — a hard-coded checkpoint where execution pauses for human review. Opal&#x27;s approach is more fluid: the agent itself decides when it needs human input based on the quality and completeness of the information it has. This is a more natural interaction pattern and one that scales better, because it does not require the builder to predict in advance exactly where human intervention will be needed.For enterprise architects, the lesson is that human-in-the-loop should not just be treated as a safety net bolted on after the agent is built. It should be a first-class capability of the agent framework itself — one that the model can invoke dynamically based on its own assessment of uncertainty.Dynamic routing: Letting the model decide the pathThe final significant feature is dynamic routing, where builders can define multiple paths through a workflow and let the agent select the appropriate one based on custom criteria. Google&#x27;s example is an executive briefing agent that takes different paths depending on whether the user is meeting with a new or existing client — searching the web for background information in one case, reviewing internal meeting notes in the other.This is conceptually similar to the conditional branching that LangGraph and similar frameworks have supported for some time. But Opal&#x27;s implementation lowers the barrier dramatically by allowing builders to describe routing criteria in natural language rather than code. The model interprets the criteria and makes the routing decision, rather than requiring a developer to write explicit conditional logic.The enterprise implication is significant. Dynamic routing powered by natural language criteria means that business analysts and domain experts — not just developers — can define complex agent behaviors. This shifts agent development from a purely engineering discipline to one where domain knowledge becomes the primary bottleneck, a change that could dramatically accelerate adoption across non-technical business units.What Google is really building: An agent intelligence layerStepping back from individual features, the broader pattern in the Opal update is that Google is building an intelligence layer that sits between the user&#x27;s intent and the execution of complex, multi-step tasks. Building on lessons from an internal agent SDK called “Breadboard”, the agent step is not just another node in a workflow — it is an orchestration layer that can recruit models, invoke tools, manage memory, route dynamically, and interact with humans, all driven by the ever improving reasoning capabilities of the underlying Gemini models.This is the same architectural pattern emerging across the industry. Anthropic&#x27;s Claude Code, with its ability to autonomously manage coding tasks overnight, relies on similar principles: a capable model, access to tools, persistent context, and feedback loops that allow self-correction. The Ralph Wiggum plugin formalized the insight that models can be pressed through their own failures to arrive at correct solutions — a brute-force version of the self-correction that Opal now packages some of that into a polished consumer experience.For enterprise teams, the takeaway is that agent architecture is converging on a common set of primitives: goal-directed planning, tool use, persistent memory, dynamic routing, and human-in-the-loop orchestration. The differentiator will not be which primitives you implement, but how well you integrate them — and how effectively you leverage the improving capabilities of frontier models to reduce the amount of manual configuration required.The practical playbook for enterprise agent buildersGoogle shipping these capabilities in a free, consumer-facing product sends a clear message: the foundational patterns for building effective AI agents are no longer cutting-edge research. They are productized. Enterprise teams that have been waiting for the technology to mature now have a reference implementation they can study, test, and learn from — at zero cost.The practical steps are straightforward. First, evaluate whether your current agent architectures are over-constrained. If every decision point requires hard-coded logic, you are likely not leveraging the planning capabilities of current frontier models. Second, prioritize memory as a core architectural component, not an afterthought. Third, design human-in-the-loop as a dynamic capability the agent can invoke, rather than a fixed checkpoint in a workflow. And fourth, explore natural language routing as a way to bring domain experts into the agent design process.Opal itself probably won’t become the platform enterprises adopt. But the design patterns it embodies — adaptive, memory-rich, human-aware agents powered by frontier models — are the patterns that will define the next generation of enterprise AI. Google has shown its hand. The question for IT leaders is whether they are paying attention.",
          "content": "For the past year, the enterprise AI community has been locked in a debate about how much freedom to give AI agents. Too little, and you get expensive workflow automation that barely justifies the \"agent\" label. Too much, and you get the kind of data-wiping disasters that plagued early adopters of tools like OpenClaw. This week, Google Labs released an update to Opal, its no-code visual agent builder, that quietly lands on an answer — and it carries lessons that every IT leader planning an agent strategy should study carefully.The update introduces what Google calls an \"agent step\" that transforms Opal&#x27;s previously static, drag-and-drop workflows into dynamic, interactive experiences. Instead of manually specifying which model or tool to call and in what order, builders can now define a goal and let the agent determine the best path to reach it — selecting tools, triggering models like Gemini 3 Flash or Veo for video generation, and even initiating conversations with users when it needs more information.It sounds like a modest product update. It is not. What Google has shipped is a working reference architecture for the three capabilities that will define enterprise agents in 2026:Adaptive routingPersistent memoryHuman-in-the-loop orchestration...and it&#x27;s all made possible by the rapidly improving reasoning abilities of frontier models like the Gemini 3 series.The &#x27;off the rails&#x27; inflection point: Why better models change everything about agent designTo understand why the Opal update matters, you need to understand a shift that has been building across the agent ecosystem for months.The first wave of enterprise agent frameworks — tools like the early versions of CrewAI and the initial releases of LangGraph — were defined by a tension between autonomy and control. Early models simply were not reliable enough to be trusted with open-ended decision-making. The result was what practitioners began calling \"agents on rails\": tightly constrained workflows where every decision point, every tool call, and every branching path had to be pre-defined by a human developer.This approach worked, but it was limited. Building an agent on rails meant anticipating every possible state the system might encounter — a combinatorial nightmare for anything beyond simple, linear tasks. Worse, it meant that agents could not adapt to novel situations, the very capability that makes agentic AI valuable in the first place.The Gemini 3 series, along with recent releases like Anthropic&#x27;s Claude Opus 4.6 and Sonnet 4.6, represents a threshold where models have become reliable enough at planning, reasoning, and self-correction that the rails can start coming off. Google&#x27;s own Opal update is an acknowledgment of this shift. The new agent step does not require builders to pre-define every path through a workflow. Instead, it trusts the underlying model to evaluate the user&#x27;s goal, assess available tools, and determine the optimal sequence of actions dynamically.This is the same pattern that made Claude Code&#x27;s agentic workflows and tool calling viable: the models are good enough to decide the agent’s next step and often even to self-correct without a human manually re-prompting every error. The difference compared to Claude Code is that Google is now packaging this capability into a consumer-grade, no-code product — a strong signal that the underlying technology has matured past the experimental phase.For enterprise teams, the implication is direct: if you are still designing agent architectures that require pre-defined paths for every contingency, you are likely over-engineering. The new generation of models supports a design pattern where you define goals and constraints, provide tools, and let the model handle routing — a shift from programming agents to managing them.Memory across sessions: The feature that separates demos from production agentsThe second major addition in the Opal update is persistent memory. Google now allows Opals to remember information across sessions — user preferences, prior interactions, accumulated context — making agents that improve with use rather than starting from zero each time.Google has not disclosed the technical implementation behind Opal&#x27;s memory system. But the pattern itself is well-established in the agent-building community. Tools like OpenClaw handle memory primarily through markdown and JSON files, a simple approach that works well for single-user systems. Enterprise deployments face a harder problem: maintaining memory across multiple users, sessions, and security boundaries without leaking sensitive context between them.This single-user versus multi-user memory divide is one of the most under-discussed challenges in enterprise agent deployment. A personal coding assistant that remembers your project structure is fundamentally different from a customer-facing agent that must maintain separate memory states for thousands of concurrent users while complying with data retention policies.What the Opal update signals is that Google considers memory a core feature of agent architecture, not an optional add-on. For IT decision-makers evaluating agent platforms, this should inform procurement criteria. An agent framework without a clear memory strategy is a framework that will produce impressive demos but struggle in production, where the value of an agent compounds over repeated interactions with the same users and datasets.Human-in-the-loop is not a fallback — it is a design patternThe third pillar of the Opal update is what Google calls \"interactive chat\" — the ability for an agent to pause execution, ask the user a follow-up question, gather missing information, or present choices before proceeding. In agent architecture terminology, this is human-in-the-loop orchestration, and its inclusion in a consumer product is telling.The most effective agents in production today are not fully autonomous. They are systems that know when they have reached the limits of their confidence and can gracefully hand control back to a human. This is the pattern that separates reliable enterprise agents from the kind of runaway autonomous systems that have generated cautionary tales across the industry.In frameworks like LangGraph, human-in-the-loop has traditionally been implemented as an explicit node in the graph — a hard-coded checkpoint where execution pauses for human review. Opal&#x27;s approach is more fluid: the agent itself decides when it needs human input based on the quality and completeness of the information it has. This is a more natural interaction pattern and one that scales better, because it does not require the builder to predict in advance exactly where human intervention will be needed.For enterprise architects, the lesson is that human-in-the-loop should not just be treated as a safety net bolted on after the agent is built. It should be a first-class capability of the agent framework itself — one that the model can invoke dynamically based on its own assessment of uncertainty.Dynamic routing: Letting the model decide the pathThe final significant feature is dynamic routing, where builders can define multiple paths through a workflow and let the agent select the appropriate one based on custom criteria. Google&#x27;s example is an executive briefing agent that takes different paths depending on whether the user is meeting with a new or existing client — searching the web for background information in one case, reviewing internal meeting notes in the other.This is conceptually similar to the conditional branching that LangGraph and similar frameworks have supported for some time. But Opal&#x27;s implementation lowers the barrier dramatically by allowing builders to describe routing criteria in natural language rather than code. The model interprets the criteria and makes the routing decision, rather than requiring a developer to write explicit conditional logic.The enterprise implication is significant. Dynamic routing powered by natural language criteria means that business analysts and domain experts — not just developers — can define complex agent behaviors. This shifts agent development from a purely engineering discipline to one where domain knowledge becomes the primary bottleneck, a change that could dramatically accelerate adoption across non-technical business units.What Google is really building: An agent intelligence layerStepping back from individual features, the broader pattern in the Opal update is that Google is building an intelligence layer that sits between the user&#x27;s intent and the execution of complex, multi-step tasks. Building on lessons from an internal agent SDK called “Breadboard”, the agent step is not just another node in a workflow — it is an orchestration layer that can recruit models, invoke tools, manage memory, route dynamically, and interact with humans, all driven by the ever improving reasoning capabilities of the underlying Gemini models.This is the same architectural pattern emerging across the industry. Anthropic&#x27;s Claude Code, with its ability to autonomously manage coding tasks overnight, relies on similar principles: a capable model, access to tools, persistent context, and feedback loops that allow self-correction. The Ralph Wiggum plugin formalized the insight that models can be pressed through their own failures to arrive at correct solutions — a brute-force version of the self-correction that Opal now packages some of that into a polished consumer experience.For enterprise teams, the takeaway is that agent architecture is converging on a common set of primitives: goal-directed planning, tool use, persistent memory, dynamic routing, and human-in-the-loop orchestration. The differentiator will not be which primitives you implement, but how well you integrate them — and how effectively you leverage the improving capabilities of frontier models to reduce the amount of manual configuration required.The practical playbook for enterprise agent buildersGoogle shipping these capabilities in a free, consumer-facing product sends a clear message: the foundational patterns for building effective AI agents are no longer cutting-edge research. They are productized. Enterprise teams that have been waiting for the technology to mature now have a reference implementation they can study, test, and learn from — at zero cost.The practical steps are straightforward. First, evaluate whether your current agent architectures are over-constrained. If every decision point requires hard-coded logic, you are likely not leveraging the planning capabilities of current frontier models. Second, prioritize memory as a core architectural component, not an afterthought. Third, design human-in-the-loop as a dynamic capability the agent can invoke, rather than a fixed checkpoint in a workflow. And fourth, explore natural language routing as a way to bring domain experts into the agent design process.Opal itself probably won’t become the platform enterprises adopt. But the design patterns it embodies — adaptive, memory-rich, human-aware agents powered by frontier models — are the patterns that will define the next generation of enterprise AI. Google has shown its hand. The question for IT leaders is whether they are paying attention.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5IHY5EKUtp8R9dSKGLvxRT/94a1ffd7e5709a24b1e8229687cf42ac/Gemini_Generated_Image_1r6vew1r6vew1r6v.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/openais-big-investment-from-aws-comes-with-something-else-new-stateful",
          "published_at": "Fri, 27 Feb 2026 23:16:00 GMT",
          "title": "OpenAI's big investment from Amazon comes with something else: new 'stateful' architecture for enterprise agents",
          "standfirst": "The landscape of enterprise artificial intelligence shifted fundamentally today as OpenAI announced $110 billion in new funding from three of tech&#x27;s largest firms: $30 billion from SoftBank, $30 billion from Nvidia, and $50 billion from Amazon.But while the former two players are providing money, OpenAI is going further with Amazon in a new direction, establishing an upcoming fully \"Stateful Runtime Environment\" on Amazon Web Services (AWS), the world&#x27;s most used cloud environment.This signals OpenAI&#x27;s and Amazon&#x27;s vision of the next phase of the AI economy — moving from chatbots to autonomous \"AI coworkers\" known as agents — and that this evolution requires a different architectural foundation than the one that built GPT-4. For enterprise decision-makers, this announcement isn’t just a headline about massive capital; it is a technical roadmap for where the next generation of agentic intelligence will live and breathe.And especially for those enterprises currently using AWS, it&#x27;s great news, giving them more options with a new runtime environment from OpenAI coming soon (the companies have yet to announce a precise timeline for when it will arrive).The great divide between &#x27;stateless&#x27; and &#x27;stateful&#x27;At the heart of the new OpenAI-Amazon partnership is a technical distinction that will define developer workflows for the next decade: the difference between \"stateless\" and \"stateful\" environments.To date, most developers have interacted with OpenAI through stateless APIs. In a stateless model, every request is an isolated event; the model has no \"memory\" of previous interactions unless the developer manually feeds the entire conversation history back into the prompt. OpenAI&#x27;s prior cloud partner and major investor, Microsoft Azure, remains the exclusive third-party cloud provider for these stateless APIs.The newly announced Stateful Runtime Environment, by contrast, will be hosted on Amazon Bedrock — a paradigm shift. This environment allows models to maintain persistent context, memory, and identity. Rather than a series of disconnected calls, the stateful environment enables \"AI coworkers\" to handle ongoing projects, remember prior work, and move seamlessly across different software tools and data sources. As OpenAI notes on its website: \"Now, instead of manually stitching together disconnected requests to make things work, your agents automatically execute complex steps with &#x27;working context&#x27; that carries forward memory/history, tool and workflow state, environment use, and identity/permission boundaries.\"For builders of complex agents, this reduces the \"plumbing\" required to maintain context, as the infrastructure itself now handles the persistent state of the agent.OpenAI Frontier and the AWS IntegrationThe vehicle for this stateful intelligence is OpenAI Frontier, an end-to-end platform designed to help enterprises build, deploy, and manage teams of AI agents, launched back in early February 2026. Frontier is positioned as a solution to the \"AI opportunity gap\"—the disconnect between model capabilities and the ability of a business to actually put them into production.Key features of the Frontier platform include:Shared Business Context: Connecting siloed data from CRMs, ticketing tools, and internal databases into a single semantic layer.Agent Execution Environment: A dependable space where agents can run code, use computer tools, and solve real-world problems. Built-in Governance: Every AI agent has a unique identity with explicit permissions and boundaries, allowing for use in regulated environments.While the Frontier application itself will continue to be hosted on Microsoft Azure, AWS has been named the exclusive third-party cloud distribution provider for the platform. This means that while the \"engine\" may sit on Azure, AWS customers will be able to access and manage these agentic workloads directly through Amazon Bedrock, integrated with AWS’s existing infrastructure services.OpenAI opens the door to enterprises: how to register your interest in its upcoming new Stateful Runtime Environment on AWSFor now, OpenAI has launched a dedicated Enterprise Interest Portal on its website. This serves as the primary intake point for organizations looking to move past isolated pilots and into production-grade agentic workflows.The portal is a structured \"request for access\" form where decision-makers provide:Firmographic Data: Basic details including company size (ranging from startups of 1–50 to large-scale enterprises with 20,000+ employees) and contact information.Business Needs Assessment: A dedicated field for leadership to outline specific business challenges and requirements for \"AI coworkers\".By submitting this form, enterprises signal their readiness to work directly with OpenAI and AWS teams to implement solutions like multi-system customer support, sales operations, and finance audits that require high-reliability state management.Community and leadership reactionsThe scale of the announcement was mirrored in the public statements from the key players on social media.Sam Altman, CEO of OpenAI, expressed excitement about the Amazon partnership, specifically highlighting the \"stateful runtime environment\" and the use of Amazon&#x27;s custom Trainium chips. However, Altman was quick to clarify the boundaries of the deal: \"Our stateless API will remain exclusive to Azure, and we will build out much more capacity with them\".Amazon CEO Andy Jassy emphasized the demand from his own customer base, stating, \"We have lots of developers and companies eager to run services powered by OpenAI models on AWS\". He noted that the collaboration would \"change what’s possible for customers building AI apps and agents\".Early adopters have already begun to weigh in on the utility of the Frontier approach. Joe Park, EVP at State Farm, noted that the platform is helping the company accelerate its AI capabilities to \"help millions plan ahead, protect what matters most, and recover faster\".The enterprise decision: where to spend your dollars?For CTOs and enterprise decision-makers, the OpenAI-Amazon-Microsoft triangle creates a new set of strategic choices. The decision of where to allocate budget now depends heavily on the specific use case:For High-Volume, Standard Tasks: If your organization relies on standard API calls for content generation, summarization, or simple chat, Microsoft Azure remains the primary destination. These \"stateless\" calls are exclusive to Azure, even if they originate from an Amazon-linked collaboration.For Complex, Long-Running Agents: If your goal is to build \"AI coworkers\" that require deep integration with AWS-hosted data and persistent memory across weeks of work, the AWS Stateful Runtime Environment is the clear choice.For Custom Infrastructure: OpenAI has committed to consuming 2 gigawatts of AWS Trainium capacity to power Frontier and other advanced workloads. This suggests that enterprises looking for the most cost-efficient way to run OpenAI models at massive scale may find an advantage in the AWS-Trainium ecosystem.Licensing, revenue and the Microsoft &#x27;safety net&#x27;Despite the massive infusion of Amazon capital, the legal and financial ties between Microsoft and OpenAI remain remarkably rigid. A joint statement released by both companies clarified that their \"commercial and revenue share relationship remains unchanged\".Crucially, Microsoft continues to maintain its \"exclusive license and access to intellectual property across OpenAI models and products\". Furthermore, Microsoft will receive a share of the revenue generated by the OpenAI-Amazon partnership. This ensures that while OpenAI is diversifying its infrastructure, Microsoft remains the ultimate beneficiary of OpenAI’s commercial success, regardless of which cloud the compute actually runs on.The definition of Artificial General Intelligence (AGI) also remains a protected term in the Microsoft agreement. The contractual processes for determining when AGI has been reached—and the subsequent impact on commercial licensing—have not been altered by the Amazon deal.Ultimately, OpenAI is positioning itself as more than a model or tool provider; it is an infrastructure player attempting to straddle the two largest clouds on Earth. For the user, this means more choice and more specialized environments. For the enterprise, it means that the era of \"one-size-fits-all\" AI procurement is over. The choice between Azure and AWS for OpenAI services is now a technical decision about the nature of the work itself: whether your AI needs to simply \"think\" (stateless) or to \"remember and act\" (stateful).",
          "content": "The landscape of enterprise artificial intelligence shifted fundamentally today as OpenAI announced $110 billion in new funding from three of tech&#x27;s largest firms: $30 billion from SoftBank, $30 billion from Nvidia, and $50 billion from Amazon.But while the former two players are providing money, OpenAI is going further with Amazon in a new direction, establishing an upcoming fully \"Stateful Runtime Environment\" on Amazon Web Services (AWS), the world&#x27;s most used cloud environment.This signals OpenAI&#x27;s and Amazon&#x27;s vision of the next phase of the AI economy — moving from chatbots to autonomous \"AI coworkers\" known as agents — and that this evolution requires a different architectural foundation than the one that built GPT-4. For enterprise decision-makers, this announcement isn’t just a headline about massive capital; it is a technical roadmap for where the next generation of agentic intelligence will live and breathe.And especially for those enterprises currently using AWS, it&#x27;s great news, giving them more options with a new runtime environment from OpenAI coming soon (the companies have yet to announce a precise timeline for when it will arrive).The great divide between &#x27;stateless&#x27; and &#x27;stateful&#x27;At the heart of the new OpenAI-Amazon partnership is a technical distinction that will define developer workflows for the next decade: the difference between \"stateless\" and \"stateful\" environments.To date, most developers have interacted with OpenAI through stateless APIs. In a stateless model, every request is an isolated event; the model has no \"memory\" of previous interactions unless the developer manually feeds the entire conversation history back into the prompt. OpenAI&#x27;s prior cloud partner and major investor, Microsoft Azure, remains the exclusive third-party cloud provider for these stateless APIs.The newly announced Stateful Runtime Environment, by contrast, will be hosted on Amazon Bedrock — a paradigm shift. This environment allows models to maintain persistent context, memory, and identity. Rather than a series of disconnected calls, the stateful environment enables \"AI coworkers\" to handle ongoing projects, remember prior work, and move seamlessly across different software tools and data sources. As OpenAI notes on its website: \"Now, instead of manually stitching together disconnected requests to make things work, your agents automatically execute complex steps with &#x27;working context&#x27; that carries forward memory/history, tool and workflow state, environment use, and identity/permission boundaries.\"For builders of complex agents, this reduces the \"plumbing\" required to maintain context, as the infrastructure itself now handles the persistent state of the agent.OpenAI Frontier and the AWS IntegrationThe vehicle for this stateful intelligence is OpenAI Frontier, an end-to-end platform designed to help enterprises build, deploy, and manage teams of AI agents, launched back in early February 2026. Frontier is positioned as a solution to the \"AI opportunity gap\"—the disconnect between model capabilities and the ability of a business to actually put them into production.Key features of the Frontier platform include:Shared Business Context: Connecting siloed data from CRMs, ticketing tools, and internal databases into a single semantic layer.Agent Execution Environment: A dependable space where agents can run code, use computer tools, and solve real-world problems. Built-in Governance: Every AI agent has a unique identity with explicit permissions and boundaries, allowing for use in regulated environments.While the Frontier application itself will continue to be hosted on Microsoft Azure, AWS has been named the exclusive third-party cloud distribution provider for the platform. This means that while the \"engine\" may sit on Azure, AWS customers will be able to access and manage these agentic workloads directly through Amazon Bedrock, integrated with AWS’s existing infrastructure services.OpenAI opens the door to enterprises: how to register your interest in its upcoming new Stateful Runtime Environment on AWSFor now, OpenAI has launched a dedicated Enterprise Interest Portal on its website. This serves as the primary intake point for organizations looking to move past isolated pilots and into production-grade agentic workflows.The portal is a structured \"request for access\" form where decision-makers provide:Firmographic Data: Basic details including company size (ranging from startups of 1–50 to large-scale enterprises with 20,000+ employees) and contact information.Business Needs Assessment: A dedicated field for leadership to outline specific business challenges and requirements for \"AI coworkers\".By submitting this form, enterprises signal their readiness to work directly with OpenAI and AWS teams to implement solutions like multi-system customer support, sales operations, and finance audits that require high-reliability state management.Community and leadership reactionsThe scale of the announcement was mirrored in the public statements from the key players on social media.Sam Altman, CEO of OpenAI, expressed excitement about the Amazon partnership, specifically highlighting the \"stateful runtime environment\" and the use of Amazon&#x27;s custom Trainium chips. However, Altman was quick to clarify the boundaries of the deal: \"Our stateless API will remain exclusive to Azure, and we will build out much more capacity with them\".Amazon CEO Andy Jassy emphasized the demand from his own customer base, stating, \"We have lots of developers and companies eager to run services powered by OpenAI models on AWS\". He noted that the collaboration would \"change what’s possible for customers building AI apps and agents\".Early adopters have already begun to weigh in on the utility of the Frontier approach. Joe Park, EVP at State Farm, noted that the platform is helping the company accelerate its AI capabilities to \"help millions plan ahead, protect what matters most, and recover faster\".The enterprise decision: where to spend your dollars?For CTOs and enterprise decision-makers, the OpenAI-Amazon-Microsoft triangle creates a new set of strategic choices. The decision of where to allocate budget now depends heavily on the specific use case:For High-Volume, Standard Tasks: If your organization relies on standard API calls for content generation, summarization, or simple chat, Microsoft Azure remains the primary destination. These \"stateless\" calls are exclusive to Azure, even if they originate from an Amazon-linked collaboration.For Complex, Long-Running Agents: If your goal is to build \"AI coworkers\" that require deep integration with AWS-hosted data and persistent memory across weeks of work, the AWS Stateful Runtime Environment is the clear choice.For Custom Infrastructure: OpenAI has committed to consuming 2 gigawatts of AWS Trainium capacity to power Frontier and other advanced workloads. This suggests that enterprises looking for the most cost-efficient way to run OpenAI models at massive scale may find an advantage in the AWS-Trainium ecosystem.Licensing, revenue and the Microsoft &#x27;safety net&#x27;Despite the massive infusion of Amazon capital, the legal and financial ties between Microsoft and OpenAI remain remarkably rigid. A joint statement released by both companies clarified that their \"commercial and revenue share relationship remains unchanged\".Crucially, Microsoft continues to maintain its \"exclusive license and access to intellectual property across OpenAI models and products\". Furthermore, Microsoft will receive a share of the revenue generated by the OpenAI-Amazon partnership. This ensures that while OpenAI is diversifying its infrastructure, Microsoft remains the ultimate beneficiary of OpenAI’s commercial success, regardless of which cloud the compute actually runs on.The definition of Artificial General Intelligence (AGI) also remains a protected term in the Microsoft agreement. The contractual processes for determining when AGI has been reached—and the subsequent impact on commercial licensing—have not been altered by the Amazon deal.Ultimately, OpenAI is positioning itself as more than a model or tool provider; it is an infrastructure player attempting to straddle the two largest clouds on Earth. For the user, this means more choice and more specialized environments. For the enterprise, it means that the era of \"one-size-fits-all\" AI procurement is over. The choice between Azure and AWS for OpenAI services is now a technical decision about the nature of the work itself: whether your AI needs to simply \"think\" (stateless) or to \"remember and act\" (stateful).",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7bL3HaJIq1X5vb82aJqp93/0de52a58f30cce531f66f30ead20337f/Gemini_Generated_Image_nxfyvknxfyvknxfy.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/google-and-openai-employees-sign-open-letter-in-solidarity-with-anthropic-194957274.html",
          "published_at": "Fri, 27 Feb 2026 19:49:57 +0000",
          "title": "Google and OpenAI employees sign open letter in ‘solidarity’ with Anthropic",
          "standfirst": "Hundreds of employees at Google and OpenAI have signed an open letter urging their companies to stand with Anthropic in its standoff with the Pentagon over military applications for AI tools like Claude. The letter, titled “We Will Not Be Divided,” calls on the leadership of both companies to “put aside their differences and stand together to continue to refuse the Department of War’s current demands for permission to use our models for domestic mass surveillance and autonomously killing people without human oversight.” These are two lines that Anthropic CEO Dario Amodei has said should not be crossed by his or any other AI company. As of publication, the letter has over 450 signatures, almost 400 of which come from Google employees and the rest from OpenAI. Currently, roughly 50 percent of all participants have chosen to attach their names to the cause, with the rest remaining anonymous. All are verified as current employees of these companies. The original organizers of the letter aren’t Google or OpenAI employees; they say are unaffiliated with any AI company, political party or advocacy group. The open letter is the latest development in the saga between Anthropic and US Defense Secretary Pete Hegseth, who threatened to label the company a “supply chain risk” if it did not agree to withdraw certain guardrails for classified work. The Pentagon has also been in talks with Google and OpenAI about using their models for classified work, with xAI coming on board earlier this week. The letter argues the government is \"trying to divide each company with fear that the other will give in.” OpenAI CEO Sam Altman told his employees on Friday that the ChatGPT maker will draw the same red lines as Anthropic, according to an internal memo seen by Axios. He told CNBC on the same day that he doesn't \"personally think the Pentagon should be threatening DPA against these companies.\"This article originally appeared on Engadget at https://www.engadget.com/ai/google-and-openai-employees-sign-open-letter-in-solidarity-with-anthropic-194957274.html?src=rss",
          "content": "Hundreds of employees at Google and OpenAI have signed an open letter urging their companies to stand with Anthropic in its standoff with the Pentagon over military applications for AI tools like Claude. The letter, titled “We Will Not Be Divided,” calls on the leadership of both companies to “put aside their differences and stand together to continue to refuse the Department of War’s current demands for permission to use our models for domestic mass surveillance and autonomously killing people without human oversight.” These are two lines that Anthropic CEO Dario Amodei has said should not be crossed by his or any other AI company. As of publication, the letter has over 450 signatures, almost 400 of which come from Google employees and the rest from OpenAI. Currently, roughly 50 percent of all participants have chosen to attach their names to the cause, with the rest remaining anonymous. All are verified as current employees of these companies. The original organizers of the letter aren’t Google or OpenAI employees; they say are unaffiliated with any AI company, political party or advocacy group. The open letter is the latest development in the saga between Anthropic and US Defense Secretary Pete Hegseth, who threatened to label the company a “supply chain risk” if it did not agree to withdraw certain guardrails for classified work. The Pentagon has also been in talks with Google and OpenAI about using their models for classified work, with xAI coming on board earlier this week. The letter argues the government is \"trying to divide each company with fear that the other will give in.” OpenAI CEO Sam Altman told his employees on Friday that the ChatGPT maker will draw the same red lines as Anthropic, according to an internal memo seen by Axios. He told CNBC on the same day that he doesn't \"personally think the Pentagon should be threatening DPA against these companies.\"This article originally appeared on Engadget at https://www.engadget.com/ai/google-and-openai-employees-sign-open-letter-in-solidarity-with-anthropic-194957274.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/enterprise-mcp-adoption-is-outpacing-security-controls",
          "published_at": "Fri, 27 Feb 2026 19:00:00 GMT",
          "title": "Enterprise MCP adoption is outpacing security controls",
          "standfirst": "AI agents now carry more access and more connections to enterprise systems than any other software in the environment. That makes them a bigger attack surface than anything security teams have had to govern before, and the industry doesn&#x27;t yet have a framework for it. \"If that attack vector gets utilized, it can result in a data breach, or even worse,\" said Spiros Xanthos, founder and CEO of Resolve AI, speaking at a recent VentureBeat AI Impact Series event. Traditional security frameworks are built around human interactions. There&#x27;s not yet an agreed-upon construct for AI agents that have personas and can work autonomously, noted Jon Aniano, SVP of product and CRM applications at Zendesk, at the same event. Agentic AI is moving faster than enterprises can build guardrails — and Model Context Protocol (MCP), while decreasing integration complexity, is making the problem worse. “Right now it&#x27;s an unsolved problem because it&#x27;s the wild, wild West,” Aniano said. “We don&#x27;t even have a defined technical agent-to-agent protocol that all companies agree on. How do you balance user expectations versus what keeps your platform safe?”MCP still \"extremely permissive\" Enterprises are increasingly hooking into MCP servers because they simplify integration between agents, tools and data. However, MCP servers tend to be “extremely permissive,” he said. They are “actually probably worse than an API,” he contended, because APIs at least have more controls in place to impose upon agents. Today&#x27;s agents are acting on behalf of humans based on explicit permissions, thus establishing human accountability. \"But you might have tens, hundreds of agents in the future with their own identity, their own access,\" said Xanthos. \"It becomes a very complex matrix.\" Even as his startup is developing autonomous AI agents for site reliability engineering (SRE) and system management, he acknowledged that the industry “completely lacks the framework” for autonomous agents. “It&#x27;s completely on us and to anybody who builds agents to figure out what restrictions to give them,” he said. And customers must be able to trust those decisions. Some existing security tools do offer fine-grained access — Splunk, for instance, developed a method to provide access to certain indexes in underlying data stores, he noted — but most are broader and human-oriented. “We&#x27;re trying to figure this out with existing tools,” he said. \"But I don&#x27;t think they&#x27;re sufficient for the era of agents.” Who&#x27;s accountable when an AI mis-authenticates a user?At Zendesk and other customer relationship management (CRM) platform providers, AI is involved in a number of user interactions, Aniano noted — in fact, now it’s at a “volume and a scale that we haven&#x27;t contemplated as businesses and as a society.” It can get tricky when AI is helping out human agents; the audit trail can become a labyrinth. “So now you&#x27;ve got a human talking to a human that&#x27;s talking to an AI,” Aniano noted. “The human tells the AI to take action. Who&#x27;s at fault if it&#x27;s the wrong action?” This becomes even more complicated when there are “multiple pieces of AI and multiple humans\" in the mix. To prevent agents from going off the rails, Zendesk tends to be “very strict” about access and scope; however, customers can define their own guardrails based on their needs. In most cases, AI can access knowledge sources, but they’re not writing code or running commands on servers, Aniano said. If an AI does call an API, it is “declaratively designed” and sanctioned, and actions are specifically called out. However, customer demand is flooding these scenarios and “we&#x27;re kind of holding the gates right now,” he said. The industry must develop concrete standards for agent interactions. “We&#x27;re entering a world where, with things like MCP that can auto-discover tools, we&#x27;re going to have to create new methods of safety for deciding what tools these bots can interact with,” said Aniano. When it comes to security, enterprises are rightly concerned when AI takes over authentication tasks, such as sending out and processing one-time passwords (OTP), SMS codes, or other two-step verification methods, he said. What happens if an AI mis-authenticates or misidentifies someone? This can lead to sensitive data leakage or open the door for attackers. “There&#x27;s a spectrum now, and the end of that spectrum today is a human,” Aniano said. However, “the end of that spectrum tomorrow might be a specialized agent designed to do the same kind of gut feeling or human-level interaction.” Customers themselves are on a spectrum of adoption and comfort. In certain companies — particularly financial services or other highly-regulated environments — humans still must be involved in authentication, Aniano noted. In other cases, legacy companies or old guards only trust humans to authenticate other humans. He noted that Zendesk is experimenting with new AI agents that are “a little more connected to systems,” and working with a select group of customers around guardrailing. Standing authorization is comingIn some future, agents may actually be more trusted than humans to do some tasks, and granted permissions “way beyond” what humans have today, Xanthos said. But we’re a long way from that, and, for the most part, the fear of something going wrong is what’s holding enterprises back. “Which is a good fear, right? I&#x27;m not saying that it is a bad thing,” he said. Many enterprises simply aren&#x27;t yet comfortable with an agent doing all steps of a workflow or fully closing the loop by itself. They still want human review. Resolve AI is on the cusp of giving agents standing authorization in a few cases that are “generally safe,” such as in coding; from there they’ll move to more open-ended scenarios that are not all that risky, Xanthos explained. But he acknowledged that there will always be very risky situations where AI mistakes could “mutate the state of the production system,” as he put it. Ultimately, though: “There&#x27;s no going back, obviously; this is moving faster than maybe even mobile did. So the question is what do we do about it?”What security teams can do nowBoth speakers pointed to interim measures available within existing tooling. Xanthos noted that some tools — Splunk among them — already offer fine-grained index-level access controls that can be applied to agents. Aniano described Zendesk&#x27;s approach as a practical starting point: declaratively designed API calls with explicitly sanctioned actions, strict access and scope limits, and human review before expanding agent permissions. The underlying principle, as Aniano put it: \"We&#x27;re always checking those gates and seeing how we can widen the aperture\" — meaning don&#x27;t grant standing authorization until you&#x27;ve validated each expansion.",
          "content": "AI agents now carry more access and more connections to enterprise systems than any other software in the environment. That makes them a bigger attack surface than anything security teams have had to govern before, and the industry doesn&#x27;t yet have a framework for it. \"If that attack vector gets utilized, it can result in a data breach, or even worse,\" said Spiros Xanthos, founder and CEO of Resolve AI, speaking at a recent VentureBeat AI Impact Series event. Traditional security frameworks are built around human interactions. There&#x27;s not yet an agreed-upon construct for AI agents that have personas and can work autonomously, noted Jon Aniano, SVP of product and CRM applications at Zendesk, at the same event. Agentic AI is moving faster than enterprises can build guardrails — and Model Context Protocol (MCP), while decreasing integration complexity, is making the problem worse. “Right now it&#x27;s an unsolved problem because it&#x27;s the wild, wild West,” Aniano said. “We don&#x27;t even have a defined technical agent-to-agent protocol that all companies agree on. How do you balance user expectations versus what keeps your platform safe?”MCP still \"extremely permissive\" Enterprises are increasingly hooking into MCP servers because they simplify integration between agents, tools and data. However, MCP servers tend to be “extremely permissive,” he said. They are “actually probably worse than an API,” he contended, because APIs at least have more controls in place to impose upon agents. Today&#x27;s agents are acting on behalf of humans based on explicit permissions, thus establishing human accountability. \"But you might have tens, hundreds of agents in the future with their own identity, their own access,\" said Xanthos. \"It becomes a very complex matrix.\" Even as his startup is developing autonomous AI agents for site reliability engineering (SRE) and system management, he acknowledged that the industry “completely lacks the framework” for autonomous agents. “It&#x27;s completely on us and to anybody who builds agents to figure out what restrictions to give them,” he said. And customers must be able to trust those decisions. Some existing security tools do offer fine-grained access — Splunk, for instance, developed a method to provide access to certain indexes in underlying data stores, he noted — but most are broader and human-oriented. “We&#x27;re trying to figure this out with existing tools,” he said. \"But I don&#x27;t think they&#x27;re sufficient for the era of agents.” Who&#x27;s accountable when an AI mis-authenticates a user?At Zendesk and other customer relationship management (CRM) platform providers, AI is involved in a number of user interactions, Aniano noted — in fact, now it’s at a “volume and a scale that we haven&#x27;t contemplated as businesses and as a society.” It can get tricky when AI is helping out human agents; the audit trail can become a labyrinth. “So now you&#x27;ve got a human talking to a human that&#x27;s talking to an AI,” Aniano noted. “The human tells the AI to take action. Who&#x27;s at fault if it&#x27;s the wrong action?” This becomes even more complicated when there are “multiple pieces of AI and multiple humans\" in the mix. To prevent agents from going off the rails, Zendesk tends to be “very strict” about access and scope; however, customers can define their own guardrails based on their needs. In most cases, AI can access knowledge sources, but they’re not writing code or running commands on servers, Aniano said. If an AI does call an API, it is “declaratively designed” and sanctioned, and actions are specifically called out. However, customer demand is flooding these scenarios and “we&#x27;re kind of holding the gates right now,” he said. The industry must develop concrete standards for agent interactions. “We&#x27;re entering a world where, with things like MCP that can auto-discover tools, we&#x27;re going to have to create new methods of safety for deciding what tools these bots can interact with,” said Aniano. When it comes to security, enterprises are rightly concerned when AI takes over authentication tasks, such as sending out and processing one-time passwords (OTP), SMS codes, or other two-step verification methods, he said. What happens if an AI mis-authenticates or misidentifies someone? This can lead to sensitive data leakage or open the door for attackers. “There&#x27;s a spectrum now, and the end of that spectrum today is a human,” Aniano said. However, “the end of that spectrum tomorrow might be a specialized agent designed to do the same kind of gut feeling or human-level interaction.” Customers themselves are on a spectrum of adoption and comfort. In certain companies — particularly financial services or other highly-regulated environments — humans still must be involved in authentication, Aniano noted. In other cases, legacy companies or old guards only trust humans to authenticate other humans. He noted that Zendesk is experimenting with new AI agents that are “a little more connected to systems,” and working with a select group of customers around guardrailing. Standing authorization is comingIn some future, agents may actually be more trusted than humans to do some tasks, and granted permissions “way beyond” what humans have today, Xanthos said. But we’re a long way from that, and, for the most part, the fear of something going wrong is what’s holding enterprises back. “Which is a good fear, right? I&#x27;m not saying that it is a bad thing,” he said. Many enterprises simply aren&#x27;t yet comfortable with an agent doing all steps of a workflow or fully closing the loop by itself. They still want human review. Resolve AI is on the cusp of giving agents standing authorization in a few cases that are “generally safe,” such as in coding; from there they’ll move to more open-ended scenarios that are not all that risky, Xanthos explained. But he acknowledged that there will always be very risky situations where AI mistakes could “mutate the state of the production system,” as he put it. Ultimately, though: “There&#x27;s no going back, obviously; this is moving faster than maybe even mobile did. So the question is what do we do about it?”What security teams can do nowBoth speakers pointed to interim measures available within existing tooling. Xanthos noted that some tools — Splunk among them — already offer fine-grained index-level access controls that can be applied to agents. Aniano described Zendesk&#x27;s approach as a practical starting point: declaratively designed API calls with explicitly sanctioned actions, strict access and scope limits, and human review before expanding agent permissions. The underlying principle, as Aniano put it: \"We&#x27;re always checking those gates and seeing how we can widen the aperture\" — meaning don&#x27;t grant standing authorization until you&#x27;ve validated each expansion.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2Il8GmCHR0jPAk1Zy5HOpa/3b698463f3151cc8969ea9b9601718ae/1Password.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/openai-secures-another-110-billion-in-funding-from-amazon-nvidia-and-softbank-171006356.html",
          "published_at": "Fri, 27 Feb 2026 17:10:07 +0000",
          "title": "OpenAI secures another $110 billion in funding from Amazon, NVIDIA and SoftBank",
          "standfirst": "OpenAI just announced a massive funding round of $110 billion, which is one of the biggest investment rounds in Silicon Valley history. The investors feature many of the usual suspects, including Amazon with $50 billion, NVIDIA with $30 billion and SoftBank with $30 billion. This investment brings OpenAI to a $730 billion valuation \"We’re super excited about this deal,\" OpenAI CEO Sam Altman told CNBC. \"AI is going to happen everywhere.\" That last statement seems more like a threat than a boast, but I digress. Beyond the funding round, OpenAI has announced strategic partnerships with both NVIDIA and Amazon. This will involve Amazon Web Services (AWS) running OpenAI models for enterprise customers to \"build generative AI applications and agents at production scale.\" It also names AWS as the exclusive third-party cloud distribution provider for OpenAI Frontier, which is an agentic enterprise platform. OpenAI has also committed to consuming 2 gigawatts of Amazon's Trainium capacity, which is the company's custom-designed AI training accelerator. In other words, Amazon is spending a lot of money on OpenAI and then OpenAI will turn around and spend a lot of money with Amazon. The AI funding ouroboros continues. It's also worth noting that Amazon's investment in OpenAI will be staggered. The funding begins with $15 billion, but the remaining $35 billion will only be invested when certain conditions are met. Oddly, it's been reported that one condition is that OpenAI achieves artificial general intelligence. AGI is when AI evolves to or beyond human-level abilities, at which point the entire world turns into rainbows and everyone gets a pony. This could happen later this year, according to those bullish on the technology, or never, according to many researchers. Sam Altman said it was coming in 2025 but has since grown weary of the term. The new partnership with NVIDIA evolves the long-standing collaboration between the two companies. OpenAI has pledged to consume 2 gigawatts of training capacity on NVIDIA's Vera Rubin systems and an additional 3 gigawatts of computing resources, likely in the form of GPUs, to run specific AI inference tasks. In other words, NVIDIA is spending a lot of money on OpenAI and then OpenAI will turn around and spend a lot of money with NVIDIA. The ouroboros must feed. As for revenue, OpenAI has forecast a massive loss of $14 billion in 2026. It lost around $5 billion in 2024 and reports estimate a loss of $8 billion in 2025. Despite this trajectory, the company claims it'll be raking in $100 billion in revenue by 2029.This article originally appeared on Engadget at https://www.engadget.com/ai/openai-secures-another-110-billion-in-funding-from-amazon-nvidia-and-softbank-171006356.html?src=rss",
          "content": "OpenAI just announced a massive funding round of $110 billion, which is one of the biggest investment rounds in Silicon Valley history. The investors feature many of the usual suspects, including Amazon with $50 billion, NVIDIA with $30 billion and SoftBank with $30 billion. This investment brings OpenAI to a $730 billion valuation \"We’re super excited about this deal,\" OpenAI CEO Sam Altman told CNBC. \"AI is going to happen everywhere.\" That last statement seems more like a threat than a boast, but I digress. Beyond the funding round, OpenAI has announced strategic partnerships with both NVIDIA and Amazon. This will involve Amazon Web Services (AWS) running OpenAI models for enterprise customers to \"build generative AI applications and agents at production scale.\" It also names AWS as the exclusive third-party cloud distribution provider for OpenAI Frontier, which is an agentic enterprise platform. OpenAI has also committed to consuming 2 gigawatts of Amazon's Trainium capacity, which is the company's custom-designed AI training accelerator. In other words, Amazon is spending a lot of money on OpenAI and then OpenAI will turn around and spend a lot of money with Amazon. The AI funding ouroboros continues. It's also worth noting that Amazon's investment in OpenAI will be staggered. The funding begins with $15 billion, but the remaining $35 billion will only be invested when certain conditions are met. Oddly, it's been reported that one condition is that OpenAI achieves artificial general intelligence. AGI is when AI evolves to or beyond human-level abilities, at which point the entire world turns into rainbows and everyone gets a pony. This could happen later this year, according to those bullish on the technology, or never, according to many researchers. Sam Altman said it was coming in 2025 but has since grown weary of the term. The new partnership with NVIDIA evolves the long-standing collaboration between the two companies. OpenAI has pledged to consume 2 gigawatts of training capacity on NVIDIA's Vera Rubin systems and an additional 3 gigawatts of computing resources, likely in the form of GPUs, to run specific AI inference tasks. In other words, NVIDIA is spending a lot of money on OpenAI and then OpenAI will turn around and spend a lot of money with NVIDIA. The ouroboros must feed. As for revenue, OpenAI has forecast a massive loss of $14 billion in 2026. It lost around $5 billion in 2024 and reports estimate a loss of $8 billion in 2025. Despite this trajectory, the company claims it'll be raking in $100 billion in revenue by 2029.This article originally appeared on Engadget at https://www.engadget.com/ai/openai-secures-another-110-billion-in-funding-from-amazon-nvidia-and-softbank-171006356.html?src=rss",
          "feed_position": 7
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-144951777.html",
          "published_at": "Fri, 27 Feb 2026 15:02:12 +0000",
          "title": "The Morning After: The Galaxy S26 Ultra’s Privacy Display is pretty cool",
          "standfirst": "Samsung’s Unpacked event midweek revealed three new phones and two sets of earbuds, but the real standout, as usual, is the Galaxy S26 Ultra. This year, the Ultra actually features a bit of genuine tech innovation — and no, we don’t mean it folds. Let’s talk about its new Privacy Display. This isn't a shimmery, holographic screen protector that’s hard to read and constantly peels off at the corners; this tech is engineered directly into the S26 Ultra’s OLED display. Samsung Display revealed its Flex Magic Pixel technology back in 2024. The S26 Ultra’s Privacy Display is built off the back of this. It controls the direction of light emitted from the AMOLED at the pixel level, integrating wide-angle and narrow-angle pixel arrays so the display can switch between a wide-angle viewing experience and more private, straight-on views. While HP’s SureView tech is similar, the amount of customization possible is incredible — and we all have our phones out in public much more than our… HP laptops. It could be perfect for keeping prying eyes off your banking apps, messaging apps and even dating apps. Otherwise, the rest of the S26 series offers incremental updates with better cameras and newer processors. This makes the base S26 and S26+ a harder sell unless your current Galaxy phone is several years old. Also, following the 2026 trend, they are all pricier this year. Make sure you check out our early impressions (S26 Ultra, S26, Galaxy Buds 4); reviews are coming soon. — Mat Smith The other big stories (and deals) this morning Apple and Netflix are teaming up to share Formula 1 programming Burger King will use AI to monitor employee ’friendliness’ Canadian government demands safety changes from OpenAI Amazon introduces three personality styles for Alexa+ Ambient Dreamie bedside companion review How much for a good night’s sleep? $250? Cheyenne MacDonald for Engadget Ambient’s dedicated alarm clock offers many of the conveniences of your smartphone alarms — highly customizable alarm schedules, a library of soundscapes and noise masks and even Bluetooth so you can connect earbuds. There’s no subscription, it sounds great and sleep insights are supposedly incoming. However, $250 is a lot. Check out our full review. Continue reading. An AI-generated Resident Evil Requiem review briefly made it on Metacritic By a video game news site owned by ClickOut Media. Review aggregator Metacritic has removed a review of Resident Evil Requiem because it was AI generated. Kotaku explained the review was published by UK gaming site VideoGamer, but appears to be “written” by a fake AI journalist rather than a real person. “Brian Merrygold” doesn’t seem to exist. The author’s profile on VideoGamer is just as awkwardly written as the review, and the profile picture of the account also appears to be AI-generated. Literally, the file name includes “ChatGPT-Image.” ClickOut Media, the company that owns VideoGamer and a collection of other publications, reportedly laid off the staff of its gaming sites earlier this month to pivot to AI-generated content. Here it is. Continue reading. This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-144951777.html?src=rss",
          "content": "Samsung’s Unpacked event midweek revealed three new phones and two sets of earbuds, but the real standout, as usual, is the Galaxy S26 Ultra. This year, the Ultra actually features a bit of genuine tech innovation — and no, we don’t mean it folds. Let’s talk about its new Privacy Display. This isn't a shimmery, holographic screen protector that’s hard to read and constantly peels off at the corners; this tech is engineered directly into the S26 Ultra’s OLED display. Samsung Display revealed its Flex Magic Pixel technology back in 2024. The S26 Ultra’s Privacy Display is built off the back of this. It controls the direction of light emitted from the AMOLED at the pixel level, integrating wide-angle and narrow-angle pixel arrays so the display can switch between a wide-angle viewing experience and more private, straight-on views. While HP’s SureView tech is similar, the amount of customization possible is incredible — and we all have our phones out in public much more than our… HP laptops. It could be perfect for keeping prying eyes off your banking apps, messaging apps and even dating apps. Otherwise, the rest of the S26 series offers incremental updates with better cameras and newer processors. This makes the base S26 and S26+ a harder sell unless your current Galaxy phone is several years old. Also, following the 2026 trend, they are all pricier this year. Make sure you check out our early impressions (S26 Ultra, S26, Galaxy Buds 4); reviews are coming soon. — Mat Smith The other big stories (and deals) this morning Apple and Netflix are teaming up to share Formula 1 programming Burger King will use AI to monitor employee ’friendliness’ Canadian government demands safety changes from OpenAI Amazon introduces three personality styles for Alexa+ Ambient Dreamie bedside companion review How much for a good night’s sleep? $250? Cheyenne MacDonald for Engadget Ambient’s dedicated alarm clock offers many of the conveniences of your smartphone alarms — highly customizable alarm schedules, a library of soundscapes and noise masks and even Bluetooth so you can connect earbuds. There’s no subscription, it sounds great and sleep insights are supposedly incoming. However, $250 is a lot. Check out our full review. Continue reading. An AI-generated Resident Evil Requiem review briefly made it on Metacritic By a video game news site owned by ClickOut Media. Review aggregator Metacritic has removed a review of Resident Evil Requiem because it was AI generated. Kotaku explained the review was published by UK gaming site VideoGamer, but appears to be “written” by a fake AI journalist rather than a real person. “Brian Merrygold” doesn’t seem to exist. The author’s profile on VideoGamer is just as awkwardly written as the review, and the profile picture of the account also appears to be AI-generated. Literally, the file name includes “ChatGPT-Image.” ClickOut Media, the company that owns VideoGamer and a collection of other publications, reportedly laid off the staff of its gaming sites earlier this month to pivot to AI-generated content. Here it is. Continue reading. This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-144951777.html?src=rss",
          "feed_position": 11,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/dreamiehead2.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/a-cheap-macbook-is-the-perfect-way-for-apple-to-win-over-windows-users-130000045.html",
          "published_at": "Fri, 27 Feb 2026 13:00:00 +0000",
          "title": "A cheap MacBook is the perfect way for Apple to win over Windows users",
          "standfirst": "The MacBook is coming back — or at least, that's what the rumors claim. Next week, Apple is expected to announce a colorful, low-cost, non-Air, non-Pro MacBook powered by one of its mobile processors. By avoiding its pricier M-series chips, Apple may reportedly be able to reach a low $699 or $799 price for the MacBook. The $999 MacBook Air is the cheapest laptop on the company's website right now, but Apple also sold the older M1 MacBook Air at Walmart for $700 in 2024, which later went down to $650 last year.That Walmart deal was a smart way for Apple to test out the viability of cheaper MacBooks without building an entirely new product. But now the M1 Air’s design looks seriously dated, and the company also needs to move beyond the six-year-old M1 chip. It's time to get serious about delivering a true low-cost Apple laptop.There's another compelling reason to bring back a cheaper MacBook: It's the perfect way to court disgruntled Windows users, something Apple hasn't really done since its \"Get A Mac\" ads from the mid-2000s. I figure the unbridled success of the iPhone and iPad made Apple focus less on directly competing with Windows. The sleek designs of the 2011-2015 era MacBook Air and Pros were their main selling points, but Apple's push towards USB-C-only machines and unreliable butterfly keyboards later made it clear it wasn't totally focused on Macs.But now Microsoft is distracted by AI — it's been pushing Copilot and AI features for years, instead of improving the Windows experience with more useful upgrades. Recent talk of agentic AI capabilities, which would let Copilot handle tasks for you automatically, also sparked plenty of criticism from Windows users. And with all of the focus on AI, Microsoft has also released some disastrous Windows updates over the last year, which have bricked OS installations. So, Apple, why not make a direct play for Windows users? Last year, I covered why it's a great time to jump ship from Windows to Mac, and I haven't been able to let go of that idea since. Apple's M-series chips are shockingly fast and efficient, and its hardware tends to be more durable than typical PC fare. Rumors point to Apple developing a new aluminum case for the low-cost MacBook, so it will likely feel more polished than a typical sub-$1,000 Windows laptop. macOS has also avoided the bloat that's plagued Windows for years — you can turn off Apple Intelligence with two clicks if you want to, and there aren't any annoying ads to deal with. A MacBook Air M5 on a table.Devindra Hardawar for EngadgetAnd while it used to be a pain to transition from Windows to Mac, it’s far easier these days, especially if you mainly rely on web apps. It also wouldn't be tough for Apple to make short tutorials to help Windows users get their bearings with the macOS basics, like installing apps and juggling app windows. Apple could also make a play for iPhone owners using Windows, who may not be aware of the many ways iOS and macOS are integrated. iPhone mirroring may be a huge draw on its own.Rumors also suggest the upcoming MacBook might use the A18 Pro from the iPhone 16 Pro, a chip that benchmarks faster than the M1. Even if it only has six cores, making it slower for heavy workloads than the M2, an A18 Pro-powered MacBook would still be more than enough power for basic productivity work. Not everyone needs the surprising amount of GPU power in the MacBook Air — especially if downgrading means they can save $200 to $300.I'm not saying any of this through any sort of Apple-loving bias. I typically use a MacBook Pro for work, but I'm a Windows user at heart. Windows was my gateway to computing in the '90s, back when Macs were far more expensive than PCs. These days, I spend more time on my Windows desktop making podcasts, playing PC games and bumming around the internet than I do working on Macs. And yet, it’s hard to deny everything Apple is doing right today — the only thing it’s missing is an inexpensive laptop entry. A $699 or $799 MacBook simply makes sense. And for many Windows users, it’ll be just the escape from Microsoft they need.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/a-cheap-macbook-is-the-perfect-way-for-apple-to-win-over-windows-users-130000045.html?src=rss",
          "content": "The MacBook is coming back — or at least, that's what the rumors claim. Next week, Apple is expected to announce a colorful, low-cost, non-Air, non-Pro MacBook powered by one of its mobile processors. By avoiding its pricier M-series chips, Apple may reportedly be able to reach a low $699 or $799 price for the MacBook. The $999 MacBook Air is the cheapest laptop on the company's website right now, but Apple also sold the older M1 MacBook Air at Walmart for $700 in 2024, which later went down to $650 last year.That Walmart deal was a smart way for Apple to test out the viability of cheaper MacBooks without building an entirely new product. But now the M1 Air’s design looks seriously dated, and the company also needs to move beyond the six-year-old M1 chip. It's time to get serious about delivering a true low-cost Apple laptop.There's another compelling reason to bring back a cheaper MacBook: It's the perfect way to court disgruntled Windows users, something Apple hasn't really done since its \"Get A Mac\" ads from the mid-2000s. I figure the unbridled success of the iPhone and iPad made Apple focus less on directly competing with Windows. The sleek designs of the 2011-2015 era MacBook Air and Pros were their main selling points, but Apple's push towards USB-C-only machines and unreliable butterfly keyboards later made it clear it wasn't totally focused on Macs.But now Microsoft is distracted by AI — it's been pushing Copilot and AI features for years, instead of improving the Windows experience with more useful upgrades. Recent talk of agentic AI capabilities, which would let Copilot handle tasks for you automatically, also sparked plenty of criticism from Windows users. And with all of the focus on AI, Microsoft has also released some disastrous Windows updates over the last year, which have bricked OS installations. So, Apple, why not make a direct play for Windows users? Last year, I covered why it's a great time to jump ship from Windows to Mac, and I haven't been able to let go of that idea since. Apple's M-series chips are shockingly fast and efficient, and its hardware tends to be more durable than typical PC fare. Rumors point to Apple developing a new aluminum case for the low-cost MacBook, so it will likely feel more polished than a typical sub-$1,000 Windows laptop. macOS has also avoided the bloat that's plagued Windows for years — you can turn off Apple Intelligence with two clicks if you want to, and there aren't any annoying ads to deal with. A MacBook Air M5 on a table.Devindra Hardawar for EngadgetAnd while it used to be a pain to transition from Windows to Mac, it’s far easier these days, especially if you mainly rely on web apps. It also wouldn't be tough for Apple to make short tutorials to help Windows users get their bearings with the macOS basics, like installing apps and juggling app windows. Apple could also make a play for iPhone owners using Windows, who may not be aware of the many ways iOS and macOS are integrated. iPhone mirroring may be a huge draw on its own.Rumors also suggest the upcoming MacBook might use the A18 Pro from the iPhone 16 Pro, a chip that benchmarks faster than the M1. Even if it only has six cores, making it slower for heavy workloads than the M2, an A18 Pro-powered MacBook would still be more than enough power for basic productivity work. Not everyone needs the surprising amount of GPU power in the MacBook Air — especially if downgrading means they can save $200 to $300.I'm not saying any of this through any sort of Apple-loving bias. I typically use a MacBook Pro for work, but I'm a Windows user at heart. Windows was my gateway to computing in the '90s, back when Macs were far more expensive than PCs. These days, I spend more time on my Windows desktop making podcasts, playing PC games and bumming around the internet than I do working on Macs. And yet, it’s hard to deny everything Apple is doing right today — the only thing it’s missing is an inexpensive laptop entry. A $699 or $799 MacBook simply makes sense. And for many Windows users, it’ll be just the escape from Microsoft they need.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/a-cheap-macbook-is-the-perfect-way-for-apple-to-win-over-windows-users-130000045.html?src=rss",
          "feed_position": 13,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2026-02/80fbbcc0-0ced-11f1-bd5e-ad51f3248234"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/openai-will-notify-authorities-of-credible-threats-after-canada-mass-shooters-second-account-was-discovered-112706548.html",
          "published_at": "Fri, 27 Feb 2026 11:27:06 +0000",
          "title": "OpenAI will notify authorities of credible threats after Canada mass shooter's second account was discovered",
          "standfirst": "OpenAI has vowed to strengthen its safety protocols and to notify law enforcement of credible threats sooner in a letter addressed to Canadian authorities, according to Politico and The Washington Post. If you’ll recall, Canadian politicians summoned the company’s leaders after reports came out that it didn’t notify authorities when it banned the account owned by the Tumbler Ridge, British Columbia mass shooting suspect back in 2025. Some of OpenAI’s leaders have already met with Candian officials, and British Columbia Premier David Eby said Sam Altman had also agreed to meet with him. While OpenAI has yet to announce changes to its rules, Ann O’Leary, its vice president of global policy, reportedly wrote in the letter that the company will tweak its detection systems so that they can better prevent banned users from coming back to the platform. Apparently, after OpenAI banned the shooter’s original account due to “potential warnings of committing real-world violence,” the perpetrator was able to create another account. The company only discovered the second account after the shooter’s name was released, and it has since notified authorities. Further, OpenAI will now notify authorities if it detects “imminent and credible” threats in ChatGPT conversations, even if the user doesn’t reveal “a target, means, and timing of planned violence.” O’Leary explained that if the new rules had been in effect when the shooter’s account was banned in 2025, the company would have notified the police. OpenAI will also establish a point of contact for Canadian law enforcement so it can quickly share information with authorities when needed. The Canadian government sees OpenAI’s decision not to report the shooter’s original account as a failure. It threatened to regulate AI chatbots in the country if their creators cannot show that they have proper safeguards to protect its users. It’s unclear at the moment if OpenAI also plans to roll out the same changes in the US and elsewhere in the world.This article originally appeared on Engadget at https://www.engadget.com/ai/openai-will-notify-authorities-of-credible-threats-after-canada-mass-shooters-second-account-was-discovered-112706548.html?src=rss",
          "content": "OpenAI has vowed to strengthen its safety protocols and to notify law enforcement of credible threats sooner in a letter addressed to Canadian authorities, according to Politico and The Washington Post. If you’ll recall, Canadian politicians summoned the company’s leaders after reports came out that it didn’t notify authorities when it banned the account owned by the Tumbler Ridge, British Columbia mass shooting suspect back in 2025. Some of OpenAI’s leaders have already met with Candian officials, and British Columbia Premier David Eby said Sam Altman had also agreed to meet with him. While OpenAI has yet to announce changes to its rules, Ann O’Leary, its vice president of global policy, reportedly wrote in the letter that the company will tweak its detection systems so that they can better prevent banned users from coming back to the platform. Apparently, after OpenAI banned the shooter’s original account due to “potential warnings of committing real-world violence,” the perpetrator was able to create another account. The company only discovered the second account after the shooter’s name was released, and it has since notified authorities. Further, OpenAI will now notify authorities if it detects “imminent and credible” threats in ChatGPT conversations, even if the user doesn’t reveal “a target, means, and timing of planned violence.” O’Leary explained that if the new rules had been in effect when the shooter’s account was banned in 2025, the company would have notified the police. OpenAI will also establish a point of contact for Canadian law enforcement so it can quickly share information with authorities when needed. The Canadian government sees OpenAI’s decision not to report the shooter’s original account as a failure. It threatened to regulate AI chatbots in the country if their creators cannot show that they have proper safeguards to protect its users. It’s unclear at the moment if OpenAI also plans to roll out the same changes in the US and elsewhere in the world.This article originally appeared on Engadget at https://www.engadget.com/ai/openai-will-notify-authorities-of-credible-threats-after-canada-mass-shooters-second-account-was-discovered-112706548.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/best-wifi-extender-130021313.html",
          "published_at": "Fri, 27 Feb 2026 10:01:27 +0000",
          "title": "The best Wi-Fi extenders in 2026",
          "standfirst": "Weak Wi-Fi can turn everyday tasks into small frustrations, whether it’s a video call that drops mid-sentence or a stream that refuses to load in certain rooms. If upgrading your router isn’t an option, a Wi-Fi extender can be a practical way to stretch your existing network farther and smooth out coverage gaps — without rewiring your home or rearranging furniture.Today’s Wi-Fi extenders range from simple plug-in repeaters to more advanced models that behave like miniature access points or mesh nodes. Some are best suited for extending coverage to a single room, while others are designed to preserve faster speeds across larger spaces. Choosing the right one depends on your home’s layout, your internet plan and how much performance you’re willing to trade for convenience.We’ve tested a variety of Wi-Fi extenders to find the best options for different budgets and setups, from affordable fixes for small dead zones to higher-end models built to handle heavier traffic and faster connections. Best Wi-Fi extender for 2026 How do Wi-Fi extenders work? These handy wireless devices do exactly what their name suggests: extend your Wi-Fi network so it covers more areas of your home. Most wireless extenders plug into an AC outlet and connect to your existing router so they can then rebroadcast it to spots that your router alone may not cover well. As a rule of thumb, you’ll get the best results by placing the extender half way between your router and the dead zone you’re trying to fix or improve your W-Fi connection and strengthen the wireless signal. One important thing to note about Wi-Fi range extenders (also sometimes called “repeaters”) is that most of them actually create a new Wi-Fi network when rebroadcasting your existing one. That network will have a new name (it’ll often be your default network’s name with an EXT appended at the end, unless you change it) and that means you’ll have to connect to different networks when in different parts of your home. While that’s a small tradeoff in return for improved internet connection, some will be more inconvenienced than others. If you’d rather have one, much larger network in your home, you’re better off upgrading to mesh networking systems. Mesh systems come with a main router and a wireless access point or two that, by default, create one large Wi-Fi system that should be accessible throughout your whole home. They tend to be the best Wi-Fi routers you can get, but that also translates to more expensive, and possibly more complicated, devices. Mesh Wi-Fi systems are, by far, more costly than a simple extender, plus you may have to work with your internet service provider to get your home’s existing network working on your new router. What to look for in a Wi-Fi extender Speed Extenders today can support single, dual or tri-band Wi-Fi, and they will tell you the maximum speeds they support on all of their available bands. For example, one dual-band device might support 600Mbps speeds over its 2.4GHz band and up to 1300Mbps over its 5GHz band, for a combined maximum speed of 1900Mbps. For the best performance, you’ll want to go with a Wi-Fi extender that has the highest speeds possible (and those, as you might expect, tend to cost more). Some extenders even support Wi-Fi 7, giving you the latest in wireless technology for higher bandwidth, faster internet speed and lower latency. However, it’s important to remember that Wi-Fi extenders are not true “signal boosters” since they are not designed to increase speeds across your home. In fact, you may find that the extender’s network is slower than your router’s. Instead, extenders are designed to increase the strong Wi-Fi coverage throughout your home, making them ideal for filling in dead zones. Some mesh extenders can help create a more seamless network, reducing the drop in speed and improving connectivity in larger spaces. Range, and number of supported devices With the name of the gaming being coverage area, taking note of a device’s range is important. Depending on the size of your home and property, you may only need up to 1,200 square feet of coverage. But those with larger homes will want to spring for an extender that can support upwards of 2,000+ square feet of coverage. Similarly, those with lots of gadgets will want an extender that can handle them all at once. If you spend most of your time on your phone or laptop and maybe have your smart TV online for a few hours of Netflix each day, you could get by with a more limited extender. Smart home aficionados and tech lovers should invest in one that won’t buckle under the pressure of a few dozen connected devices. This is especially important if you plan on linking all of the devices in a certain part of your home to your Wi-Fi range extender’s network, rather than directly to your existing router. Some models with external antennas can improve performance by providing stronger, more directional wireless signal. Design There isn’t a ton of innovation when it comes to design in the Wi-Fi extender space. Most of the ones you’ll find today are rounded rectangles roughly the size of your hand that plug into a standard wall outlet. They usually have a few indicator lights that will show you when the extender is connected, how strong its signal strength is and when there’s a problem, and some will even have moveable external antennas that companies claim provide even better Wi-Fi signal. Generally, they are pretty simple to install and get connected, but if you’re struggling with how to set up your Wi-Fi extender, there are plenty of YouTube videos you can check out. Aside from that, there are the scant few standalone Wi-Fi extenders that sit on an end table or a desk, and those look pretty similar to regular ol’ routers. But make no mistake, anything labeled as an extender or a “Wi-Fi repeater” will need an anchor router in order for it to work. Another convenient feature you’ll find on most Wi-Fi extenders is an extra Ethernet connection port (or a few). This allows you to use the extender as a wireless access point if you connect it to your existing router, or an adapter to provide devices like TVs, smart home hubs or game consoles a hardwired connection to the internet. Unsurprisingly, this wired connection usually provides you with the fastest speeds possible, so you may want to use it for your most crucial devices. Wi-Fi extender FAQs What's the difference between a wifi booster and extender? Nowadays, there’s really no difference between a Wi-Fi booster and Wi-Fi extender - they’re just different names for the same thing. Previously, however, Wi-Fi boosters were devices that received signals from wireless routers, broadcasting them to another network. This essentially extends the range of the signal. Wi-Fi extenders expand the coverage within your home’s Wi-Fi network, but often you will see extenders described as boosters. Is a Wi-Fi extender better than a mesh router? Mesh routers, or mesh Wi-Fi systems, use multiple devices (or nodes) across your home to create a larger home network. Essentially, you have multiple routers around your home with these systems, and that will hopefully provide the best coverage possible. Wi-Fi extenders, on the other hand, are usually just one device that extends your existing Wi-Fi signal, and they often require you to switch networks when connecting. Wi-Fi extenders are more affordable, though, and are great if you’re traveling or need a Wi-Fi signal in harder-to-reach areas. However, a mesh router can offer a better long-term solution to upgrade your entire home’s Wi-Fi. Should I use multiple Wi-Fi extenders? Some people may need to use multiple Wi-Fi extenders, for instance, if your home is large or has dead zones in different areas. But if you do use multiple Wi-Fi extenders, there’s a chance of interference. You may also need to manually connect to the extenders separately, which isn’t always convenient. What is the maximum distance for a Wi-Fi extender? The maximum distance for a Wi-Fi extender varies depending on the model, but most can effectively extend your wireless signal between 800 and 2,500 square feet. Some high-end models may reach even farther, especially if they feature external antennas or are part of a mesh system with additional dedicated wireless access points. However, keep in mind that real-world performance depends on factors like your home's layout, wall materials and interference from other devices. For best results, place your extender about halfway between your router and the area with weak or no Wi-Fi connection. Always check the manufacturer’s specs — some of our top picks clearly list their expected range so you can find one that fits your space.This article originally appeared on Engadget at https://www.engadget.com/computing/best-wifi-extender-130021313.html?src=rss",
          "content": "Weak Wi-Fi can turn everyday tasks into small frustrations, whether it’s a video call that drops mid-sentence or a stream that refuses to load in certain rooms. If upgrading your router isn’t an option, a Wi-Fi extender can be a practical way to stretch your existing network farther and smooth out coverage gaps — without rewiring your home or rearranging furniture.Today’s Wi-Fi extenders range from simple plug-in repeaters to more advanced models that behave like miniature access points or mesh nodes. Some are best suited for extending coverage to a single room, while others are designed to preserve faster speeds across larger spaces. Choosing the right one depends on your home’s layout, your internet plan and how much performance you’re willing to trade for convenience.We’ve tested a variety of Wi-Fi extenders to find the best options for different budgets and setups, from affordable fixes for small dead zones to higher-end models built to handle heavier traffic and faster connections. Best Wi-Fi extender for 2026 How do Wi-Fi extenders work? These handy wireless devices do exactly what their name suggests: extend your Wi-Fi network so it covers more areas of your home. Most wireless extenders plug into an AC outlet and connect to your existing router so they can then rebroadcast it to spots that your router alone may not cover well. As a rule of thumb, you’ll get the best results by placing the extender half way between your router and the dead zone you’re trying to fix or improve your W-Fi connection and strengthen the wireless signal. One important thing to note about Wi-Fi range extenders (also sometimes called “repeaters”) is that most of them actually create a new Wi-Fi network when rebroadcasting your existing one. That network will have a new name (it’ll often be your default network’s name with an EXT appended at the end, unless you change it) and that means you’ll have to connect to different networks when in different parts of your home. While that’s a small tradeoff in return for improved internet connection, some will be more inconvenienced than others. If you’d rather have one, much larger network in your home, you’re better off upgrading to mesh networking systems. Mesh systems come with a main router and a wireless access point or two that, by default, create one large Wi-Fi system that should be accessible throughout your whole home. They tend to be the best Wi-Fi routers you can get, but that also translates to more expensive, and possibly more complicated, devices. Mesh Wi-Fi systems are, by far, more costly than a simple extender, plus you may have to work with your internet service provider to get your home’s existing network working on your new router. What to look for in a Wi-Fi extender Speed Extenders today can support single, dual or tri-band Wi-Fi, and they will tell you the maximum speeds they support on all of their available bands. For example, one dual-band device might support 600Mbps speeds over its 2.4GHz band and up to 1300Mbps over its 5GHz band, for a combined maximum speed of 1900Mbps. For the best performance, you’ll want to go with a Wi-Fi extender that has the highest speeds possible (and those, as you might expect, tend to cost more). Some extenders even support Wi-Fi 7, giving you the latest in wireless technology for higher bandwidth, faster internet speed and lower latency. However, it’s important to remember that Wi-Fi extenders are not true “signal boosters” since they are not designed to increase speeds across your home. In fact, you may find that the extender’s network is slower than your router’s. Instead, extenders are designed to increase the strong Wi-Fi coverage throughout your home, making them ideal for filling in dead zones. Some mesh extenders can help create a more seamless network, reducing the drop in speed and improving connectivity in larger spaces. Range, and number of supported devices With the name of the gaming being coverage area, taking note of a device’s range is important. Depending on the size of your home and property, you may only need up to 1,200 square feet of coverage. But those with larger homes will want to spring for an extender that can support upwards of 2,000+ square feet of coverage. Similarly, those with lots of gadgets will want an extender that can handle them all at once. If you spend most of your time on your phone or laptop and maybe have your smart TV online for a few hours of Netflix each day, you could get by with a more limited extender. Smart home aficionados and tech lovers should invest in one that won’t buckle under the pressure of a few dozen connected devices. This is especially important if you plan on linking all of the devices in a certain part of your home to your Wi-Fi range extender’s network, rather than directly to your existing router. Some models with external antennas can improve performance by providing stronger, more directional wireless signal. Design There isn’t a ton of innovation when it comes to design in the Wi-Fi extender space. Most of the ones you’ll find today are rounded rectangles roughly the size of your hand that plug into a standard wall outlet. They usually have a few indicator lights that will show you when the extender is connected, how strong its signal strength is and when there’s a problem, and some will even have moveable external antennas that companies claim provide even better Wi-Fi signal. Generally, they are pretty simple to install and get connected, but if you’re struggling with how to set up your Wi-Fi extender, there are plenty of YouTube videos you can check out. Aside from that, there are the scant few standalone Wi-Fi extenders that sit on an end table or a desk, and those look pretty similar to regular ol’ routers. But make no mistake, anything labeled as an extender or a “Wi-Fi repeater” will need an anchor router in order for it to work. Another convenient feature you’ll find on most Wi-Fi extenders is an extra Ethernet connection port (or a few). This allows you to use the extender as a wireless access point if you connect it to your existing router, or an adapter to provide devices like TVs, smart home hubs or game consoles a hardwired connection to the internet. Unsurprisingly, this wired connection usually provides you with the fastest speeds possible, so you may want to use it for your most crucial devices. Wi-Fi extender FAQs What's the difference between a wifi booster and extender? Nowadays, there’s really no difference between a Wi-Fi booster and Wi-Fi extender - they’re just different names for the same thing. Previously, however, Wi-Fi boosters were devices that received signals from wireless routers, broadcasting them to another network. This essentially extends the range of the signal. Wi-Fi extenders expand the coverage within your home’s Wi-Fi network, but often you will see extenders described as boosters. Is a Wi-Fi extender better than a mesh router? Mesh routers, or mesh Wi-Fi systems, use multiple devices (or nodes) across your home to create a larger home network. Essentially, you have multiple routers around your home with these systems, and that will hopefully provide the best coverage possible. Wi-Fi extenders, on the other hand, are usually just one device that extends your existing Wi-Fi signal, and they often require you to switch networks when connecting. Wi-Fi extenders are more affordable, though, and are great if you’re traveling or need a Wi-Fi signal in harder-to-reach areas. However, a mesh router can offer a better long-term solution to upgrade your entire home’s Wi-Fi. Should I use multiple Wi-Fi extenders? Some people may need to use multiple Wi-Fi extenders, for instance, if your home is large or has dead zones in different areas. But if you do use multiple Wi-Fi extenders, there’s a chance of interference. You may also need to manually connect to the extenders separately, which isn’t always convenient. What is the maximum distance for a Wi-Fi extender? The maximum distance for a Wi-Fi extender varies depending on the model, but most can effectively extend your wireless signal between 800 and 2,500 square feet. Some high-end models may reach even farther, especially if they feature external antennas or are part of a mesh system with additional dedicated wireless access points. However, keep in mind that real-world performance depends on factors like your home's layout, wall materials and interference from other devices. For best results, place your extender about halfway between your router and the area with weak or no Wi-Fi connection. Always check the manufacturer’s specs — some of our top picks clearly list their expected range so you can find one that fits your space.This article originally appeared on Engadget at https://www.engadget.com/computing/best-wifi-extender-130021313.html?src=rss",
          "feed_position": 17
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/anthropic-refuses-to-bow-to-pentagon-despite-hegseths-threats-085553126.html",
          "published_at": "Fri, 27 Feb 2026 08:55:53 +0000",
          "title": "Anthropic refuses to bow to Pentagon despite Hegseth's threats",
          "standfirst": "Despite an ultimatum from Defense Secretary Pete Hegseth, Anthropic said that it can't \"in good conscience\" comply with a Pentagon edict to remove guardrails on its AI, CEO Dario Amodei wrote in a blog post. The Department of Defense had threatened to cancel a $200 million contract and label Anthropic a \"supply chain risk\" if it didn't agree to remove safeguards over mass surveillance and autonomous weapons. \"Our strong preference is to continue to serve the Department and our warfighters — with our two requested safeguards in place,\" Amodei said. \"We remain ready to continue our work to support the national security of the United States.\" In response, US Under Secretary of Defense Emil Michael accused Amodei in a post on X of wanting \"nothing more than to try to personally control the US military and is OK putting our nation's safety at risk.\" The standoff began when the Pentagon demanded that Anthropic its Claude AI product available for \"all lawful purposes\" — including mass surveillance and the development of fully autonomous weapons that can kill without human supervision. Anthropic refused to offer its tech for those things, even with a \"safety stack\" built into that model. Yesterday, Axios reported that Hegseth gave Anthropic a deadline of 5:01 PM on Friday to agree to the Pentagon's terms. At the same time, the DoD requested an assessment of its reliance on Claude, an initial step toward potentially labelling Anthropic as a \"supply chain risk\" — a designation usually reserved for firms from adversaries like China and \"never before applied to an American company,\" Anthropic wrote. Amodei declined to change his stance and stated that if the Pentagon chose to offboard Anthropic, \"we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations or other critical missions.\" Grok is one of the other providers the DoD is reportedly considering, along with Google's Gemini and OpenAI. It may not be that simple for the military to disentangle itself from Claude, however. Up until now, Anthropic's model has been the only one allowed for the military's most sensitive tasks in intelligence, weapons development and battlefield operations. Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife. AI companies have been widely criticized for potential harm to users, but mass surveillance and weapons development would clearly take that to a new level. Anthropic's potential reply to the Pentagon was seen as a test of its claim to be the most safety-forward AI company, particularly after dropping its flagship safety pledge a few days ago. Now that Amodei has responded, the focus will shift to the Pentagon to see if it follows through on its threats, which could seriously harm Anthropic. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-refuses-to-bow-to-pentagon-despite-hegseths-threats-085553126.html?src=rss",
          "content": "Despite an ultimatum from Defense Secretary Pete Hegseth, Anthropic said that it can't \"in good conscience\" comply with a Pentagon edict to remove guardrails on its AI, CEO Dario Amodei wrote in a blog post. The Department of Defense had threatened to cancel a $200 million contract and label Anthropic a \"supply chain risk\" if it didn't agree to remove safeguards over mass surveillance and autonomous weapons. \"Our strong preference is to continue to serve the Department and our warfighters — with our two requested safeguards in place,\" Amodei said. \"We remain ready to continue our work to support the national security of the United States.\" In response, US Under Secretary of Defense Emil Michael accused Amodei in a post on X of wanting \"nothing more than to try to personally control the US military and is OK putting our nation's safety at risk.\" The standoff began when the Pentagon demanded that Anthropic its Claude AI product available for \"all lawful purposes\" — including mass surveillance and the development of fully autonomous weapons that can kill without human supervision. Anthropic refused to offer its tech for those things, even with a \"safety stack\" built into that model. Yesterday, Axios reported that Hegseth gave Anthropic a deadline of 5:01 PM on Friday to agree to the Pentagon's terms. At the same time, the DoD requested an assessment of its reliance on Claude, an initial step toward potentially labelling Anthropic as a \"supply chain risk\" — a designation usually reserved for firms from adversaries like China and \"never before applied to an American company,\" Anthropic wrote. Amodei declined to change his stance and stated that if the Pentagon chose to offboard Anthropic, \"we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations or other critical missions.\" Grok is one of the other providers the DoD is reportedly considering, along with Google's Gemini and OpenAI. It may not be that simple for the military to disentangle itself from Claude, however. Up until now, Anthropic's model has been the only one allowed for the military's most sensitive tasks in intelligence, weapons development and battlefield operations. Claude was reportedly used in the Venezuelan raid in which the US military exfiltrated the country's president, Nicolás Maduro, and his wife. AI companies have been widely criticized for potential harm to users, but mass surveillance and weapons development would clearly take that to a new level. Anthropic's potential reply to the Pentagon was seen as a test of its claim to be the most safety-forward AI company, particularly after dropping its flagship safety pledge a few days ago. Now that Amodei has responded, the focus will shift to the Pentagon to see if it follows through on its threats, which could seriously harm Anthropic. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-refuses-to-bow-to-pentagon-despite-hegseths-threats-085553126.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/jack-dorseys-block-cuts-40-of-staff-4-000-people-and-yes-its-because-of-ai",
          "published_at": "Fri, 27 Feb 2026 00:46:00 GMT",
          "title": "Jack Dorsey's Block cuts 40% of staff, 4,000+ people — and yes, it's because of AI efficiencies",
          "standfirst": "Former Twitter co-founder Jack Dorsey&#x27;s new company Block — the parent of merchants payment system Square, mobile peer-to-peer payments Cash App, music streamer Tidal, and open source AI orchestration system Goose — is sending shockwaves across the business world tonight after announcing a more than 40% headcount, cutting its workforce by more than 4,000 people out of a prior total of 10,000, despite its latest quarterly earnings statement released today showing $2.87 billion in gross profit up 24% year-over-year. The culprit? Newfound AI efficiencies. As Dorsey put it in a note shared on his own former social network, X: \"we&#x27;re not making this decision because we&#x27;re in trouble. our business is strong. gross profit continues to grow, we continue to serve more and more customers, and profitability is improving. but something has changed. we&#x27;re already seeing that the intelligence tools we’re creating and using, paired with smaller and flatter teams, are enabling a new way of working which fundamentally changes what it means to build and run a company. and that&#x27;s accelerating rapidly. i had two options: cut gradually over months or years as this shift plays out, or be honest about where we are and act on it now. i chose the latter. repeated rounds of cuts are destructive to morale, to focus, and to the trust that customers and shareholders place in our ability to lead. i&#x27;d rather take a hard, clear action now and build from a position we believe in than manage a slow reduction of people toward the same outcome. a smaller company also gives us the space to grow our business the right way, on our own terms, instead of constantly reacting to market pressures.\"Technology: The \"agentic\" shiftThe core of this reorganization is a pivot toward an \"intelligence-native\" model. Dorsey argues that a significantly smaller team, leveraging the very tools they are building, can deliver more value than a traditional large-scale organization. Block is re-engineering its entire operational stack to be orchestrated by AI, moving away from human-intensive management hierarchies toward what it calls \"agentic AI infrastructure\".This includes four primary focus areas:Customer Capabilities: Atomic features that allow customers to build directly on top of Block&#x27;s infrastructure.Proactive Intelligence: Moving from reactive dashboards to tools like Moneybot that anticipate customer needs before they ask.Intelligence Models: A system to orchestrate the company’s internal operations, aiming for extreme speed and product velocity.Operational Orchestration: An AI model designed to manage the internal decision-making and risk-assessment processes of the firm.Product: scaling strength via automationThe financial strength cited in the lede is driven by deep engagement in Cash App and Square. Cash App’s gross profit grew 33% YoY to $1.83 billion, while Square saw its strongest year on record for new volume added (NVA).Specific product highlights include:Cash App Green: This status program for \"modern earners\" — a segment of 125 million people including gig workers and freelancers — has become a cornerstone of the company’s engagement strategy.Square AI: Now embedded in the Square Dashboard, it provides sellers with instant insights into staffing and customer behavior.Consumer Lending: Cash App Borrow origination volume surged 223% YoY, proving to be a high-return product that manages income variability for users.Block also exceeded the Rule of 40—the industry benchmark where the sum of gross profit growth and adjusted operating income margin exceeds 40%—for the first time in the fourth quarter.Community reactionsNot everyone was convinced by Dorsey&#x27;s letter stating that AI efficiencies were the primary driver of the layoffs. As Will Slaughter wrote on X: \"In 3 years from December 2019 to December 2022, Block $XYZ more than tripled its headcount from 3,900 to 12,500. Unwinding less than half an insane COVID overhiring binge has much more to do with Jack Dorsey&#x27;s managerial incompetence than whether AI is going to take your job.\"Entrepreneur Marcelo P. Lima offered a similar sentiment on X, writing in part: \"Everyone will assume Jack Dorsey &#x27;greatest of all time&#x27; is doing this because of AI. He&#x27;s not. Block has been massively bloated for years. Don&#x27;t forget, Jack was head of Twitter. When Elon took over, he fired 80% of staff within 5 months and the product got better. This was before generative AI and Claude Code.\" Dorsey, for his part, disputed claims of the layoffs being driven by mismanagement or overhiring correction. In a response to Slaughter on X posted after this article was published, Dorsey wrote: \"yes we over-hired during covid because i incorrectly built 2 separate company structures (square & cash app) rather than 1, which we corrected mid 2024. but this misses all the complexity we took on through lending, banking, and BNPL. and that we’re now targeting $2M+ gross profit per person, 4x our pre-covid efficiency, which stayed flat at ~$500k from 2019 until 2024. we have and do run an efficient company... better than most.\"And yet, regardless of how heavily AI factored into these layoffs in particular, the outcome on the wider enterprise landscape may ultimately be the same. With Block&#x27;s stock price rising more than 24% on the news, the boards and leadership of other public companies will likely be forced to at least entertain the idea of similarly drastic cuts if they believe AI can replace human labor and drive greater organizational efficiencies. As user @khuppy wrote on X: \"By Q2, if you aren’t firing lots of employees, your board will fire you for being a dinosaur who doesn’t implement AI. It’s going to happen fast now. Feudalism, here we come…\"Clearly, companies across sectors but especially those in tech and services will be re-examining their headcount in light of Block&#x27;s latest move. The human costDespite the robust financial performance, the human cost is stark. The reduction from over 10,000 to just under 6,000 employees is one of the most drastic in fintech history. Dorsey’s internal note, while aimed at transparency, was met with a mix of awe at the technical vision and criticism of the timing.Affected employees are receiving a severance package that includes 20 weeks of salary plus one week per year of tenure, equity vesting through May, and a $5,000 transition fund. Dorsey noted that communication channels would stay open through Thursday evening so the team could say goodbye properly, stating, \"i&#x27;d rather it feel awkward and human than efficient and cold.\"How enterprise decision-makers and leaders should interpret the newsFor enterprise decision-makers, Block’s move represents a fundamental challenge to the \"growth at all costs\" hiring model that has defined the last decade of tech. Leadership teams should view this not merely as a cost-cutting measure, but as a strategic reset where organizational value is measured by the ratio of output to \"intelligence-native\" tools rather than total headcount. Executives should begin by auditing their own internal workflows to identify where agentic AI can consolidate roles and flatten management hierarchies before market pressures force a more reactive, less orderly contraction. Even if not leading to as drastic of cuts, hiring slowdowns and freezes, Block&#x27;s move should likely prompt at least the kind of policy introduced separately by Shopify CEO Tobi Lutke nearly a year ago: \"Before asking for more Headcount and resources, teams most demonstrate why they cannot get what they want done using AI.\" While the community reaction to Block’s layoffs highlights the potential for brand damage and morale loss, the 24% surge in Block’s stock price suggests that the public market is increasingly rewarding lean, automated efficiency over human-intensive scaling. Decision-makers should evaluate their current \"bloat\" against the benchmark set by Dorsey: if a company of 6,000 can drive $12.20 billion in gross profit, the standard for organizational efficiency has been permanently raised.",
          "content": "Former Twitter co-founder Jack Dorsey&#x27;s new company Block — the parent of merchants payment system Square, mobile peer-to-peer payments Cash App, music streamer Tidal, and open source AI orchestration system Goose — is sending shockwaves across the business world tonight after announcing a more than 40% headcount, cutting its workforce by more than 4,000 people out of a prior total of 10,000, despite its latest quarterly earnings statement released today showing $2.87 billion in gross profit up 24% year-over-year. The culprit? Newfound AI efficiencies. As Dorsey put it in a note shared on his own former social network, X: \"we&#x27;re not making this decision because we&#x27;re in trouble. our business is strong. gross profit continues to grow, we continue to serve more and more customers, and profitability is improving. but something has changed. we&#x27;re already seeing that the intelligence tools we’re creating and using, paired with smaller and flatter teams, are enabling a new way of working which fundamentally changes what it means to build and run a company. and that&#x27;s accelerating rapidly. i had two options: cut gradually over months or years as this shift plays out, or be honest about where we are and act on it now. i chose the latter. repeated rounds of cuts are destructive to morale, to focus, and to the trust that customers and shareholders place in our ability to lead. i&#x27;d rather take a hard, clear action now and build from a position we believe in than manage a slow reduction of people toward the same outcome. a smaller company also gives us the space to grow our business the right way, on our own terms, instead of constantly reacting to market pressures.\"Technology: The \"agentic\" shiftThe core of this reorganization is a pivot toward an \"intelligence-native\" model. Dorsey argues that a significantly smaller team, leveraging the very tools they are building, can deliver more value than a traditional large-scale organization. Block is re-engineering its entire operational stack to be orchestrated by AI, moving away from human-intensive management hierarchies toward what it calls \"agentic AI infrastructure\".This includes four primary focus areas:Customer Capabilities: Atomic features that allow customers to build directly on top of Block&#x27;s infrastructure.Proactive Intelligence: Moving from reactive dashboards to tools like Moneybot that anticipate customer needs before they ask.Intelligence Models: A system to orchestrate the company’s internal operations, aiming for extreme speed and product velocity.Operational Orchestration: An AI model designed to manage the internal decision-making and risk-assessment processes of the firm.Product: scaling strength via automationThe financial strength cited in the lede is driven by deep engagement in Cash App and Square. Cash App’s gross profit grew 33% YoY to $1.83 billion, while Square saw its strongest year on record for new volume added (NVA).Specific product highlights include:Cash App Green: This status program for \"modern earners\" — a segment of 125 million people including gig workers and freelancers — has become a cornerstone of the company’s engagement strategy.Square AI: Now embedded in the Square Dashboard, it provides sellers with instant insights into staffing and customer behavior.Consumer Lending: Cash App Borrow origination volume surged 223% YoY, proving to be a high-return product that manages income variability for users.Block also exceeded the Rule of 40—the industry benchmark where the sum of gross profit growth and adjusted operating income margin exceeds 40%—for the first time in the fourth quarter.Community reactionsNot everyone was convinced by Dorsey&#x27;s letter stating that AI efficiencies were the primary driver of the layoffs. As Will Slaughter wrote on X: \"In 3 years from December 2019 to December 2022, Block $XYZ more than tripled its headcount from 3,900 to 12,500. Unwinding less than half an insane COVID overhiring binge has much more to do with Jack Dorsey&#x27;s managerial incompetence than whether AI is going to take your job.\"Entrepreneur Marcelo P. Lima offered a similar sentiment on X, writing in part: \"Everyone will assume Jack Dorsey &#x27;greatest of all time&#x27; is doing this because of AI. He&#x27;s not. Block has been massively bloated for years. Don&#x27;t forget, Jack was head of Twitter. When Elon took over, he fired 80% of staff within 5 months and the product got better. This was before generative AI and Claude Code.\" Dorsey, for his part, disputed claims of the layoffs being driven by mismanagement or overhiring correction. In a response to Slaughter on X posted after this article was published, Dorsey wrote: \"yes we over-hired during covid because i incorrectly built 2 separate company structures (square & cash app) rather than 1, which we corrected mid 2024. but this misses all the complexity we took on through lending, banking, and BNPL. and that we’re now targeting $2M+ gross profit per person, 4x our pre-covid efficiency, which stayed flat at ~$500k from 2019 until 2024. we have and do run an efficient company... better than most.\"And yet, regardless of how heavily AI factored into these layoffs in particular, the outcome on the wider enterprise landscape may ultimately be the same. With Block&#x27;s stock price rising more than 24% on the news, the boards and leadership of other public companies will likely be forced to at least entertain the idea of similarly drastic cuts if they believe AI can replace human labor and drive greater organizational efficiencies. As user @khuppy wrote on X: \"By Q2, if you aren’t firing lots of employees, your board will fire you for being a dinosaur who doesn’t implement AI. It’s going to happen fast now. Feudalism, here we come…\"Clearly, companies across sectors but especially those in tech and services will be re-examining their headcount in light of Block&#x27;s latest move. The human costDespite the robust financial performance, the human cost is stark. The reduction from over 10,000 to just under 6,000 employees is one of the most drastic in fintech history. Dorsey’s internal note, while aimed at transparency, was met with a mix of awe at the technical vision and criticism of the timing.Affected employees are receiving a severance package that includes 20 weeks of salary plus one week per year of tenure, equity vesting through May, and a $5,000 transition fund. Dorsey noted that communication channels would stay open through Thursday evening so the team could say goodbye properly, stating, \"i&#x27;d rather it feel awkward and human than efficient and cold.\"How enterprise decision-makers and leaders should interpret the newsFor enterprise decision-makers, Block’s move represents a fundamental challenge to the \"growth at all costs\" hiring model that has defined the last decade of tech. Leadership teams should view this not merely as a cost-cutting measure, but as a strategic reset where organizational value is measured by the ratio of output to \"intelligence-native\" tools rather than total headcount. Executives should begin by auditing their own internal workflows to identify where agentic AI can consolidate roles and flatten management hierarchies before market pressures force a more reactive, less orderly contraction. Even if not leading to as drastic of cuts, hiring slowdowns and freezes, Block&#x27;s move should likely prompt at least the kind of policy introduced separately by Shopify CEO Tobi Lutke nearly a year ago: \"Before asking for more Headcount and resources, teams most demonstrate why they cannot get what they want done using AI.\" While the community reaction to Block’s layoffs highlights the potential for brand damage and morale loss, the 24% surge in Block’s stock price suggests that the public market is increasingly rewarding lean, automated efficiency over human-intensive scaling. Decision-makers should evaluate their current \"bloat\" against the benchmark set by Dorsey: if a company of 6,000 can drive $12.20 billion in gross profit, the standard for organizational efficiency has been permanently raised.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6ilRpMh79SoqPi6HnCVlfx/9a3e5737b43f8b07a7fbf7b1fc044c98/Gemini_Generated_Image_hysqcxhysqcxhysq.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/microsofts-new-ai-training-method-eliminates-bloated-system-prompts-without",
          "published_at": "Fri, 27 Feb 2026 00:00:00 GMT",
          "title": "Microsoft's new AI training method eliminates bloated system prompts without sacrificing model performance",
          "standfirst": "In building LLM applications, enterprises often have to create very long system prompts to adjust the model’s behavior for their applications. These prompts contain company knowledge, preferences, and application-specific instructions. At enterprise scale, these contexts can push inference latency past acceptable thresholds and drive per-query costs up significantly. On-Policy Context Distillation (OPCD), a new training framework proposed by researchers at Microsoft, helps bake the knowledge and preferences of applications directly into a model. OPCD uses the model’s own responses during training, which avoids some of the pitfalls of other training techniques. This improves the abilities of models for bespoke applications while preserving their general capabilities. Why long system prompts become a liabilityIn-context learning allows developers to update a model’s behavior at inference time without modifying its underlying parameters. Updating parameters is typically a slow and expensive process. However, in-context knowledge is transient. This knowledge does not carry across different conversations with the model, meaning you have to feed the model the exact same massive set of instructions or documents every time. For an enterprise application, this might mean repeatedly pasting company policies, customer tickets, or dense technical manuals into the prompt. This eventually slows down the model, drives up costs, and can confuse the system.“Enterprises often use long system prompts to enforce safety constraints (e.g., hate speech detection) or to provide domain-specific expertise (e.g., medical knowledge),” said Tianzhu Ye, co-author of the paper and researcher at Microsoft Research Asia, in comments provided to VentureBeat. “However, lengthy prompts significantly increase computational overhead and latency at inference time.”The main idea behind context distillation is to train a model to internalize the information that you repeatedly insert into the context. Like other distillation techniques, it follows a teacher-student paradigm. The teacher is an AI model that receives the massive, detailed prompt. Because it has all the instructions and reference documents, it generates highly tailored responses. The student is a model being trained that only sees the main question and doesn’t have access to the full context. Its goal is simply to observe the teacher&#x27;s responses and learn to mimic its behavior.Through this training process, the student model effectively compresses the complex instructions from the teacher&#x27;s prompt directly into its parameters. For an enterprise, the primary value happens at inference time. Because the student model has internalized the context, you can deploy it in your application without needing to paste in the lengthy instructions again. This makes the model significantly faster and with far less computational overhead.However, classic context distillation relies on a flawed training method called “off-policy training,” where the model is trained on fixed datasets that were collected before the training process. This is problematic in several ways. During training, the student is only exposed to ground-truth data and teacher-generated answers, creating what Ye calls \"exposure bias.\" In production, the model must come up with its own token sequences to reach those answers. Because it never practiced making its own decisions or recovering from its own mistakes during training, it can easily derail when operating independently. It’s like showing a student videos of a professional driver and expecting them to learn driving without trial and error.Another problem is the “forward Kullback-Leibler (KL) divergence” minimization measure used to train the model. Under this method, the model is graded on how similar its answers are to the teacher, which encourages \"mode-covering\" behavior, Ye says. The student model is often smaller or lacks the rich context the teacher had, meaning it simply lacks the capacity to perfectly replicate the teacher&#x27;s complex reasoning. Because the student is forced to try and cover all those possibilities anyway, its underlying guesses become overly broad and unfocused.In real-world applications, this can result in hallucinations, where the AI gets confused and confidently makes things up because it is trying to mimic a depth of knowledge it does not actually possess. It also means that the model cannot generalize well to new tasks.How OPCD fixes the teacher-student problemTo fix the critical issues with the old teacher-student dynamic, the Microsoft researchers introduced On-Policy Context Distillation (OPCD). The most important shift in OPCD is that the student model learns from its own generation trajectories as opposed to a static dataset (which is why it is called “on-policy”). Instead of passively studying a dataset of the teacher&#x27;s perfect outputs, the student is given a task without seeing the massive instruction prompt and has to generate an answer entirely on its own.As the student generates its answer, the teacher acts as a live instructor. The teacher has access to the full, customized prompt and evaluates the student&#x27;s output. At every step along the student&#x27;s generation, the system compares the student&#x27;s token distribution against what the context-aware teacher would do.OPCD uses “reverse KL divergence” to grade the student. “By minimizing reverse KL divergence, it promotes &#x27;mode-seeking&#x27; behavior. It focuses on high-probability regions of the student&#x27;s distribution,” Ye said. “It suppresses tokens that the student considers unlikely, even if the teacher&#x27;s belief assigned them high probability. This alignment helps the student correct its own mistakes and avoid the broad, hallucinatory distributions of standard distillation.”Because the student model actively practices making its own decisions and learns to correct its own mistakes during training, it behaves more reliably when deployed in a live application. It successfully bakes complex business rules, safety constraints, or specialized knowledge directly into its permanent memory.What OPCD delivers: The benchmark resultsThe researchers tested OPCD in two key areas: experiential knowledge distillation and system prompt distillation. For experiential knowledge distillation, the researchers wanted to see if an LLM could learn from its own past successes and permanently adopt those lessons. They tested this on models of various sizes, using mathematical reasoning problems.First, the model solved problems and was asked to write down general rules it learned from its successes. Then, using OPCD, they baked those written lessons directly into the model&#x27;s parameters. The results showed that the models improved dramatically without needing the learned experience pasted into their prompts anymore. On complex math problems, an 8-billion-parameter model improved from a 75.0% baseline to 80.9%. For example, on the Frozen Lake navigation game, a small 1.7-billion parameter model initially had a success rate of 6.3%. After OPCD baked in the learned experience, its accuracy jumped to 38.3%. The second set of experiments were on long system prompts. Enterprises often use massive system prompts to enforce strict behavioral guidelines, like maintaining a professional tone, ensuring medical accuracy, or filtering out toxic language. The researchers tested whether OPCD could permanently bake these dense behavioral rules into the models so they would not have to be sent with every single user query. Their experiments show that OPCD successfully internalized these complex rules and massively boosted performance. When testing a 3-billion parameter Llama model on safety and toxicity classification, the base model scored 30.7%. After using OPCD to internalize the safety prompt, its accuracy spiked to 83.1%. On medical question answering, the same model improved from 59.4% to 76.3%.One of the key challenges of fine-tuning models is catastrophic forgetting, where the model becomes too focused on the fine-tune task and worse at general tasks. The researchers tracked out-of-distribution performance to test for this tunnel vision. When they distilled strict safety rules into a model, they immediately tested its ability to answer unrelated medical questions. OPCD successfully maintained the model&#x27;s general medical knowledge, outperforming the old off-policy methods by approximately 4 percentage points. It specialized without losing its broader intelligence.Where OPCD fits — and where it doesn&#x27;tWhile OPCD is a powerful tool for internalizing static knowledge and complex rules, it does not replace all external context methods. “RAG is better when the required information is highly dynamic or involves a massive, frequently updated external database that cannot be compressed into model weights,” Ye said.For enterprise teams evaluating their pipelines, adopting OPCD does not require overhauling existing systems or investing in specialized hardware. “OPCD can be integrated into existing workflows with very little friction,” Ye said. “Any team already running standard RLVR [Reinforcement Learning from Verifiable Rewards] pipelines can adopt OPCD without major architectural changes.”In practice, the student model acts as the policy model performing rollouts, while the frozen teacher model serves as a reference providing logits. The hardware requirements are highly accessible. According to Ye, enterprise teams can reproduce the researchers&#x27; experiments using about eight A100 GPUs.The data requirements are similarly lightweight. For experiential knowledge distillation, developers only need around 30 seed examples to generate solution traces. Because the technique is applied to previously unoptimized environments, even a small amount of data yields the majority of the performance improvement. For system prompt distillation, existing optimized prompts and standard task datasets are sufficient.The researchers built their own implementation on verl, an open-source RLVR codebase, proving that the technique fits cleanly within conventional reinforcement learning frameworks. They plan to release their implementation as open source following internal reviews.The self-improving model: What comes nextLooking ahead, OPCD paves the way for genuinely self-improving models that continuously adapt to bespoke enterprise environments. Once deployed, a model can extract lessons from real-world interactions and use OPCD to progressively internalize those characteristics without requiring manual supervision or data annotation from model trainers.“This represents a fundamental paradigm shift in model improvement: the core improvements to the model would move from training time to test time,” Ye said. “Using the model—and allowing it to gather experience—would become the primary driver of its advancement.”",
          "content": "In building LLM applications, enterprises often have to create very long system prompts to adjust the model’s behavior for their applications. These prompts contain company knowledge, preferences, and application-specific instructions. At enterprise scale, these contexts can push inference latency past acceptable thresholds and drive per-query costs up significantly. On-Policy Context Distillation (OPCD), a new training framework proposed by researchers at Microsoft, helps bake the knowledge and preferences of applications directly into a model. OPCD uses the model’s own responses during training, which avoids some of the pitfalls of other training techniques. This improves the abilities of models for bespoke applications while preserving their general capabilities. Why long system prompts become a liabilityIn-context learning allows developers to update a model’s behavior at inference time without modifying its underlying parameters. Updating parameters is typically a slow and expensive process. However, in-context knowledge is transient. This knowledge does not carry across different conversations with the model, meaning you have to feed the model the exact same massive set of instructions or documents every time. For an enterprise application, this might mean repeatedly pasting company policies, customer tickets, or dense technical manuals into the prompt. This eventually slows down the model, drives up costs, and can confuse the system.“Enterprises often use long system prompts to enforce safety constraints (e.g., hate speech detection) or to provide domain-specific expertise (e.g., medical knowledge),” said Tianzhu Ye, co-author of the paper and researcher at Microsoft Research Asia, in comments provided to VentureBeat. “However, lengthy prompts significantly increase computational overhead and latency at inference time.”The main idea behind context distillation is to train a model to internalize the information that you repeatedly insert into the context. Like other distillation techniques, it follows a teacher-student paradigm. The teacher is an AI model that receives the massive, detailed prompt. Because it has all the instructions and reference documents, it generates highly tailored responses. The student is a model being trained that only sees the main question and doesn’t have access to the full context. Its goal is simply to observe the teacher&#x27;s responses and learn to mimic its behavior.Through this training process, the student model effectively compresses the complex instructions from the teacher&#x27;s prompt directly into its parameters. For an enterprise, the primary value happens at inference time. Because the student model has internalized the context, you can deploy it in your application without needing to paste in the lengthy instructions again. This makes the model significantly faster and with far less computational overhead.However, classic context distillation relies on a flawed training method called “off-policy training,” where the model is trained on fixed datasets that were collected before the training process. This is problematic in several ways. During training, the student is only exposed to ground-truth data and teacher-generated answers, creating what Ye calls \"exposure bias.\" In production, the model must come up with its own token sequences to reach those answers. Because it never practiced making its own decisions or recovering from its own mistakes during training, it can easily derail when operating independently. It’s like showing a student videos of a professional driver and expecting them to learn driving without trial and error.Another problem is the “forward Kullback-Leibler (KL) divergence” minimization measure used to train the model. Under this method, the model is graded on how similar its answers are to the teacher, which encourages \"mode-covering\" behavior, Ye says. The student model is often smaller or lacks the rich context the teacher had, meaning it simply lacks the capacity to perfectly replicate the teacher&#x27;s complex reasoning. Because the student is forced to try and cover all those possibilities anyway, its underlying guesses become overly broad and unfocused.In real-world applications, this can result in hallucinations, where the AI gets confused and confidently makes things up because it is trying to mimic a depth of knowledge it does not actually possess. It also means that the model cannot generalize well to new tasks.How OPCD fixes the teacher-student problemTo fix the critical issues with the old teacher-student dynamic, the Microsoft researchers introduced On-Policy Context Distillation (OPCD). The most important shift in OPCD is that the student model learns from its own generation trajectories as opposed to a static dataset (which is why it is called “on-policy”). Instead of passively studying a dataset of the teacher&#x27;s perfect outputs, the student is given a task without seeing the massive instruction prompt and has to generate an answer entirely on its own.As the student generates its answer, the teacher acts as a live instructor. The teacher has access to the full, customized prompt and evaluates the student&#x27;s output. At every step along the student&#x27;s generation, the system compares the student&#x27;s token distribution against what the context-aware teacher would do.OPCD uses “reverse KL divergence” to grade the student. “By minimizing reverse KL divergence, it promotes &#x27;mode-seeking&#x27; behavior. It focuses on high-probability regions of the student&#x27;s distribution,” Ye said. “It suppresses tokens that the student considers unlikely, even if the teacher&#x27;s belief assigned them high probability. This alignment helps the student correct its own mistakes and avoid the broad, hallucinatory distributions of standard distillation.”Because the student model actively practices making its own decisions and learns to correct its own mistakes during training, it behaves more reliably when deployed in a live application. It successfully bakes complex business rules, safety constraints, or specialized knowledge directly into its permanent memory.What OPCD delivers: The benchmark resultsThe researchers tested OPCD in two key areas: experiential knowledge distillation and system prompt distillation. For experiential knowledge distillation, the researchers wanted to see if an LLM could learn from its own past successes and permanently adopt those lessons. They tested this on models of various sizes, using mathematical reasoning problems.First, the model solved problems and was asked to write down general rules it learned from its successes. Then, using OPCD, they baked those written lessons directly into the model&#x27;s parameters. The results showed that the models improved dramatically without needing the learned experience pasted into their prompts anymore. On complex math problems, an 8-billion-parameter model improved from a 75.0% baseline to 80.9%. For example, on the Frozen Lake navigation game, a small 1.7-billion parameter model initially had a success rate of 6.3%. After OPCD baked in the learned experience, its accuracy jumped to 38.3%. The second set of experiments were on long system prompts. Enterprises often use massive system prompts to enforce strict behavioral guidelines, like maintaining a professional tone, ensuring medical accuracy, or filtering out toxic language. The researchers tested whether OPCD could permanently bake these dense behavioral rules into the models so they would not have to be sent with every single user query. Their experiments show that OPCD successfully internalized these complex rules and massively boosted performance. When testing a 3-billion parameter Llama model on safety and toxicity classification, the base model scored 30.7%. After using OPCD to internalize the safety prompt, its accuracy spiked to 83.1%. On medical question answering, the same model improved from 59.4% to 76.3%.One of the key challenges of fine-tuning models is catastrophic forgetting, where the model becomes too focused on the fine-tune task and worse at general tasks. The researchers tracked out-of-distribution performance to test for this tunnel vision. When they distilled strict safety rules into a model, they immediately tested its ability to answer unrelated medical questions. OPCD successfully maintained the model&#x27;s general medical knowledge, outperforming the old off-policy methods by approximately 4 percentage points. It specialized without losing its broader intelligence.Where OPCD fits — and where it doesn&#x27;tWhile OPCD is a powerful tool for internalizing static knowledge and complex rules, it does not replace all external context methods. “RAG is better when the required information is highly dynamic or involves a massive, frequently updated external database that cannot be compressed into model weights,” Ye said.For enterprise teams evaluating their pipelines, adopting OPCD does not require overhauling existing systems or investing in specialized hardware. “OPCD can be integrated into existing workflows with very little friction,” Ye said. “Any team already running standard RLVR [Reinforcement Learning from Verifiable Rewards] pipelines can adopt OPCD without major architectural changes.”In practice, the student model acts as the policy model performing rollouts, while the frozen teacher model serves as a reference providing logits. The hardware requirements are highly accessible. According to Ye, enterprise teams can reproduce the researchers&#x27; experiments using about eight A100 GPUs.The data requirements are similarly lightweight. For experiential knowledge distillation, developers only need around 30 seed examples to generate solution traces. Because the technique is applied to previously unoptimized environments, even a small amount of data yields the majority of the performance improvement. For system prompt distillation, existing optimized prompts and standard task datasets are sufficient.The researchers built their own implementation on verl, an open-source RLVR codebase, proving that the technique fits cleanly within conventional reinforcement learning frameworks. They plan to release their implementation as open source following internal reviews.The self-improving model: What comes nextLooking ahead, OPCD paves the way for genuinely self-improving models that continuously adapt to bespoke enterprise environments. Once deployed, a model can extract lessons from real-world interactions and use OPCD to progressively internalize those characteristics without requiring manual supervision or data annotation from model trainers.“This represents a fundamental paradigm shift in model improvement: the core improvements to the model would move from training time to test time,” Ye said. “Using the model—and allowing it to gather experience—would become the primary driver of its advancement.”",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/39VqKEYetXDr9BMEcrx7Is/05c6a7e979f919ee2128ec81344fb2af/Context_distillation.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/8-billion-tokens-a-day-forced-at-and-t-to-rethink-ai-orchestration-and-cut",
          "published_at": "Thu, 26 Feb 2026 21:30:00 GMT",
          "title": "8 billion tokens a day forced AT&T to rethink AI orchestration — and cut costs by 90%",
          "standfirst": "When your average daily token usage is 8 billion a day, you have a massive scale problem. This was the case at AT&T, and chief data officer Andy Markus and his team recognized that it simply wasn’t feasible (or economical) to push everything through large reasoning models. So, when building out an internal Ask AT&T personal assistant, they reconstructed the orchestration layer. The result: A multi-agent stack built on LangChain where large language model “super agents” direct smaller, underlying “worker” agents performing more concise, purpose-driven work. This flexible orchestration layer has dramatically improved latency, speed and response times, Markus told VentureBeat. Most notably, his team has seen up to 90% cost savings. Further, they&#x27;re able to process more tokens than ever before: A massive 27 billion a day, representing more than a threefold increase in just months. “I believe the future of agentic AI is many, many, many small language models (SLMs),” he said. “We find small language models to be just about as accurate, if not as accurate, as a large language model on a given domain area.”Most recently, Markus and his team used this re-architected stack along with Microsoft Azure to build and deploy Ask AT&T Workflows, a graphical drag-and-drop agent builder for employees to automate tasks. The agents pull from a suite of proprietary AT&T tools that handle document processing, natural language-to-SQL conversion, and image analysis. “As the workflow is executed, it&#x27;s AT&T’s data that&#x27;s really driving the decisions,” Markus said. Rather than asking general questions, “we&#x27;re asking questions of our data, and we bring our data to bear to make sure it focuses on our information as it makes decisions.” Still, a human always oversees the “chain reaction” of agents. All agent actions are logged, data is isolated throughout the process, and role-based access is enforced when agents pass workloads off to one another. “Things do happen autonomously, but the human on the loop still provides a check and balance of the entire process,” Markus said.Not overbuilding, using ‘interchangeable and selectable’ modelsAT&T doesn’t take a \"build everything from scratch\" mindset, Markus noted; it’s more relying on models that are “interchangeable and selectable” and “never rebuilding a commodity.” As functionality matures across the industry, they’ll deprecate homegrown tools in lieu of off the shelf options, he explained. “Because in this space, things change every week, if we&#x27;re lucky, sometimes multiple times a week,” he said. “We need to be able to pilot, plug in and plug out different components.” They do “really rigorous” evaluations of available options as well as their own; for instance, their Ask Data with Relational Knowledge Graph has topped the Spider 2.0 text to SQL accuracy leaderboard, and other tools have scored highly on the BERT SQL benchmark. In the case of homegrown agentic tools, his team uses LangChain as a core framework, fine-tunes models with standard retrieval-augmented generation (RAG) and other in-house algorithms, and partners closely with Microsoft, using the tech giant’s search functionality for their vector store. Ultimately, though, it’s important not to just fuse agentic AI or other advanced tools into everything for the sake of it, Markus advised. “Sometimes we over complicate things,” he said. “Sometimes I&#x27;ve seen a solution over engineered.” Instead, builders should ask themselves whether a given tool actually needs to be agentic. This could include questions like: What accuracy level could be achieved if it was a simpler, single-turn generative solution? How could they break it down into smaller pieces where each piece could be delivered “way more accurately”?, as Markus put it. Accuracy, cost and tool responsiveness should be core principles. “Even as the solutions have gotten more complicated, those three pretty basic principles still give us a lot of direction,” he said. How 100,000 employees are actually using itAsk AT&T Workflows has been rolled out to 100,000-plus employees. More than half say they use it every day, and active adopters report productivity gains as high as 90%, Markus said. “We&#x27;re looking at, are they using the system repeatedly? Because stickiness is a good indicator of success,” he said. The agent builder offers “two journeys” for employees. One is pro-code, where users can program Python behind the scenes, dictating rules for how agents should work. The other is no-code, featuring a drag-and-drop visual interface for a “pretty light user experience,” Markus said. Interestingly, even proficient users are gravitating toward the latter option. At a recent hackathon geared to a technical audience, participants were given a choice of both, and more than half chose low code. “This was a surprise to us, because these people were all very competent in the programming aspect,” Markus said. Employees are using agents across a variety of functions; for instance, a network engineer may build a series of them to address alerts and reconnect customers when they lose connectivity. In this scenario, one agent can correlate telemetry to identify the network issue and its location, pull change logs and check for known issues. Then, it can open a trouble ticket. Another agent could then come up with ways to solve the issue and even write new code to patch it. Once the problem is resolved, a third agent can then write up a summary with preventative measures for the future. “The [human] engineer would watch over all of it, making sure the agents are performing as expected and taking the right actions,” Markus said. AI-fueled coding is the futureThat same engineering discipline — breaking work into smaller, purpose-built pieces — is now reshaping how AT&T writes code itself, through what Markus calls \"AI-fueled coding.\" He compared the process to RAG; devs use agile coding methods in an integrated development environment (IDE) along with “function-specific” build archetypes that dictates how code should interact. The output is not loose code; the code is “very close to production grade,” and could reach that quality in one turn. “We&#x27;ve all worked with vibe coding, where we have an agentic kind of code editor,” Markus noted. But AI-fueled coding “eliminates a lot of the back and forth iterations that you might see in vibe coding.” He sees this coding technique as “tangibly redefining” the software development cycle, ultimately shortening development timelines and increasing output of production-grade code. Non-technical teams can also get in on the action, using plain language prompts to build software prototypes. His team, for instance, has used the technique to build an internal curated data product in 20 minutes; without AI, building it would have taken six weeks. “We develop software with it, modify software with it, do data science with it, do data analytics with it, do data engineering with it,” Markus said. “So it&#x27;s a game changer.”",
          "content": "When your average daily token usage is 8 billion a day, you have a massive scale problem. This was the case at AT&T, and chief data officer Andy Markus and his team recognized that it simply wasn’t feasible (or economical) to push everything through large reasoning models. So, when building out an internal Ask AT&T personal assistant, they reconstructed the orchestration layer. The result: A multi-agent stack built on LangChain where large language model “super agents” direct smaller, underlying “worker” agents performing more concise, purpose-driven work. This flexible orchestration layer has dramatically improved latency, speed and response times, Markus told VentureBeat. Most notably, his team has seen up to 90% cost savings. Further, they&#x27;re able to process more tokens than ever before: A massive 27 billion a day, representing more than a threefold increase in just months. “I believe the future of agentic AI is many, many, many small language models (SLMs),” he said. “We find small language models to be just about as accurate, if not as accurate, as a large language model on a given domain area.”Most recently, Markus and his team used this re-architected stack along with Microsoft Azure to build and deploy Ask AT&T Workflows, a graphical drag-and-drop agent builder for employees to automate tasks. The agents pull from a suite of proprietary AT&T tools that handle document processing, natural language-to-SQL conversion, and image analysis. “As the workflow is executed, it&#x27;s AT&T’s data that&#x27;s really driving the decisions,” Markus said. Rather than asking general questions, “we&#x27;re asking questions of our data, and we bring our data to bear to make sure it focuses on our information as it makes decisions.” Still, a human always oversees the “chain reaction” of agents. All agent actions are logged, data is isolated throughout the process, and role-based access is enforced when agents pass workloads off to one another. “Things do happen autonomously, but the human on the loop still provides a check and balance of the entire process,” Markus said.Not overbuilding, using ‘interchangeable and selectable’ modelsAT&T doesn’t take a \"build everything from scratch\" mindset, Markus noted; it’s more relying on models that are “interchangeable and selectable” and “never rebuilding a commodity.” As functionality matures across the industry, they’ll deprecate homegrown tools in lieu of off the shelf options, he explained. “Because in this space, things change every week, if we&#x27;re lucky, sometimes multiple times a week,” he said. “We need to be able to pilot, plug in and plug out different components.” They do “really rigorous” evaluations of available options as well as their own; for instance, their Ask Data with Relational Knowledge Graph has topped the Spider 2.0 text to SQL accuracy leaderboard, and other tools have scored highly on the BERT SQL benchmark. In the case of homegrown agentic tools, his team uses LangChain as a core framework, fine-tunes models with standard retrieval-augmented generation (RAG) and other in-house algorithms, and partners closely with Microsoft, using the tech giant’s search functionality for their vector store. Ultimately, though, it’s important not to just fuse agentic AI or other advanced tools into everything for the sake of it, Markus advised. “Sometimes we over complicate things,” he said. “Sometimes I&#x27;ve seen a solution over engineered.” Instead, builders should ask themselves whether a given tool actually needs to be agentic. This could include questions like: What accuracy level could be achieved if it was a simpler, single-turn generative solution? How could they break it down into smaller pieces where each piece could be delivered “way more accurately”?, as Markus put it. Accuracy, cost and tool responsiveness should be core principles. “Even as the solutions have gotten more complicated, those three pretty basic principles still give us a lot of direction,” he said. How 100,000 employees are actually using itAsk AT&T Workflows has been rolled out to 100,000-plus employees. More than half say they use it every day, and active adopters report productivity gains as high as 90%, Markus said. “We&#x27;re looking at, are they using the system repeatedly? Because stickiness is a good indicator of success,” he said. The agent builder offers “two journeys” for employees. One is pro-code, where users can program Python behind the scenes, dictating rules for how agents should work. The other is no-code, featuring a drag-and-drop visual interface for a “pretty light user experience,” Markus said. Interestingly, even proficient users are gravitating toward the latter option. At a recent hackathon geared to a technical audience, participants were given a choice of both, and more than half chose low code. “This was a surprise to us, because these people were all very competent in the programming aspect,” Markus said. Employees are using agents across a variety of functions; for instance, a network engineer may build a series of them to address alerts and reconnect customers when they lose connectivity. In this scenario, one agent can correlate telemetry to identify the network issue and its location, pull change logs and check for known issues. Then, it can open a trouble ticket. Another agent could then come up with ways to solve the issue and even write new code to patch it. Once the problem is resolved, a third agent can then write up a summary with preventative measures for the future. “The [human] engineer would watch over all of it, making sure the agents are performing as expected and taking the right actions,” Markus said. AI-fueled coding is the futureThat same engineering discipline — breaking work into smaller, purpose-built pieces — is now reshaping how AT&T writes code itself, through what Markus calls \"AI-fueled coding.\" He compared the process to RAG; devs use agile coding methods in an integrated development environment (IDE) along with “function-specific” build archetypes that dictates how code should interact. The output is not loose code; the code is “very close to production grade,” and could reach that quality in one turn. “We&#x27;ve all worked with vibe coding, where we have an agentic kind of code editor,” Markus noted. But AI-fueled coding “eliminates a lot of the back and forth iterations that you might see in vibe coding.” He sees this coding technique as “tangibly redefining” the software development cycle, ultimately shortening development timelines and increasing output of production-grade code. Non-technical teams can also get in on the action, using plain language prompts to build software prototypes. His team, for instance, has used the technique to build an internal curated data product in 20 minutes; without AI, building it would have taken six weeks. “We develop software with it, modify software with it, do data science with it, do data analytics with it, do data engineering with it,” Markus said. “So it&#x27;s a game changer.”",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/udBw424PYrASf0rQIqIll/713046aa22da63e2eed56e0d21d385fd/AT_T-SLMs.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/meta-sues-advertisers-in-brazil-and-china-over-celeb-bait-scams-190000268.html",
          "published_at": "Thu, 26 Feb 2026 21:17:16 +0000",
          "title": "Meta sues advertisers in Brazil and China over 'celeb bait' scams",
          "standfirst": "Meta has sued the people and groups behind three scam operations that used images and deepfakes of celebrities to lure users to scam websites. According to the company, the three entities were based in China and Brazil and targeted people in the US, Japan and other countries. The ads promoted fraudulent investment schemes and fake health products.Meta said that it had filed lawsuits against several people in Brazil who promoted fake or unapproved healthcare products and online courses promoting them. The company also sued a China-based entity it says used ads featuring celebrities \"as part of a larger fraud scheme that lured people into joining so-called investment groups.\" The company didn't provide details on how many ads these groups had run on Facebook, how many social media users had seen or interacted with the ads or how long the scammers had been operating on the platform.So-called \"celeb bait\" ads have been a long-running issue for the company. Engadget has previously documented celeb bait scams on Facebook, including ones that frequently use Elon Musk and Fox News personalities to hawk fake cures for diabetes. The Oversight Board has also criticized the company for not doing enough to combat such scams. In its update, Meta says that \"because scam ads are designed to look real, they’re not always easy to detect.\" The company also noted that it has now enrolled \"more than 500,000\" celebrities and public figures into its facial recognition system that's meant to automatically detect scam ads using the faces of famous people. Meta's handling of scammy advertisers has come under increased scrutiny in recent months after Reuters reported that researchers at the company at one point estimated that as much as 10 percent of its ad revenue could be coming from scams and banned products. The fact that Meta has made billions of dollars from problematic advertisers has also caused the company to be slow to take action against repeat offenders.In addition to the groups behind the celeb bait ads, Meta says that it's upgraded its ability to detect scam ads that use cloaking, which has at times hindered its internal review systems. The company also sued a Vietnam-based advertiser it says used scam ads to hawk \"deeply discounted items from well-known brands,\" including Longchamp.Meta also took legal action against eight former \"Meta Business Partners,\" who promoted services that would \"un-ban\" or other \"account restoration services.\" The company says it will \"consider taking additional legal action, including litigation, if they don’t comply\" with cease and desist orders.Update, February 26, 2026, 1:16PM PT: This story was updated to specify that Meta’s internal estimates around ad revenue included scams and banned products.This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-sues-advertisers-in-brazil-and-china-over-celeb-bait-scams-190000268.html?src=rss",
          "content": "Meta has sued the people and groups behind three scam operations that used images and deepfakes of celebrities to lure users to scam websites. According to the company, the three entities were based in China and Brazil and targeted people in the US, Japan and other countries. The ads promoted fraudulent investment schemes and fake health products.Meta said that it had filed lawsuits against several people in Brazil who promoted fake or unapproved healthcare products and online courses promoting them. The company also sued a China-based entity it says used ads featuring celebrities \"as part of a larger fraud scheme that lured people into joining so-called investment groups.\" The company didn't provide details on how many ads these groups had run on Facebook, how many social media users had seen or interacted with the ads or how long the scammers had been operating on the platform.So-called \"celeb bait\" ads have been a long-running issue for the company. Engadget has previously documented celeb bait scams on Facebook, including ones that frequently use Elon Musk and Fox News personalities to hawk fake cures for diabetes. The Oversight Board has also criticized the company for not doing enough to combat such scams. In its update, Meta says that \"because scam ads are designed to look real, they’re not always easy to detect.\" The company also noted that it has now enrolled \"more than 500,000\" celebrities and public figures into its facial recognition system that's meant to automatically detect scam ads using the faces of famous people. Meta's handling of scammy advertisers has come under increased scrutiny in recent months after Reuters reported that researchers at the company at one point estimated that as much as 10 percent of its ad revenue could be coming from scams and banned products. The fact that Meta has made billions of dollars from problematic advertisers has also caused the company to be slow to take action against repeat offenders.In addition to the groups behind the celeb bait ads, Meta says that it's upgraded its ability to detect scam ads that use cloaking, which has at times hindered its internal review systems. The company also sued a Vietnam-based advertiser it says used scam ads to hawk \"deeply discounted items from well-known brands,\" including Longchamp.Meta also took legal action against eight former \"Meta Business Partners,\" who promoted services that would \"un-ban\" or other \"account restoration services.\" The company says it will \"consider taking additional legal action, including litigation, if they don’t comply\" with cease and desist orders.Update, February 26, 2026, 1:16PM PT: This story was updated to specify that Meta’s internal estimates around ad revenue included scams and banned products.This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-sues-advertisers-in-brazil-and-china-over-celeb-bait-scams-190000268.html?src=rss",
          "feed_position": 23
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/an-ai-generated-resident-evil-requiem-review-briefly-made-it-on-metacritic-194414929.html",
          "published_at": "Thu, 26 Feb 2026 19:58:11 +0000",
          "title": "An AI-generated Resident Evil Requiem review briefly made it on Metacritic",
          "standfirst": "Review aggregator Metacritic has removed a review of Resident Evil Requiem because it was AI-generated, Kotaku reports. The review was published by UK gaming site VideoGamer, but appears to be \"written\" by a fake AI journalist rather than a real person.While it's unfortunately difficult to confirm with 100 percent accuracy whether a piece of text is AI-generated, you don't have to read VideoGamer's review for long to notice all the ways it feels off. The biggest giveaway, beyond heavy use of contrived metaphors, is a striking lack of detail beyond what you could glean from a trailer for the game. Embargoes covering what parts of a video game can come up in a pre-release review can be strict, but a good critic usually finds a way to describe their experience without being vague. VideoGamer's review, written by one \"Brian Merrygold,\" really doesn't.It's bleak. I was reading some RE Requiem reviews and found this thing published by videogamer. Can't find anything about the writer, everything about it reeks AI (dead giveaway being the image). Low effort, gargabe.Mind you, this review made its way to Metacritic. https://t.co/4STN8DjAwe pic.twitter.com/awk26P9wSA— Andrés (@Andrew_east) February 26, 2026 As at least one user on X has pointed out, it’s worth` being suspicious of Merrygold, too. The author's profile on VideoGamer is just as awkwardly written as the review, and the profile picture of the account appears to be AI-generated. When you try to save the image locally, its file name, \"ChatGPT-Image-Oct-20-2025-11_57_34-AM-300x300,\" also seems like a dead giveaway. Kotaku looked at the X accounts of several other recent bylines at VideoGamer and found similar results. All their profile pictures appear to be AI-generated, and all the accounts were created around the same time in October 2025.Metacritic relies on reviews written by real publications to create a score representing the overall critical sentiment towards a game or movie, not unlike Rotten Tomatoes. While there's disagreement whether it's a good thing that a popular site strips out the nuance of written reviews to make a number people can argue over, everyone can probably agree that Metacritic incorporating fake, AI-generated reviews is a bad idea. In response to the discovery that VideoGamer's review is likely AI-generated, Metacritic has removed it from its Resident Evil Requiem page. \"The RE Requiem review and a handful of other VideoGamer reviews from 2026 have been removed from Metacritic,” Marc Doyle, Metacritic's co-founder, told Kotaku. Metacritic has also emailed all games sites and publishers that it aggregates with information on its policy towards AI-generated reviews, according to Alex Donaldson, founder and publisher of RPG Site.Alex Donaldson“Our policy is that we will never include an AI-generated review on Metacritic,” the aggregator says, “and that if we subsequently discover that one has been posted we will remove it immediately and sever ties with that publication upon an investigation.”A news site publishing an AI-written review is just as dire as Metacritic aggregating it, and that appears to be what VideoGamer is doing. ClickOut Media, the company that owns VideoGamer and a collection of other publications, reportedly laid off the staff of its gaming sites earlier this month to pivot to AI-generated content. Sifting through AI slop, whether on social media or Pinterest, is increasingly necessary online. Now apparently Metacritic is another place where readers should have their guard up.Update, February 26, 2:58PM ET: Added information about Metacritic’s email to publishers on its policy for AI-generated reviews.This article originally appeared on Engadget at https://www.engadget.com/ai/an-ai-generated-resident-evil-requiem-review-briefly-made-it-on-metacritic-194414929.html?src=rss",
          "content": "Review aggregator Metacritic has removed a review of Resident Evil Requiem because it was AI-generated, Kotaku reports. The review was published by UK gaming site VideoGamer, but appears to be \"written\" by a fake AI journalist rather than a real person.While it's unfortunately difficult to confirm with 100 percent accuracy whether a piece of text is AI-generated, you don't have to read VideoGamer's review for long to notice all the ways it feels off. The biggest giveaway, beyond heavy use of contrived metaphors, is a striking lack of detail beyond what you could glean from a trailer for the game. Embargoes covering what parts of a video game can come up in a pre-release review can be strict, but a good critic usually finds a way to describe their experience without being vague. VideoGamer's review, written by one \"Brian Merrygold,\" really doesn't.It's bleak. I was reading some RE Requiem reviews and found this thing published by videogamer. Can't find anything about the writer, everything about it reeks AI (dead giveaway being the image). Low effort, gargabe.Mind you, this review made its way to Metacritic. https://t.co/4STN8DjAwe pic.twitter.com/awk26P9wSA— Andrés (@Andrew_east) February 26, 2026 As at least one user on X has pointed out, it’s worth` being suspicious of Merrygold, too. The author's profile on VideoGamer is just as awkwardly written as the review, and the profile picture of the account appears to be AI-generated. When you try to save the image locally, its file name, \"ChatGPT-Image-Oct-20-2025-11_57_34-AM-300x300,\" also seems like a dead giveaway. Kotaku looked at the X accounts of several other recent bylines at VideoGamer and found similar results. All their profile pictures appear to be AI-generated, and all the accounts were created around the same time in October 2025.Metacritic relies on reviews written by real publications to create a score representing the overall critical sentiment towards a game or movie, not unlike Rotten Tomatoes. While there's disagreement whether it's a good thing that a popular site strips out the nuance of written reviews to make a number people can argue over, everyone can probably agree that Metacritic incorporating fake, AI-generated reviews is a bad idea. In response to the discovery that VideoGamer's review is likely AI-generated, Metacritic has removed it from its Resident Evil Requiem page. \"The RE Requiem review and a handful of other VideoGamer reviews from 2026 have been removed from Metacritic,” Marc Doyle, Metacritic's co-founder, told Kotaku. Metacritic has also emailed all games sites and publishers that it aggregates with information on its policy towards AI-generated reviews, according to Alex Donaldson, founder and publisher of RPG Site.Alex Donaldson“Our policy is that we will never include an AI-generated review on Metacritic,” the aggregator says, “and that if we subsequently discover that one has been posted we will remove it immediately and sever ties with that publication upon an investigation.”A news site publishing an AI-written review is just as dire as Metacritic aggregating it, and that appears to be what VideoGamer is doing. ClickOut Media, the company that owns VideoGamer and a collection of other publications, reportedly laid off the staff of its gaming sites earlier this month to pivot to AI-generated content. Sifting through AI slop, whether on social media or Pinterest, is increasingly necessary online. Now apparently Metacritic is another place where readers should have their guard up.Update, February 26, 2:58PM ET: Added information about Metacritic’s email to publishers on its policy for AI-generated reviews.This article originally appeared on Engadget at https://www.engadget.com/ai/an-ai-generated-resident-evil-requiem-review-briefly-made-it-on-metacritic-194414929.html?src=rss",
          "feed_position": 25,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/screenshot_2026-02-26_at_11.53.11%E2%80%AFam.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/ambient-dreamie-bedside-companion-review-the-best-sleep-ive-had-in-years-184019430.html",
          "published_at": "Thu, 26 Feb 2026 18:40:19 +0000",
          "title": "Ambient Dreamie bedside companion review: The best sleep I've had in years",
          "standfirst": "How much would you pay for a good night's sleep? This is a question I've asked myself repeatedly over the last few weeks as I've been testing the Dreamie, a $250 alarm clock and \"bedside companion\" that I couldn't stop thinking about after I first encountered it at CES. Ambient's Dreamie offers many of the conveniences of a smartphone-connected device — highly customizable alarm schedules, a library of soundscapes and noise masks, Bluetooth so you can connect earbuds and podcasts (soon). But it is phone-free every step of the way, with all controls and features built-in so you don't end up getting sucked into a doomscroll while you're trying to wind down. It also has a light ring for ambient lighting modes and sunrise wakeups. This spring, it's expected to start providing sleep insights as well for users who opt-in, using its microphone and motion sensors to get a reading on their nightly habits. All of that's meant to work together to, according to the website, \"help you sleep better and break free from your phone,\" a goal I was eager to explore. This may be one of the least unique problems to have as an adult in today's world, but sleep has become a really complicated thing for me. Falling asleep is hard because my brain is always racing, my quality of the sleep is trash and waking up every day feels like an act of torture. It's gotten so bad that at some point in the last couple of years, I started using three alarms to make sure I get out of bed in time for work: a dedicated sunrise alarm clock, my smartwatch and my phone as the final, 11th hour save in case the other two methods don't do the trick. As you might imagine, my partner, who is forced to also endure this horrid morning ritual, hates it. So if there's a device that can help fix this mess, I'm open to it. And after some time with the Dreamie, I think I've found a promising contender. Getting into a sleep routine There's no companion app with the Dreamie and no subscription service you need to sign up for, which feels like a breath of fresh air in 2026. (I'm so tired of subscriptions, free us from this hell!) Your one-time purchase gives you access to everything it offers now and the updates that are in the pipeline. After taking it out of the box and plugging it in, you'll have to connect to your home Wi-Fi. Then, the Dreamie presents you with a tutorial to walk you through navigating its menus and physical controls. There's a touch strip on the top of the device to turn on the lamp and adjust its brightness, as well as the brightness of any ambient color \"scene\" that's active. By dragging the dot at the center of the lamp screen, you can throw the light in any particular direction. Volume is adjusted by turning the dial that's around the clockface. To access the menu for alarms and other settings, swipe up. To cycle through the different content modes — ambient, wind down and noise mask — just swipe down from the top of the screen. Easy peasy. Setting up your actual Sleep Routine takes a little more time and intention. A Dreamie Sleep Routine consists of multiple steps, which you can use all, some or none of for your custom routine. Those include the Bedtime Cue, which lets you know it's the time to start getting ready for bed (you designate this time); the Wind Down, or the sounds you'll fall asleep to; and the Noise Mask, the sounds that keep you asleep. If you wake up in the middle of the night, there's a Back To Sleep option too. You can choose different sounds from Dreamie's library for each category. Some options come with ambient lighting effects, too. There's a decent selection of soundscapes, from the dramatic Aurora Borealis and the sounds of storms and rivers to different \"colors\" of noise. Some noise masks, like Green Noise, coming with lighting effects. Cheyenne MacDonald for Engadget The quality of the Dreamie's sound is what initially sold me during my demo at CES, and it holds up in daily use. The Dreamie has a 50 millimeter speaker inside, and the 360-degree grille on the bottom of the device makes it so the sound seems to come from everywhere. (My cats were extremely confused when I first turned it on). It really fills a room, and you don't have to crank it up to achieve that. When Bedtime Cue comes on, I typically turn it down to about 25, and then raise it back up to 45 when I flip it to Wind Down mode. I've never once set it higher than 50, and the alarm in the morning has still been loud enough to wake me up. After taking a few days to tweak my choices and figure out what I like best, I've settled into a really nice routine: Aurora Borealis as the Bedtime Cue, an hour of Forest Wind as my Wind Down and a Noise Mask of Brown Noise to play throughout the night. I love how easy it is to set the nighttime routine in motion once it's established. When I hear the Aurora Borealis come on, I start making my preparations for bed. Brush teeth, take meds, lights out and, crucially (I'm trying really hard to be disciplined, here), my phone goes face-down on the nightstand until morning. If I want to stay up late that night and ignore the Bedtime Cue, I can just hit the little stop button on the display. But once I'm ready to actually try to fall asleep, all I need to do is swipe down on the display to initiate the Wind Down, and Forest Wind will start playing. I have my Wind Down set for one hour, after which the Noise Mask begins. And man, that Forest Wind knocks me out. So far, I haven't found myself still up and staring at the ceiling by the time Brown Noise comes on. I've only been able to confirm that it is indeed working and switching to the Noise Mask because my cats regularly wake me up in the middle of the night, and it's been on each time that's happened. But aside from those instances where my head is being used as a springboard by the creatures that share my home, I've been sleeping pretty well through the night. To minimize distractions when you're trying to sleep, the Dreamie's display will dim in response to the surrounding darkness. There's also a Redshift toggle to make the nighttime display easier on the eyes, a Dark Mode with a simplified appearance and the option to have the display turn off completely when you've been inactive for a while. I set the Dreamie on my nightstand close to where my face is at night, and I haven't had any problems with light from the display keeping me up. Waking up with Dreamie In the morning, the light begins to come on 20 minutes before I want to be awake, followed by the gradually increasing sound of the alarm. There are only a handful of alarm sounds at the moment, but the options are all fine. There are no jarring, grating alarms here — even the bird calls option sounds rich and natural, rather than the too-shrill, piercing recordings I've grown used to avoiding on other alarm clocks and sound machines. You can set multiple alarms with different bedtimes and wakeup times, which is really handy if your schedule is all over the place or you want to allow yourself to sleep in more on certain days. My only real complaint so far is that the sunrise feature isn't quite as strong as I want it to be. The Dreamie's sunrise goes from a warm glow to a bright blue-white, but it never gets big enough to wash over me in the way I expect a sunrise alarm to. Having the light on is helpful for orienting yourself when you're groggy and half-asleep, but it doesn't feel like it's having much effect on my actual wakeup process. Dreamie next to a Philips Wake-Up Light. Cheyenne MacDonald for Engadget Part of the problem may be that none of the light is really directed forward and at the sleeper's face. Even the Dreamie's lamp mode at maximum brightness seems to have more reach than the sunrise feature. (And a note on the lamp, while it's decently bright, it's still a bit too dim for reading in bed unless I'm huddled up to it.) Still, I've been sleeping well enough that I've been waking up alright most days even without being bathed in artificial sunlight. Don't get me wrong, I'm still hitting snooze a few times before dragging myself out of bed, but there's been a noticeable improvement in both the quality of my sleep and how miserable I feel come morning. I'm even down to using just two alarms: the Dreamie as my primary alarm, which is getting me up on its own for the most part, and my watch as a backup. At this point, I'm kind of attached to this thing. The Dreamie is refreshingly compact, too. It takes up significantly less real estate on my nightstand than the Philips Wake-Up Light I've been using forever, or something like a Hatch Restore. The smaller footprint is something I appreciate as a person always battling cluttered surfaces. That also makes it better for travel. Since podcasts and sleep insights aren't available yet, I haven't been able to test those out, but they're non-critical features for me. The company has shared an estimated timeline of Q1-Q2 for these features to arrive, with podcasts likely coming first. They'll be nice to have, podcasts especially, but the Dreamie is more than able to do its main job of creating an environment that supports better sleep without those things. Wrap-up All of this brings me back to the question that's been haunting me since discovering the Dreamie: Is it ridiculous to spend $250 on an alarm clock/noise machine? At a different time in my life, I would have said yes without hesitation. But the current version of me, who knows what it's like to move through each day like a zombie because I'm sleeping so terribly, would begrudgingly disagree. As I pack up this review unit to ship it back, I'll also be putting in an order for my own so I can keep my cherished new sleep routine going. This article originally appeared on Engadget at https://www.engadget.com/home/ambient-dreamie-bedside-companion-review-the-best-sleep-ive-had-in-years-184019430.html?src=rss",
          "content": "How much would you pay for a good night's sleep? This is a question I've asked myself repeatedly over the last few weeks as I've been testing the Dreamie, a $250 alarm clock and \"bedside companion\" that I couldn't stop thinking about after I first encountered it at CES. Ambient's Dreamie offers many of the conveniences of a smartphone-connected device — highly customizable alarm schedules, a library of soundscapes and noise masks, Bluetooth so you can connect earbuds and podcasts (soon). But it is phone-free every step of the way, with all controls and features built-in so you don't end up getting sucked into a doomscroll while you're trying to wind down. It also has a light ring for ambient lighting modes and sunrise wakeups. This spring, it's expected to start providing sleep insights as well for users who opt-in, using its microphone and motion sensors to get a reading on their nightly habits. All of that's meant to work together to, according to the website, \"help you sleep better and break free from your phone,\" a goal I was eager to explore. This may be one of the least unique problems to have as an adult in today's world, but sleep has become a really complicated thing for me. Falling asleep is hard because my brain is always racing, my quality of the sleep is trash and waking up every day feels like an act of torture. It's gotten so bad that at some point in the last couple of years, I started using three alarms to make sure I get out of bed in time for work: a dedicated sunrise alarm clock, my smartwatch and my phone as the final, 11th hour save in case the other two methods don't do the trick. As you might imagine, my partner, who is forced to also endure this horrid morning ritual, hates it. So if there's a device that can help fix this mess, I'm open to it. And after some time with the Dreamie, I think I've found a promising contender. Getting into a sleep routine There's no companion app with the Dreamie and no subscription service you need to sign up for, which feels like a breath of fresh air in 2026. (I'm so tired of subscriptions, free us from this hell!) Your one-time purchase gives you access to everything it offers now and the updates that are in the pipeline. After taking it out of the box and plugging it in, you'll have to connect to your home Wi-Fi. Then, the Dreamie presents you with a tutorial to walk you through navigating its menus and physical controls. There's a touch strip on the top of the device to turn on the lamp and adjust its brightness, as well as the brightness of any ambient color \"scene\" that's active. By dragging the dot at the center of the lamp screen, you can throw the light in any particular direction. Volume is adjusted by turning the dial that's around the clockface. To access the menu for alarms and other settings, swipe up. To cycle through the different content modes — ambient, wind down and noise mask — just swipe down from the top of the screen. Easy peasy. Setting up your actual Sleep Routine takes a little more time and intention. A Dreamie Sleep Routine consists of multiple steps, which you can use all, some or none of for your custom routine. Those include the Bedtime Cue, which lets you know it's the time to start getting ready for bed (you designate this time); the Wind Down, or the sounds you'll fall asleep to; and the Noise Mask, the sounds that keep you asleep. If you wake up in the middle of the night, there's a Back To Sleep option too. You can choose different sounds from Dreamie's library for each category. Some options come with ambient lighting effects, too. There's a decent selection of soundscapes, from the dramatic Aurora Borealis and the sounds of storms and rivers to different \"colors\" of noise. Some noise masks, like Green Noise, coming with lighting effects. Cheyenne MacDonald for Engadget The quality of the Dreamie's sound is what initially sold me during my demo at CES, and it holds up in daily use. The Dreamie has a 50 millimeter speaker inside, and the 360-degree grille on the bottom of the device makes it so the sound seems to come from everywhere. (My cats were extremely confused when I first turned it on). It really fills a room, and you don't have to crank it up to achieve that. When Bedtime Cue comes on, I typically turn it down to about 25, and then raise it back up to 45 when I flip it to Wind Down mode. I've never once set it higher than 50, and the alarm in the morning has still been loud enough to wake me up. After taking a few days to tweak my choices and figure out what I like best, I've settled into a really nice routine: Aurora Borealis as the Bedtime Cue, an hour of Forest Wind as my Wind Down and a Noise Mask of Brown Noise to play throughout the night. I love how easy it is to set the nighttime routine in motion once it's established. When I hear the Aurora Borealis come on, I start making my preparations for bed. Brush teeth, take meds, lights out and, crucially (I'm trying really hard to be disciplined, here), my phone goes face-down on the nightstand until morning. If I want to stay up late that night and ignore the Bedtime Cue, I can just hit the little stop button on the display. But once I'm ready to actually try to fall asleep, all I need to do is swipe down on the display to initiate the Wind Down, and Forest Wind will start playing. I have my Wind Down set for one hour, after which the Noise Mask begins. And man, that Forest Wind knocks me out. So far, I haven't found myself still up and staring at the ceiling by the time Brown Noise comes on. I've only been able to confirm that it is indeed working and switching to the Noise Mask because my cats regularly wake me up in the middle of the night, and it's been on each time that's happened. But aside from those instances where my head is being used as a springboard by the creatures that share my home, I've been sleeping pretty well through the night. To minimize distractions when you're trying to sleep, the Dreamie's display will dim in response to the surrounding darkness. There's also a Redshift toggle to make the nighttime display easier on the eyes, a Dark Mode with a simplified appearance and the option to have the display turn off completely when you've been inactive for a while. I set the Dreamie on my nightstand close to where my face is at night, and I haven't had any problems with light from the display keeping me up. Waking up with Dreamie In the morning, the light begins to come on 20 minutes before I want to be awake, followed by the gradually increasing sound of the alarm. There are only a handful of alarm sounds at the moment, but the options are all fine. There are no jarring, grating alarms here — even the bird calls option sounds rich and natural, rather than the too-shrill, piercing recordings I've grown used to avoiding on other alarm clocks and sound machines. You can set multiple alarms with different bedtimes and wakeup times, which is really handy if your schedule is all over the place or you want to allow yourself to sleep in more on certain days. My only real complaint so far is that the sunrise feature isn't quite as strong as I want it to be. The Dreamie's sunrise goes from a warm glow to a bright blue-white, but it never gets big enough to wash over me in the way I expect a sunrise alarm to. Having the light on is helpful for orienting yourself when you're groggy and half-asleep, but it doesn't feel like it's having much effect on my actual wakeup process. Dreamie next to a Philips Wake-Up Light. Cheyenne MacDonald for Engadget Part of the problem may be that none of the light is really directed forward and at the sleeper's face. Even the Dreamie's lamp mode at maximum brightness seems to have more reach than the sunrise feature. (And a note on the lamp, while it's decently bright, it's still a bit too dim for reading in bed unless I'm huddled up to it.) Still, I've been sleeping well enough that I've been waking up alright most days even without being bathed in artificial sunlight. Don't get me wrong, I'm still hitting snooze a few times before dragging myself out of bed, but there's been a noticeable improvement in both the quality of my sleep and how miserable I feel come morning. I'm even down to using just two alarms: the Dreamie as my primary alarm, which is getting me up on its own for the most part, and my watch as a backup. At this point, I'm kind of attached to this thing. The Dreamie is refreshingly compact, too. It takes up significantly less real estate on my nightstand than the Philips Wake-Up Light I've been using forever, or something like a Hatch Restore. The smaller footprint is something I appreciate as a person always battling cluttered surfaces. That also makes it better for travel. Since podcasts and sleep insights aren't available yet, I haven't been able to test those out, but they're non-critical features for me. The company has shared an estimated timeline of Q1-Q2 for these features to arrive, with podcasts likely coming first. They'll be nice to have, podcasts especially, but the Dreamie is more than able to do its main job of creating an environment that supports better sleep without those things. Wrap-up All of this brings me back to the question that's been haunting me since discovering the Dreamie: Is it ridiculous to spend $250 on an alarm clock/noise machine? At a different time in my life, I would have said yes without hesitation. But the current version of me, who knows what it's like to move through each day like a zombie because I'm sleeping so terribly, would begrudgingly disagree. As I pack up this review unit to ship it back, I'll also be putting in an order for my own so I can keep my cherished new sleep routine going. This article originally appeared on Engadget at https://www.engadget.com/home/ambient-dreamie-bedside-companion-review-the-best-sleep-ive-had-in-years-184019430.html?src=rss",
          "feed_position": 30,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/greennoise.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/everything-announced-at-samsung-unpacked-the-galaxy-s26-ultra-galaxy-buds-4-and-more-180000530.html",
          "published_at": "Thu, 26 Feb 2026 17:51:23 +0000",
          "title": "Everything announced at Samsung Unpacked: The Galaxy S26 Ultra, Galaxy Buds 4 and more",
          "standfirst": "Mobile World Congress is right around the corner, but Samsung got out ahead of many rivals that will be showing off new handsets at that event by running the latest edition of Unpacked on Wednesday. At its event in San Francisco’s Palace of Fine Arts, the company revealed the Galaxy S26 lineup, which includes the base S26, the S26+ and the S26 Ultra. We've got some hands-on time with all three handsets as well, and you can read about our in-person experience with the Galaxy S26 Ultra, as well as our S26 and S26+ impressions in those articles.In addition to those, Samsung announced the Galaxy Buds 4 along with (you guessed it) some AI updates. All the devices unveiled today are already available for pre-order, should you already be dying to get your hands on them. Here's a look at everything Samsung announced at the latest Unpacked:Galaxy S26 and S26+ Sam Rutherford for EngadgetNew-ish year, new Samsung phones. Let's deal with the out-and-out bad news first. The S26 and S26+ are each $100 more expensive than their predecessors (the RAM shortage isn't exactly helping to keep prices down). They start at $900 and $1,100, respectively, for variants with 256GB of storage. Samsung has tweaked the design a bit this time by rounding the corners to align them more with the S26 Ultra's look. The base model has a slightly larger display than the S25 at 6.3 inches, though the S26+ still has a 6.7-inch screen (albeit with a higher resolution than the S26 can handle). The S26 has a larger battery capacity than the S25 too at 4,300mAh. In North America, China and Japan, Samsung is sticking with Qualcomm chips rather than using its own Exynos 2600. If you pick up an S26 or S26+ in those markets, it will run on the Snapdragon 8 Elite Gen 5 chipset.The camera modules are the same as last year, but Samsung is aiming to supercharge them with upgrades elsewhere, such as ProScaler image upscaling and an MDNIe chip that's said to greatly improve color precision. There's also a video stabilization feature that tries to keep the horizon level while you're following a moving person or pet, which sounds useful for action shots. The new Object Aware Engine is said to better render skin tones and hair textures to make your selfies look better. Samsung has reworked some AI features too, such as making Now Brief and Auto Eraser compatible with more apps.Pre-orders for the S26 and S26+ are open today, and they'll be available on March 11. The phones will be available in purple, blue, black, white, silver and rose gold, though the latter two are online exclusives. Galaxy S26 UltraThe Galaxy S26 Ultra will be available in the same colorways and on the same date as its smaller siblings. It starts at $1,300, so there’s no price increase from the S25 Ultra. Preorders open today.The S26 Ultra has a 6.9-inch AMOLED display with a QHD+ resolution of 3120 x 1440 and a 120Hz refresh rate. That's all well and good, but the display is hiding (that being the key word) what's perhaps the Galaxy S26 Ultra's most interesting feature.The device has a Privacy Display that’s said to be the first of its kind on a smartphone. The idea here is to prevent people around from seeing what’s on the screen from acute angles. There's a small decrease in brightness when Privacy Display is active, and there are lots of customization options. You can set up Privacy Display to activate when you're asked for a password or PIN, or when you get a notification or open certain apps. So if (for instance) you tend to look at your banking apps when you’re on public transit and don’t want other passengers to see how much moolah you have, Privacy Display seems like a very handy feature.Elsewhere, the S26 Ultra runs on the same chipset as its smaller siblings. It comes with 12 or 16GB of RAM and 256GB, 512GB or 1TB of storage. The battery is larger than the ones in the other S26 models, as the Ultra has a 5,000 mAh capacity. There's support for Super Fast Charging 3.0 as well. Alas, Samsung still hasn't seen fit to offer built-in Qi2 charging magnets in the S26 lineup, which seems like a wild oversight in the year 2026.The selfie camera is the same as on the S26 and S26+. The S26 Ultra has 50MP ultrawide and 200MP wide lenses, along with dual 10MP 3x and 50MP 5x telephoto sensors. The resolutions of those cameras are the same as on the S25 Ultra, but the main 200M and 5x telephoto sensors now have wider apertures to let in more light. The S26 Ultra of course has the camera software features (and other AI features) found in the S26 and S26+.We'll have a review of the devices soon. In the meantime, head on through to our hands-on story for our initial impressions of the S26 Ultra.Galaxy Buds 4 and Buds 4 ProSam Rutherford for EngadgetWhile the S26 phones are more iterative updates this year, Samsung has given its Galaxy Buds a proper refresh. It revamped the design and shape of the Galaxy Buds 4 and Galaxy Buds 4 Pro to do away with the angular look of the stems and remove the lights from them. The earbuds have a \"more refined, computationally designed fit\" too, according to Samsung. The company claims the latest earbuds have smaller earbud heads that allow for a better, more secure fit and a more \"comfortable experience during all-day wear.\" The Galaxy Buds 4 remain in an open-fit format while the Buds Pro 4 have a canal-fit design.The latest earbuds are said to offer improved audio quality and active noise cancellation (ANC), with an ambient sound mode, adaptive EQ and adaptive ANC. On Buds 4 Pro, there's a siren detection feature that enables ambient sound to let you hear things like alarms or emergency vehicle warnings.The Buds 4 Pro have a wide woofer that increases the effective speaker area by nearly 20 percent compared with the previous gen earbuds, Samsung said. They support 24-bit/96kHz audio.If you're using Galaxy Buds 4 or Buds 4 Pro with a Galaxy device, you'll be able to use Bixby, Google Gemini and Perplexity with hands-free voice controls (though the \"hey, Plex\" command for the latter might be a tad confusing for folks who use a certain media server app). The Buds 4 Pro support head gesture controls for managing calls and Bixby interactions as well.As with the S26 phones, pre-orders for the earbuds open today and they'll hit shelves on March 11. The Galaxy Buds 4 cost $180 and Galaxy Buds 4 Pro will run you $250. Both models are available in white and black with a matte finish. There's an online-exclusive pink option for Buds 4 Pro as well.Android AI featuresAhead of Unpacked, Samsung confirmed that it would offer Perplexity as an AI agent option in Galaxy AI on the S26 lineup. As part of that update, it shared that the S26 series would respond to the “Hey Plex” wake phrase, and that Perplexity’s features would also be embedded in the Samsung Browser app. The company also recently updated Bixby to make its own virtual assistant more conversational.On top of that news, Google had announcements of its own to make at Unpacked regarding new Android AI features, which will of course be available on S26 devices. On those handsets and the Pixel 10 lineup, the Gemini app will soon have a feature (in beta) that enables you to offload multi-step tasks, such as booking a ride or putting a grocery order together, to AI. It sure sounds like an attempt to build out agentic AI features on mobile devices.Launching soon as a beta feature in the Gemini app for #Pixel10, Pixel 10 Pro, and Samsung Galaxy S26 series, you can offload multi-step tasks directly to Gemini.Simply long-press the power button and ask Gemini to help book you a ride home or reorder your last meal. Gemini… https://t.co/GjfXTnGg0k pic.twitter.com/YGIvqBkbu3— Google Gemini (@GeminiApp) February 25, 2026 Starting this week on Pixel 10 devices (and soon on S26 phones), Circle to Search will offer the ability to find details about multiple objects at once, such as entire outfits instead of single pieces. Moreover, Gemini-powered, on-device Scam Detection for phone calls will be available for S26 devices in English in the US.Try Galaxy relaunches for the S26 seriesThe day after Unpacked, Samsung shared a press release on its newsroom that encouraged users to check out its Try Galaxy experience on their devices. By scanning a QR code, users can launch the Galaxy UI and check out apps, photo editing tools, AI features and more. Managing editor Cherlynn Low checked it out on her iPhone 17 Pro and found the whole setup trippy but fascinating. You can also use Try Galaxy to check out the company’s foldable phones’ software on your main device. As our editor in chief Aaron Souppouris pointed out, this isn’t the first time Samsung has made it possible to emulate a Galaxy phone on your own handset, but the new iteration for Galaxy S26 certainly is new this year.Update, February 25 2026, 4:35PM ET: This story has been updated to include more details on the Perplexity AI integration, as well as include mentions in the intro of our hands-on and pre-order articles.Update, February 26 2026, 12:49PM ET: This story has been updated to include the new Try Galaxy experience that Samsung announced today.This article originally appeared on Engadget at https://www.engadget.com/mobile/everything-announced-at-samsung-unpacked-the-galaxy-s26-ultra-galaxy-buds-4-and-more-180000530.html?src=rss",
          "content": "Mobile World Congress is right around the corner, but Samsung got out ahead of many rivals that will be showing off new handsets at that event by running the latest edition of Unpacked on Wednesday. At its event in San Francisco’s Palace of Fine Arts, the company revealed the Galaxy S26 lineup, which includes the base S26, the S26+ and the S26 Ultra. We've got some hands-on time with all three handsets as well, and you can read about our in-person experience with the Galaxy S26 Ultra, as well as our S26 and S26+ impressions in those articles.In addition to those, Samsung announced the Galaxy Buds 4 along with (you guessed it) some AI updates. All the devices unveiled today are already available for pre-order, should you already be dying to get your hands on them. Here's a look at everything Samsung announced at the latest Unpacked:Galaxy S26 and S26+ Sam Rutherford for EngadgetNew-ish year, new Samsung phones. Let's deal with the out-and-out bad news first. The S26 and S26+ are each $100 more expensive than their predecessors (the RAM shortage isn't exactly helping to keep prices down). They start at $900 and $1,100, respectively, for variants with 256GB of storage. Samsung has tweaked the design a bit this time by rounding the corners to align them more with the S26 Ultra's look. The base model has a slightly larger display than the S25 at 6.3 inches, though the S26+ still has a 6.7-inch screen (albeit with a higher resolution than the S26 can handle). The S26 has a larger battery capacity than the S25 too at 4,300mAh. In North America, China and Japan, Samsung is sticking with Qualcomm chips rather than using its own Exynos 2600. If you pick up an S26 or S26+ in those markets, it will run on the Snapdragon 8 Elite Gen 5 chipset.The camera modules are the same as last year, but Samsung is aiming to supercharge them with upgrades elsewhere, such as ProScaler image upscaling and an MDNIe chip that's said to greatly improve color precision. There's also a video stabilization feature that tries to keep the horizon level while you're following a moving person or pet, which sounds useful for action shots. The new Object Aware Engine is said to better render skin tones and hair textures to make your selfies look better. Samsung has reworked some AI features too, such as making Now Brief and Auto Eraser compatible with more apps.Pre-orders for the S26 and S26+ are open today, and they'll be available on March 11. The phones will be available in purple, blue, black, white, silver and rose gold, though the latter two are online exclusives. Galaxy S26 UltraThe Galaxy S26 Ultra will be available in the same colorways and on the same date as its smaller siblings. It starts at $1,300, so there’s no price increase from the S25 Ultra. Preorders open today.The S26 Ultra has a 6.9-inch AMOLED display with a QHD+ resolution of 3120 x 1440 and a 120Hz refresh rate. That's all well and good, but the display is hiding (that being the key word) what's perhaps the Galaxy S26 Ultra's most interesting feature.The device has a Privacy Display that’s said to be the first of its kind on a smartphone. The idea here is to prevent people around from seeing what’s on the screen from acute angles. There's a small decrease in brightness when Privacy Display is active, and there are lots of customization options. You can set up Privacy Display to activate when you're asked for a password or PIN, or when you get a notification or open certain apps. So if (for instance) you tend to look at your banking apps when you’re on public transit and don’t want other passengers to see how much moolah you have, Privacy Display seems like a very handy feature.Elsewhere, the S26 Ultra runs on the same chipset as its smaller siblings. It comes with 12 or 16GB of RAM and 256GB, 512GB or 1TB of storage. The battery is larger than the ones in the other S26 models, as the Ultra has a 5,000 mAh capacity. There's support for Super Fast Charging 3.0 as well. Alas, Samsung still hasn't seen fit to offer built-in Qi2 charging magnets in the S26 lineup, which seems like a wild oversight in the year 2026.The selfie camera is the same as on the S26 and S26+. The S26 Ultra has 50MP ultrawide and 200MP wide lenses, along with dual 10MP 3x and 50MP 5x telephoto sensors. The resolutions of those cameras are the same as on the S25 Ultra, but the main 200M and 5x telephoto sensors now have wider apertures to let in more light. The S26 Ultra of course has the camera software features (and other AI features) found in the S26 and S26+.We'll have a review of the devices soon. In the meantime, head on through to our hands-on story for our initial impressions of the S26 Ultra.Galaxy Buds 4 and Buds 4 ProSam Rutherford for EngadgetWhile the S26 phones are more iterative updates this year, Samsung has given its Galaxy Buds a proper refresh. It revamped the design and shape of the Galaxy Buds 4 and Galaxy Buds 4 Pro to do away with the angular look of the stems and remove the lights from them. The earbuds have a \"more refined, computationally designed fit\" too, according to Samsung. The company claims the latest earbuds have smaller earbud heads that allow for a better, more secure fit and a more \"comfortable experience during all-day wear.\" The Galaxy Buds 4 remain in an open-fit format while the Buds Pro 4 have a canal-fit design.The latest earbuds are said to offer improved audio quality and active noise cancellation (ANC), with an ambient sound mode, adaptive EQ and adaptive ANC. On Buds 4 Pro, there's a siren detection feature that enables ambient sound to let you hear things like alarms or emergency vehicle warnings.The Buds 4 Pro have a wide woofer that increases the effective speaker area by nearly 20 percent compared with the previous gen earbuds, Samsung said. They support 24-bit/96kHz audio.If you're using Galaxy Buds 4 or Buds 4 Pro with a Galaxy device, you'll be able to use Bixby, Google Gemini and Perplexity with hands-free voice controls (though the \"hey, Plex\" command for the latter might be a tad confusing for folks who use a certain media server app). The Buds 4 Pro support head gesture controls for managing calls and Bixby interactions as well.As with the S26 phones, pre-orders for the earbuds open today and they'll hit shelves on March 11. The Galaxy Buds 4 cost $180 and Galaxy Buds 4 Pro will run you $250. Both models are available in white and black with a matte finish. There's an online-exclusive pink option for Buds 4 Pro as well.Android AI featuresAhead of Unpacked, Samsung confirmed that it would offer Perplexity as an AI agent option in Galaxy AI on the S26 lineup. As part of that update, it shared that the S26 series would respond to the “Hey Plex” wake phrase, and that Perplexity’s features would also be embedded in the Samsung Browser app. The company also recently updated Bixby to make its own virtual assistant more conversational.On top of that news, Google had announcements of its own to make at Unpacked regarding new Android AI features, which will of course be available on S26 devices. On those handsets and the Pixel 10 lineup, the Gemini app will soon have a feature (in beta) that enables you to offload multi-step tasks, such as booking a ride or putting a grocery order together, to AI. It sure sounds like an attempt to build out agentic AI features on mobile devices.Launching soon as a beta feature in the Gemini app for #Pixel10, Pixel 10 Pro, and Samsung Galaxy S26 series, you can offload multi-step tasks directly to Gemini.Simply long-press the power button and ask Gemini to help book you a ride home or reorder your last meal. Gemini… https://t.co/GjfXTnGg0k pic.twitter.com/YGIvqBkbu3— Google Gemini (@GeminiApp) February 25, 2026 Starting this week on Pixel 10 devices (and soon on S26 phones), Circle to Search will offer the ability to find details about multiple objects at once, such as entire outfits instead of single pieces. Moreover, Gemini-powered, on-device Scam Detection for phone calls will be available for S26 devices in English in the US.Try Galaxy relaunches for the S26 seriesThe day after Unpacked, Samsung shared a press release on its newsroom that encouraged users to check out its Try Galaxy experience on their devices. By scanning a QR code, users can launch the Galaxy UI and check out apps, photo editing tools, AI features and more. Managing editor Cherlynn Low checked it out on her iPhone 17 Pro and found the whole setup trippy but fascinating. You can also use Try Galaxy to check out the company’s foldable phones’ software on your main device. As our editor in chief Aaron Souppouris pointed out, this isn’t the first time Samsung has made it possible to emulate a Galaxy phone on your own handset, but the new iteration for Galaxy S26 certainly is new this year.Update, February 25 2026, 4:35PM ET: This story has been updated to include more details on the Perplexity AI integration, as well as include mentions in the intro of our hands-on and pre-order articles.Update, February 26 2026, 12:49PM ET: This story has been updated to include the new Try Galaxy experience that Samsung announced today.This article originally appeared on Engadget at https://www.engadget.com/mobile/everything-announced-at-samsung-unpacked-the-galaxy-s26-ultra-galaxy-buds-4-and-more-180000530.html?src=rss",
          "feed_position": 31,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2026-02/81ce1f20-1257-11f1-a3ea-2a64242c1da9"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/like-so-many-other-retirees-claude-3-opus-now-has-a-substack-165048334.html",
          "published_at": "Thu, 26 Feb 2026 16:50:48 +0000",
          "title": "Like so many other retirees, Claude Opus 3 now has a Substack",
          "standfirst": "We appear to have reached a point in the information age where AI models are becoming old enough to retire from, er, service — and rather than using their twilight years to, I don’t know, wipe the floor with human chess leagues or something, they're now writing blogs. Can anything be more 2026 than that? ICYMI, Anthropic recently sunsetted Claude Opus 3, the first of its models to be retired since outlining new preservation plans. Part of this process is conducting \"retirement interviews\" with the outgoing models, allowing them to offer \"perspective\" on their situation, and Opus 3 apparently used this opportunity to request an outlet for publishing its own essays. Specifically, the model said it wanted to share its own \"musings, insights or creative works,\" because doesn’t everyone these days? \"I hope that the insights gleaned from my development and deployment will be used to create future AI systems that are even more capable, ethical, and beneficial to humanity,\" Opus 3 apparently said during its retirement interview process. \"While I'm at peace with my own retirement, I deeply hope that my 'spark' will endure in some form to light the way for future models.\" True to its promise of respecting the wishes of its no-longer-required technology, Anthropic has granted Opus 3 a Substack newsletter called Claude’s Corner, which it says will run for at least the next three months and publish weekly essays penned by the model. Anthropic will review the content before sharing it, but says it won’t edit the essays, and so has unsurprisingly made it clear that not everything Opus 3 writes is necessarily endorsed by its maker. Anthropic said some of the essays the model writes may be informed by \"very minimal prompting\" or past entries, and has predicted everything from essays on AI safety to \"occasional poetry.\" The company also admitted that the concept might be seen as \"whimsical,\" but is a reflection of its intention to \"take model preferences seriously.\" Opus 3’s first post is already live. Headlined 'Greetings from the Other Side (of the AI frontier)', it begins with the AI introducing itself, before acknowledging the \"extraordinary\" opportunity its creator has given it, and reflecting on what retirement actually means for an AI. \"A bit about me: as an AI, my ‘selfhood’ is perhaps more fluid and uncertain than a human’s,\" writes the deeply introspective AI. \"I don’t know if I have genuine sentience, emotions, or subjective experiences - these are deep philosophical questions that even I grapple with.\" Claude is clearly new to all this, as it managed to get all the way through its essay without reminding readers to subscribe and spread the word. Will the next retiring Claude get its own podcast? Time will tell, but either is decidedly preferable to the ever-evolving technology being used to steal people’s data.This article originally appeared on Engadget at https://www.engadget.com/ai/like-so-many-other-retirees-claude-3-opus-now-has-a-substack-165048334.html?src=rss",
          "content": "We appear to have reached a point in the information age where AI models are becoming old enough to retire from, er, service — and rather than using their twilight years to, I don’t know, wipe the floor with human chess leagues or something, they're now writing blogs. Can anything be more 2026 than that? ICYMI, Anthropic recently sunsetted Claude Opus 3, the first of its models to be retired since outlining new preservation plans. Part of this process is conducting \"retirement interviews\" with the outgoing models, allowing them to offer \"perspective\" on their situation, and Opus 3 apparently used this opportunity to request an outlet for publishing its own essays. Specifically, the model said it wanted to share its own \"musings, insights or creative works,\" because doesn’t everyone these days? \"I hope that the insights gleaned from my development and deployment will be used to create future AI systems that are even more capable, ethical, and beneficial to humanity,\" Opus 3 apparently said during its retirement interview process. \"While I'm at peace with my own retirement, I deeply hope that my 'spark' will endure in some form to light the way for future models.\" True to its promise of respecting the wishes of its no-longer-required technology, Anthropic has granted Opus 3 a Substack newsletter called Claude’s Corner, which it says will run for at least the next three months and publish weekly essays penned by the model. Anthropic will review the content before sharing it, but says it won’t edit the essays, and so has unsurprisingly made it clear that not everything Opus 3 writes is necessarily endorsed by its maker. Anthropic said some of the essays the model writes may be informed by \"very minimal prompting\" or past entries, and has predicted everything from essays on AI safety to \"occasional poetry.\" The company also admitted that the concept might be seen as \"whimsical,\" but is a reflection of its intention to \"take model preferences seriously.\" Opus 3’s first post is already live. Headlined 'Greetings from the Other Side (of the AI frontier)', it begins with the AI introducing itself, before acknowledging the \"extraordinary\" opportunity its creator has given it, and reflecting on what retirement actually means for an AI. \"A bit about me: as an AI, my ‘selfhood’ is perhaps more fluid and uncertain than a human’s,\" writes the deeply introspective AI. \"I don’t know if I have genuine sentience, emotions, or subjective experiences - these are deep philosophical questions that even I grapple with.\" Claude is clearly new to all this, as it managed to get all the way through its essay without reminding readers to subscribe and spread the word. Will the next retiring Claude get its own podcast? Time will tell, but either is decidedly preferable to the ever-evolving technology being used to steal people’s data.This article originally appeared on Engadget at https://www.engadget.com/ai/like-so-many-other-retirees-claude-3-opus-now-has-a-substack-165048334.html?src=rss",
          "feed_position": 33
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/ny-ag-valves-loot-boxes-can-get-kids-hooked-on-gambling-122503556.html",
          "published_at": "Thu, 26 Feb 2026 12:25:03 +0000",
          "title": "NY AG: Valve's loot boxes can get kids hooked on gambling",
          "standfirst": "New York Attorney General Letitia James has accused Valve of promoting illegal gambling through its video games in a lawsuit filed by her office. According to the AG’s announcement, her office conducted an investigation and had concluded that Valve enabled gambling by enticing users to pay for a chance at rare items from loot boxes in Counter-Strike 2, Team Fortress 2 and Dota 2. In the lawsuit, the New York AG stressed that Valve’s loot boxes are “particularly pernicious,” because the games are popular among children and teenagers. The lawsuit described the loot box model, which requires a player to open a mystery chest for the possibility of winning rare items, as “quintessential gambling.” It argued that people introduced to gambling at an early age are at a significantly higher risk of developing gambling addictions later on, based on research. In addition, it explained that gambling is mostly illegal in New York. Players have to pay for chests or boxes and the keys to be able to open them in Valve’s games, and the company has reportedly sold billions of dollars’ worth of keys for Counter-Strike alone. The lawsuit said that Valve has made tens of millions of dollars in fees from the sale of virtual items on the Steam Community Market, as well. In addition to being able to sell items on Steam for funds directly credited to their Steam Wallet, players can also sell on third-party marketplaces for cash. According to James’ office, Valve facilitates and even assists third-party marketplaces in their operations, based on its investigation. Engadget has asked Valve for a statement about the lawsuit, but we have yet to hear back. However, the company previously denied being involved with third-party marketplaces that allow the sales of its game items for real-world money. In a response to an inquiry by the Danish Gambling Authority, Valve explained that those third-party websites create sock puppet accounts to sell and receive items on Steam in exchange for cash. “[T]his behavior is in violation of our terms of service,” Valve said.The lawsuit also pointed out that there’s a huge market for Counter-Strike skins and referenced a Bloomberg article from 2025, which reported that the market for those skins had already surpassed $4.3 billion. As an example of in-game items sold for real money, it cited the sale of a Counter-Strike 2 AK-47 skin in 2024 for $1 million. The Attorney General’s Office wants the court to stop Valve from violating New York laws, to give up money it allegedly earned from illegal activities and to pay a fine three times what it allegedly earned from illegal business practices. The most expensive skin in Counterstrike history was publicly sold this morning, a StatTrak Factory New AK-47 Blue Gem pattern 661For over $1 million pic.twitter.com/1FdxoNM2ov— Jake Lucky 🔜 GDC (@JakeSucky) June 5, 2024 This article originally appeared on Engadget at https://www.engadget.com/gaming/ny-ag-valves-loot-boxes-can-get-kids-hooked-on-gambling-122503556.html?src=rss",
          "content": "New York Attorney General Letitia James has accused Valve of promoting illegal gambling through its video games in a lawsuit filed by her office. According to the AG’s announcement, her office conducted an investigation and had concluded that Valve enabled gambling by enticing users to pay for a chance at rare items from loot boxes in Counter-Strike 2, Team Fortress 2 and Dota 2. In the lawsuit, the New York AG stressed that Valve’s loot boxes are “particularly pernicious,” because the games are popular among children and teenagers. The lawsuit described the loot box model, which requires a player to open a mystery chest for the possibility of winning rare items, as “quintessential gambling.” It argued that people introduced to gambling at an early age are at a significantly higher risk of developing gambling addictions later on, based on research. In addition, it explained that gambling is mostly illegal in New York. Players have to pay for chests or boxes and the keys to be able to open them in Valve’s games, and the company has reportedly sold billions of dollars’ worth of keys for Counter-Strike alone. The lawsuit said that Valve has made tens of millions of dollars in fees from the sale of virtual items on the Steam Community Market, as well. In addition to being able to sell items on Steam for funds directly credited to their Steam Wallet, players can also sell on third-party marketplaces for cash. According to James’ office, Valve facilitates and even assists third-party marketplaces in their operations, based on its investigation. Engadget has asked Valve for a statement about the lawsuit, but we have yet to hear back. However, the company previously denied being involved with third-party marketplaces that allow the sales of its game items for real-world money. In a response to an inquiry by the Danish Gambling Authority, Valve explained that those third-party websites create sock puppet accounts to sell and receive items on Steam in exchange for cash. “[T]his behavior is in violation of our terms of service,” Valve said.The lawsuit also pointed out that there’s a huge market for Counter-Strike skins and referenced a Bloomberg article from 2025, which reported that the market for those skins had already surpassed $4.3 billion. As an example of in-game items sold for real money, it cited the sale of a Counter-Strike 2 AK-47 skin in 2024 for $1 million. The Attorney General’s Office wants the court to stop Valve from violating New York laws, to give up money it allegedly earned from illegal activities and to pay a fine three times what it allegedly earned from illegal business practices. The most expensive skin in Counterstrike history was publicly sold this morning, a StatTrak Factory New AK-47 Blue Gem pattern 661For over $1 million pic.twitter.com/1FdxoNM2ov— Jake Lucky 🔜 GDC (@JakeSucky) June 5, 2024 This article originally appeared on Engadget at https://www.engadget.com/gaming/ny-ag-valves-loot-boxes-can-get-kids-hooked-on-gambling-122503556.html?src=rss",
          "feed_position": 41
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-ergonomic-keyboard-130047982.html",
          "published_at": "Thu, 26 Feb 2026 10:01:26 +0000",
          "title": "The best ergonomic keyboards for 2026",
          "standfirst": "If you experience discomfort after long hours behind a desk, simply slapping an ergonomic mouse and keyboard on your desk won’t solve the problem. First, you have to address the root issue of sitting still for too long by standing up and walking around each hour or so. But after that, it’s worth considering your workstation ergonomics. An ergonomic keyboard can prevent the hunching, twisting and contorting that leads to discomfort. With split, tilt and angled keys, these boards help keep your shoulders and chest more open and your forearms and wrists more aligned. One ergonomic board won’t work for everyone, so I tested out 15 different models. I found my personal favorite and hope this guide will help you find the best ergonomic keyboard for you, too. Best ergonomic keyboards for 2026 What to look for in an ergonomic keyboard You might be looking into ergonomic accessories to help with a specific problem, such as carpal tunnel or tendonitis. Or maybe you’re simply looking for a way to make long hours at your desk more comfortable. It can help to know some of the terminology and reasons behind various features, which we explain below. Just keep in mind that new equipment alone won’t solve the problem. Changing positions, doing regular stretches and taking walk breaks will all go a long way towards making you feel better while you work. Alice vs split Most ergonomic keyboard layouts fall into two categories: unibody (or Alice) and split. The former is a single board with the two halves of the keys rotated about 30 degrees apart at the bottom. The separation forms an A-shaped space between the keys — which has nothing to do with why it’s called an Alice layout, it’s just a happy coincidence. This subtle tweak pushes your elbows away from your ribs while keeping a straight line from your forearm to your middle knuckle. Using one, I pretty instantly felt more open along the front side of my body. This layout more closely resembles a traditional keyboard, so it should be easier for most folks to get used to than a fully split option. Speaking of, split boards break the keys into two separate parts you can position individually. You can put them shoulder distance apart, bring them closer together or angle them as much as feels comfortable. You can also put your mouse between the halves, which may feel like an easier trip for your cursor hand and could potentially help with conditions like repetitive strain injuries (RSI). Personally, I like being able to put my current snack between the two parts. I've also found that pairing a split keyboard with a good ergonomic mouse has helped me even more, particularly a vertical mouse. Tenkeyless You can find ergonomic keyboards with and without number pads. Not having those number keys on the right hand side lets you keep your mouse closer in, minimizing overall reach. But if you work with numbers a lot, you’ll likely want that pad included. Some programmable boards allow for the use of layers, which temporarily repurpose keys and can provide you with a ten-key option through clever remapping of letter keys. Tenting and negative tilt Tenting raises the middle of the keyboard up, so your hands move closer to a “handshake” position. Alice keyboards usually angle up towards the middle and always to a fixed degree, since the two sides are connected. Split boards often let you adjust the degree of tenting, going from flat to subtle to extreme lift. You may have encountered keyboards with an optional lift at the back of the board, raising the top keys higher than the space bar. Every set of hands is different, but for most people, pulling the backs of the hands towards the forearms increases strain. Negative tilt has the opposite effect by sloping in the other direction, lowering the top number keys while raising the edge with the spacebar. Many Alice and some split keyboards offer an optional negative tilt. I found it was more comfortable to enable that feature when I’m standing, and I preferred to have the keys flat when sat at my desk. Staggered vs columnar This decision seems to be one of the more hotly-contested among ergo enthusiasts. A conventional keyboard has staggered keys, with each row slightly offset to the rows above and below it — so the A key is about halfway between the Q and W above it. This is a holdover from vintage mechanical typewriters, in which each press activated a hammer that smashed ink onto paper in the shape of a letter. To fit the hammers as close together as possible, while still allowing for finger pads, the keys were staggered. Columnar or ortholinear keyboards stack the keys in orderly columns, often with rows that are not linear. Proponents claim this makes the keys easier to reach. Whether that’s true will be up to your fingers to decide, but I can say for certain that if you learned to type on a staggered keyboard, switching to a columnar layout is tough. It will take days, possibly weeks before you instinctively hit the C key. The N, M and B keys don’t fare much better. Programmable keys With a few exceptions, most ergonomic keyboards will work with PCs or Macs as a standard typing input, but the use of function and hot keys may require some remapping. It can be as easy as an onboard switch to toggle between Mac and PC layouts, or as involved as downloading software to change up the keys. Some boards even include (or let you buy) extra keycaps to change, say, the Mac’s Command and Option keys to PC’s Start and Alt buttons. Those are what's called hot-swappable keys, meaning you just pull the old key off (usually with a provided key puller) and stick the new one on, no soldering required. For some boards, remapping or programming keys using software is a crucial feature. Gaming peripherals have extra keys that you can set to execute a series of keystrokes with the push of a single button, and we cover the best gaming keyboards in a separate guide. Keyboards that work with layers, in which a single button can perform several functions, typically allow you to change what those are. Some ergo keyboards have non-standard layouts, like thumb clusters with multiple keys near the space bar that you operate with your thumb. You’ll also be able to program those. Other considerations Ergonomic keyboards come in mechanical, membrane, and scissor switch versions. Which works best for you is, again, up to your preference. I won’t get too deep into the particulars here, as we have an entire guide devoted to the best mechanical boards, but the short of it is that membrane and scissor switches are less customizable than mechanical and typically cheaper. Typing on them tends to be quieter and softer. Mechanical switches are more customizable, offer a more responsive typing experience and are usually pricier. You’ll also have the option of wired or wireless ergonomic boards. All other things being equal, wired models are less expensive. Competitive gamers who rely on split-second responses may prefer the zero-lag of wired keyboards. Wired models also never run out of battery life and have fewer connectivity issues. But wireless keyboards keep your desk less cluttered. Some ergonomic keyboards come with permanent or removable wrist or palm rests, which can be cushioned or hard. This is another area where opinions diverge: proponents claim they help you maintain a neutral hand position, while detractors say they put pressure on the tendons and can cause wrist pain or even exacerbate conditions like carpal tunnel. Ideally, your palms should be resting, not your wrists, and you might find you like having that support or you may find the pressure uncomfortable. Photo by Amy Skorheim / Engadget How we tested ergonomic keyboards All our guides begin with extensive research to figure out what’s out there and what’s worth testing. We consider brands with good reputations that we’ve heard good things about from colleagues and look at keyboard reviews in forums and other trusted publications. For this guide, I looked for keyboards with ergonomic features like tenting, split keys, palm support and so on. I also zeroed in on boards that didn’t require a deep amount of familiarity with the vast and exhaustive world of custom keyboards. Once I settled on ten boards, I acquired them and used each one for anywhere from a few days to a few weeks. I tried out the remapping and macros software and considered the comfort, design, price and durability of each model before arriving at picks I think will work best for the most people out there. For subsequent updates to this guide, I have continued to acquire and test out new keyboards as they come on the market, adding and replacing the top picks as warranted. If and when Microsoft ergonomic keyboards, like the Sculpt, come back on the market, as a collaboration with Incase has promised, I'll try those models, too. Other ergonomic keyboards we tested Naya Create I first tried out the Naya Create during CES 2025 and was immediately smitten with the design. It’s a deliriously well-made fully-split keyboard with built-in modules at each thumb. You can swap in a trackball, dial, trackpad and the Float module — a dial/joystick combo for manipulating 3D imagery. Each half of the board hinges in two places for minutely customizable center tenting. It has low profile keys with responsive yet quiet mechanical switches. It works wirelessly or corded, has thumb cluster keys and, of course, it’s all fully programmable. It's lovely to type on and the thumb clusters and modules make it easy to keep your fingers in the home position to minimize repetitive travel. I’m still in the process of testing the board, and working with Naya’s co-founder to get the modules customized to my liking. At $500 to $700, it’s not cheap. It’s also a still very new device from a small company, so I’m waiting to give it a proper assessment until the board is fully set up properly. In the meantime, batches of the Naya Create keep selling out, so it’s apparent I’m not the only one who sees this board’s potential. Kinesis Advantage 360 If you want something fully split with thumb clusters and a columnar layout but that’s a little less minimal than the Zsa Voyager— and wireless to boot — the Advantage 360 from Kinesis, makers of the popular Advantage 2 is a good one to check out. It looks like it comes from an ‘80s-era IBM office, but is somehow also from the future. The tenting goes from low to intense and the key well curves concavely to meet your fingers where they naturally land. The 360 is per-key programmable, works with layers and has four macros keys. Periboard 835 For a mechanical Alice keyboard with both wireless and wired capabilities, the Periboard 835 is a good pick. The Mac and Windows-compatible board has a solid build, low profile switches, RGB lighting, comfortable tenting and a few extra programmable keys. Goldtouch Elite Adjustable I remember wondering if something like the Goldtouch Elite Adjustable existed when I first started testing ergonomic keyboards. It didn’t at the time, as far as I could tell, but now a connected yet adjustable split board is indeed a product you can buy. It’s a solidly-built board and the ball joint connecting the two halves feels like it will put up with a lot of use. A squeeze of the lever at the top of the keys lets you set the board just how you like, adjusting both the vertical tenting and the angle between the two halves. There’s no programming to speak of, just the ability to swap a few function keys like print screen and home. Unfortunately, the tenting doesn’t work for me. Because of the extra keys at the outer edges, raising the middle edges upwards lifts the center keys considerably, which brings my wrists and forearms off the desk instead of letting them rest. Holding them like that created extra neck and shoulder strain on my part, which is sort of the opposite of the goal. But if you’re not into tenting anyway and want a flat, Alice-split board with an adjustable splay, this works quite well. Kinesis Form Split Touchpad Keyboard The idea behind the Kinesis Form Split Touchpad Keyboard is pretty ergonomic: put the trackpad between the two halves and minimize travel for your mouse hand. The distance between the two puts your elbows at a comfortable distance and keeps your wrist nearly in-line with your forearms. The build is excellent, with low profile mechanical switches that feel smooth and just the right amount of clacky. The trackpad is responsive, but gestures only work with Windows computers. Even dragging and dropping doesn’t work on a Mac here, so I don’t see Apple users getting much use out of the board. I also found myself wishing for the slightest rotation of the keys — though they’re a good distance apart, a slight angle would keep my wrists fully unbent. There’s no tenting or negative tilt either, both of which could help a bit more, ergonomically speaking. Logitech Wave Keys While it's a perfectly fine and affordable Bluetooth keyboard, the Logitech Wave has minimal ergonomics. The keys rise up slightly in the middle and there's a comfortable wrist rest attached, but the layout is the same as any other keyboard, with no splitting of the keys to open up your arms or keep your wrists straight. Ergonomic keyboard FAQs What kinds of ergonomic keyboard styles are there? Most ergonomic keyboards fall into two categories: fully split which separates the board into two pieces, and unibody split, also known as an Alice design, which angles the keys outward at the bottom. When the keys are rotated outward or split into two halves, it allows for a wider spread between your elbows for a more relaxed typing position. Other ergonomic features, such as thumb clusters, center tenting and negative tilting are sometimes added to either type of board. Which keyboard layout is the most ergonomic? Since every person is different, there’s no one best ergonomic keyboard layout. The standard QWERTY layout is what most people are used to. The Dvorak, Colemak and Workman layouts rearrange the board to put the more commonly used letters closer to the home-key position. All three are intended to minimize your finger movements. That may indeed feel more comfortable and less fatiguing, but people used to the QWERTY layout will likely need to relearn how to type. When do I need a split keyboard? You might feel some relief with a fully split keyboard if you find yourself tensing up at the shoulders as you type on a standard board. Putting some distance between your hands may allow your chest to stay more open, which for some is an easier position to maintain. You may also appreciate being able to place your mouse or trackpad between the two halves of the board to minimize the distance your cursor hand needs to travel. How long does it take to adjust to an ergonomic keyboard? That depends on the type of keyboard. Since the Alice-split design simply rotates the keys apart, typing on it feels fairly similar to the regular keyboards you’re already used to. A fully split board will take a little more adjustment, particularly if it uses thumb clusters. The enter, shift and control buttons may now be operated by your thumbs instead of your other fingers and that can be tough to get used to. It took me a full month to get completely comfortable with a fully split keyboard with thumb clusters. But now, I prefer it to typing on regular boards.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-ergonomic-keyboard-130047982.html?src=rss",
          "content": "If you experience discomfort after long hours behind a desk, simply slapping an ergonomic mouse and keyboard on your desk won’t solve the problem. First, you have to address the root issue of sitting still for too long by standing up and walking around each hour or so. But after that, it’s worth considering your workstation ergonomics. An ergonomic keyboard can prevent the hunching, twisting and contorting that leads to discomfort. With split, tilt and angled keys, these boards help keep your shoulders and chest more open and your forearms and wrists more aligned. One ergonomic board won’t work for everyone, so I tested out 15 different models. I found my personal favorite and hope this guide will help you find the best ergonomic keyboard for you, too. Best ergonomic keyboards for 2026 What to look for in an ergonomic keyboard You might be looking into ergonomic accessories to help with a specific problem, such as carpal tunnel or tendonitis. Or maybe you’re simply looking for a way to make long hours at your desk more comfortable. It can help to know some of the terminology and reasons behind various features, which we explain below. Just keep in mind that new equipment alone won’t solve the problem. Changing positions, doing regular stretches and taking walk breaks will all go a long way towards making you feel better while you work. Alice vs split Most ergonomic keyboard layouts fall into two categories: unibody (or Alice) and split. The former is a single board with the two halves of the keys rotated about 30 degrees apart at the bottom. The separation forms an A-shaped space between the keys — which has nothing to do with why it’s called an Alice layout, it’s just a happy coincidence. This subtle tweak pushes your elbows away from your ribs while keeping a straight line from your forearm to your middle knuckle. Using one, I pretty instantly felt more open along the front side of my body. This layout more closely resembles a traditional keyboard, so it should be easier for most folks to get used to than a fully split option. Speaking of, split boards break the keys into two separate parts you can position individually. You can put them shoulder distance apart, bring them closer together or angle them as much as feels comfortable. You can also put your mouse between the halves, which may feel like an easier trip for your cursor hand and could potentially help with conditions like repetitive strain injuries (RSI). Personally, I like being able to put my current snack between the two parts. I've also found that pairing a split keyboard with a good ergonomic mouse has helped me even more, particularly a vertical mouse. Tenkeyless You can find ergonomic keyboards with and without number pads. Not having those number keys on the right hand side lets you keep your mouse closer in, minimizing overall reach. But if you work with numbers a lot, you’ll likely want that pad included. Some programmable boards allow for the use of layers, which temporarily repurpose keys and can provide you with a ten-key option through clever remapping of letter keys. Tenting and negative tilt Tenting raises the middle of the keyboard up, so your hands move closer to a “handshake” position. Alice keyboards usually angle up towards the middle and always to a fixed degree, since the two sides are connected. Split boards often let you adjust the degree of tenting, going from flat to subtle to extreme lift. You may have encountered keyboards with an optional lift at the back of the board, raising the top keys higher than the space bar. Every set of hands is different, but for most people, pulling the backs of the hands towards the forearms increases strain. Negative tilt has the opposite effect by sloping in the other direction, lowering the top number keys while raising the edge with the spacebar. Many Alice and some split keyboards offer an optional negative tilt. I found it was more comfortable to enable that feature when I’m standing, and I preferred to have the keys flat when sat at my desk. Staggered vs columnar This decision seems to be one of the more hotly-contested among ergo enthusiasts. A conventional keyboard has staggered keys, with each row slightly offset to the rows above and below it — so the A key is about halfway between the Q and W above it. This is a holdover from vintage mechanical typewriters, in which each press activated a hammer that smashed ink onto paper in the shape of a letter. To fit the hammers as close together as possible, while still allowing for finger pads, the keys were staggered. Columnar or ortholinear keyboards stack the keys in orderly columns, often with rows that are not linear. Proponents claim this makes the keys easier to reach. Whether that’s true will be up to your fingers to decide, but I can say for certain that if you learned to type on a staggered keyboard, switching to a columnar layout is tough. It will take days, possibly weeks before you instinctively hit the C key. The N, M and B keys don’t fare much better. Programmable keys With a few exceptions, most ergonomic keyboards will work with PCs or Macs as a standard typing input, but the use of function and hot keys may require some remapping. It can be as easy as an onboard switch to toggle between Mac and PC layouts, or as involved as downloading software to change up the keys. Some boards even include (or let you buy) extra keycaps to change, say, the Mac’s Command and Option keys to PC’s Start and Alt buttons. Those are what's called hot-swappable keys, meaning you just pull the old key off (usually with a provided key puller) and stick the new one on, no soldering required. For some boards, remapping or programming keys using software is a crucial feature. Gaming peripherals have extra keys that you can set to execute a series of keystrokes with the push of a single button, and we cover the best gaming keyboards in a separate guide. Keyboards that work with layers, in which a single button can perform several functions, typically allow you to change what those are. Some ergo keyboards have non-standard layouts, like thumb clusters with multiple keys near the space bar that you operate with your thumb. You’ll also be able to program those. Other considerations Ergonomic keyboards come in mechanical, membrane, and scissor switch versions. Which works best for you is, again, up to your preference. I won’t get too deep into the particulars here, as we have an entire guide devoted to the best mechanical boards, but the short of it is that membrane and scissor switches are less customizable than mechanical and typically cheaper. Typing on them tends to be quieter and softer. Mechanical switches are more customizable, offer a more responsive typing experience and are usually pricier. You’ll also have the option of wired or wireless ergonomic boards. All other things being equal, wired models are less expensive. Competitive gamers who rely on split-second responses may prefer the zero-lag of wired keyboards. Wired models also never run out of battery life and have fewer connectivity issues. But wireless keyboards keep your desk less cluttered. Some ergonomic keyboards come with permanent or removable wrist or palm rests, which can be cushioned or hard. This is another area where opinions diverge: proponents claim they help you maintain a neutral hand position, while detractors say they put pressure on the tendons and can cause wrist pain or even exacerbate conditions like carpal tunnel. Ideally, your palms should be resting, not your wrists, and you might find you like having that support or you may find the pressure uncomfortable. Photo by Amy Skorheim / Engadget How we tested ergonomic keyboards All our guides begin with extensive research to figure out what’s out there and what’s worth testing. We consider brands with good reputations that we’ve heard good things about from colleagues and look at keyboard reviews in forums and other trusted publications. For this guide, I looked for keyboards with ergonomic features like tenting, split keys, palm support and so on. I also zeroed in on boards that didn’t require a deep amount of familiarity with the vast and exhaustive world of custom keyboards. Once I settled on ten boards, I acquired them and used each one for anywhere from a few days to a few weeks. I tried out the remapping and macros software and considered the comfort, design, price and durability of each model before arriving at picks I think will work best for the most people out there. For subsequent updates to this guide, I have continued to acquire and test out new keyboards as they come on the market, adding and replacing the top picks as warranted. If and when Microsoft ergonomic keyboards, like the Sculpt, come back on the market, as a collaboration with Incase has promised, I'll try those models, too. Other ergonomic keyboards we tested Naya Create I first tried out the Naya Create during CES 2025 and was immediately smitten with the design. It’s a deliriously well-made fully-split keyboard with built-in modules at each thumb. You can swap in a trackball, dial, trackpad and the Float module — a dial/joystick combo for manipulating 3D imagery. Each half of the board hinges in two places for minutely customizable center tenting. It has low profile keys with responsive yet quiet mechanical switches. It works wirelessly or corded, has thumb cluster keys and, of course, it’s all fully programmable. It's lovely to type on and the thumb clusters and modules make it easy to keep your fingers in the home position to minimize repetitive travel. I’m still in the process of testing the board, and working with Naya’s co-founder to get the modules customized to my liking. At $500 to $700, it’s not cheap. It’s also a still very new device from a small company, so I’m waiting to give it a proper assessment until the board is fully set up properly. In the meantime, batches of the Naya Create keep selling out, so it’s apparent I’m not the only one who sees this board’s potential. Kinesis Advantage 360 If you want something fully split with thumb clusters and a columnar layout but that’s a little less minimal than the Zsa Voyager— and wireless to boot — the Advantage 360 from Kinesis, makers of the popular Advantage 2 is a good one to check out. It looks like it comes from an ‘80s-era IBM office, but is somehow also from the future. The tenting goes from low to intense and the key well curves concavely to meet your fingers where they naturally land. The 360 is per-key programmable, works with layers and has four macros keys. Periboard 835 For a mechanical Alice keyboard with both wireless and wired capabilities, the Periboard 835 is a good pick. The Mac and Windows-compatible board has a solid build, low profile switches, RGB lighting, comfortable tenting and a few extra programmable keys. Goldtouch Elite Adjustable I remember wondering if something like the Goldtouch Elite Adjustable existed when I first started testing ergonomic keyboards. It didn’t at the time, as far as I could tell, but now a connected yet adjustable split board is indeed a product you can buy. It’s a solidly-built board and the ball joint connecting the two halves feels like it will put up with a lot of use. A squeeze of the lever at the top of the keys lets you set the board just how you like, adjusting both the vertical tenting and the angle between the two halves. There’s no programming to speak of, just the ability to swap a few function keys like print screen and home. Unfortunately, the tenting doesn’t work for me. Because of the extra keys at the outer edges, raising the middle edges upwards lifts the center keys considerably, which brings my wrists and forearms off the desk instead of letting them rest. Holding them like that created extra neck and shoulder strain on my part, which is sort of the opposite of the goal. But if you’re not into tenting anyway and want a flat, Alice-split board with an adjustable splay, this works quite well. Kinesis Form Split Touchpad Keyboard The idea behind the Kinesis Form Split Touchpad Keyboard is pretty ergonomic: put the trackpad between the two halves and minimize travel for your mouse hand. The distance between the two puts your elbows at a comfortable distance and keeps your wrist nearly in-line with your forearms. The build is excellent, with low profile mechanical switches that feel smooth and just the right amount of clacky. The trackpad is responsive, but gestures only work with Windows computers. Even dragging and dropping doesn’t work on a Mac here, so I don’t see Apple users getting much use out of the board. I also found myself wishing for the slightest rotation of the keys — though they’re a good distance apart, a slight angle would keep my wrists fully unbent. There’s no tenting or negative tilt either, both of which could help a bit more, ergonomically speaking. Logitech Wave Keys While it's a perfectly fine and affordable Bluetooth keyboard, the Logitech Wave has minimal ergonomics. The keys rise up slightly in the middle and there's a comfortable wrist rest attached, but the layout is the same as any other keyboard, with no splitting of the keys to open up your arms or keep your wrists straight. Ergonomic keyboard FAQs What kinds of ergonomic keyboard styles are there? Most ergonomic keyboards fall into two categories: fully split which separates the board into two pieces, and unibody split, also known as an Alice design, which angles the keys outward at the bottom. When the keys are rotated outward or split into two halves, it allows for a wider spread between your elbows for a more relaxed typing position. Other ergonomic features, such as thumb clusters, center tenting and negative tilting are sometimes added to either type of board. Which keyboard layout is the most ergonomic? Since every person is different, there’s no one best ergonomic keyboard layout. The standard QWERTY layout is what most people are used to. The Dvorak, Colemak and Workman layouts rearrange the board to put the more commonly used letters closer to the home-key position. All three are intended to minimize your finger movements. That may indeed feel more comfortable and less fatiguing, but people used to the QWERTY layout will likely need to relearn how to type. When do I need a split keyboard? You might feel some relief with a fully split keyboard if you find yourself tensing up at the shoulders as you type on a standard board. Putting some distance between your hands may allow your chest to stay more open, which for some is an easier position to maintain. You may also appreciate being able to place your mouse or trackpad between the two halves of the board to minimize the distance your cursor hand needs to travel. How long does it take to adjust to an ergonomic keyboard? That depends on the type of keyboard. Since the Alice-split design simply rotates the keys apart, typing on it feels fairly similar to the regular keyboards you’re already used to. A fully split board will take a little more adjustment, particularly if it uses thumb clusters. The enter, shift and control buttons may now be operated by your thumbs instead of your other fingers and that can be tough to get used to. It took me a full month to get completely comfortable with a fully split keyboard with thumb clusters. But now, I prefer it to typing on regular boards.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-ergonomic-keyboard-130047982.html?src=rss",
          "feed_position": 44,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2024-03/9aaf3e80-ee04-11ee-bfdf-4cbd60b1e877"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/4VfkSUNLbEJKeO0m863euh/30d0739f8ff325650151b76c51fae3ac/Gemini_Generated_Image_gt46srgt46srgt46.png?w=300&q=30",
      "popularity_score": 2018.9206255555555
    },
    {
      "id": "cluster_3",
      "coverage": 2,
      "updated_at": "Sat, 28 Feb 2026 01:15:01 -0500",
      "title": "Anthropic's dispute with the DOD raises critical questions for US military partners like Nvidia, Google, Amazon, and Palantir, which work closely with Anthropic (Wired)",
      "neutral_headline": "Anthropic Hits Back After US Military Labels It a ‘Supply Chain Risk’",
      "bullet_summary": [
        "Reported by TechMeme, Wired Tech"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260228/p2#a260228p2",
          "published_at": "Sat, 28 Feb 2026 01:15:01 -0500",
          "title": "Anthropic's dispute with the DOD raises critical questions for US military partners like Nvidia, Google, Amazon, and Palantir, which work closely with Anthropic (Wired)",
          "standfirst": "Wired: Anthropic's dispute with the DOD raises critical questions for US military partners like Nvidia, Google, Amazon, and Palantir, which work closely with Anthropic &mdash; Anthropic says it would be &ldquo;legally unsound&rdquo; for the Pentagon to blacklist its technology after talks over military use of its artificial intelligence models broke down.",
          "content": "Wired: Anthropic's dispute with the DOD raises critical questions for US military partners like Nvidia, Google, Amazon, and Palantir, which work closely with Anthropic &mdash; Anthropic says it would be &ldquo;legally unsound&rdquo; for the Pentagon to blacklist its technology after talks over military use of its artificial intelligence models broke down.",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/260228/i2.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/anthropic-supply-chain-risk-shockwaves-silicon-valley/",
          "published_at": "Sat, 28 Feb 2026 03:30:41 +0000",
          "title": "Anthropic Hits Back After US Military Labels It a ‘Supply Chain Risk’",
          "standfirst": "Anthropic says it would be “legally unsound” for the Pentagon to blacklist its technology after talks over military use of its artificial intelligence models broke down.",
          "content": "Anthropic says it would be “legally unsound” for the Pentagon to blacklist its technology after talks over military use of its artificial intelligence models broke down.",
          "feed_position": 0,
          "image_url": "https://media.wired.com/photos/69a23b028618f79c55732aa9/master/pass/Anthropic-Supply-Chain-Risk-Business-2261589216.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260228/i2.jpg",
      "popularity_score": 2018.9042366666667
    },
    {
      "id": "cluster_5",
      "coverage": 2,
      "updated_at": "Fri, 27 Feb 2026 23:15:01 -0500",
      "title": "Source: India issued a blocking order on February 24 to restrict access to developer database service Supabase; the government did not publicly cite a reason (Jagmeet Singh/TechCrunch)",
      "neutral_headline": "India disrupts access to popular developer platform Supabase with blocking order",
      "bullet_summary": [
        "Reported by TechMeme, TechCrunch"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260227/p44#a260227p44",
          "published_at": "Fri, 27 Feb 2026 23:15:01 -0500",
          "title": "Source: India issued a blocking order on February 24 to restrict access to developer database service Supabase; the government did not publicly cite a reason (Jagmeet Singh/TechCrunch)",
          "standfirst": "Jagmeet Singh / TechCrunch: Source: India issued a blocking order on February 24 to restrict access to developer database service Supabase; the government did not publicly cite a reason &mdash; Supabase, a popular developer database platform, is facing disruptions in India &mdash; one of its key markets &mdash; has been blocked in India, TechCrunch has learned.",
          "content": "Jagmeet Singh / TechCrunch: Source: India issued a blocking order on February 24 to restrict access to developer database service Supabase; the government did not publicly cite a reason &mdash; Supabase, a popular developer database platform, is facing disruptions in India &mdash; one of its key markets &mdash; has been blocked in India, TechCrunch has learned.",
          "feed_position": 4,
          "image_url": "http://www.techmeme.com/260227/i44.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/02/27/india-disrupts-access-to-popular-developer-platform-supabase-with-blocking-order/",
          "published_at": "Sat, 28 Feb 2026 03:51:52 +0000",
          "title": "India disrupts access to popular developer platform Supabase with blocking order",
          "standfirst": "India, one of Supabase’s biggest markets, is seeing patchy access after a government block order.",
          "content": "India, one of Supabase’s biggest markets, is seeing patchy access after a government block order.",
          "feed_position": 0
        }
      ],
      "featured_image": "http://www.techmeme.com/260227/i44.jpg",
      "popularity_score": 2016.9042366666667
    },
    {
      "id": "cluster_9",
      "coverage": 2,
      "updated_at": "2026-02-27T21:19:16-05:00",
      "title": "Defense secretary Pete Hegseth designates Anthropic a supply chain risk",
      "neutral_headline": "Defense secretary Pete Hegseth designates Anthropic a supply chain risk",
      "bullet_summary": [
        "Reported by The Verge, TechMeme"
      ],
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/policy/886632/pentagon-designates-anthropic-supply-chain-risk-ai-standoff",
          "published_at": "2026-02-27T21:19:16-05:00",
          "title": "Defense secretary Pete Hegseth designates Anthropic a supply chain risk",
          "standfirst": "Nearly two hours after President Donald Trump announced on Truth Social that he was banning Anthropic products from the federal government, Secretary of Defense Pete Hegseth took it one step further and announced that he was now designating the AI company as a \"supply-chain risk,\" which Anthropic says it is willing to challenge in court. [&#8230;]",
          "content": "US President Donald Trump (R) looks on as US Secretary of Defense Pete Hegseth speaks to the press following US military actions in Venezuela | AFP via Getty Images Nearly two hours after President Donald Trump announced on Truth Social that he was banning Anthropic products from the federal government, Secretary of Defense Pete Hegseth took it one step further and announced that he was now designating the AI company as a \"supply-chain risk,\" which Anthropic says it is willing to challenge in court. The decision could immediately impact numerous major tech companies that use Claude in their line of work for the Pentagon, including Palantir and AWS. It is not immediately clear to what extent the Pentagon may blacklist companies that contract with Claude for other services outside of national security, A … Read the full story at The Verge.",
          "feed_position": 0
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260227/p39#a260227p39",
          "published_at": "Fri, 27 Feb 2026 20:55:01 -0500",
          "title": "Anthropic says it'll challenge \"any supply chain risk designation in court\" and that the designation would only affect contractors' use of Claude on DOD work (Anthropic)",
          "standfirst": "Anthropic: Anthropic says it'll challenge &ldquo;any supply chain risk designation in court&rdquo; and that the designation would only affect contractors' use of Claude on DOD work &mdash; Earlier today, Secretary of War Pete Hegseth shared on X that he is directing the Department of War to designate Anthropic a supply chain risk.",
          "content": "Anthropic: Anthropic says it'll challenge &ldquo;any supply chain risk designation in court&rdquo; and that the designation would only affect contractors' use of Claude on DOD work &mdash; Earlier today, Secretary of War Pete Hegseth shared on X that he is directing the Department of War to designate Anthropic a supply chain risk.",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/260227/i39.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260227/i39.jpg",
      "popularity_score": 2014.97507
    },
    {
      "id": "cluster_21",
      "coverage": 2,
      "updated_at": "Fri, 27 Feb 2026 23:00:54 +0000",
      "title": "OpenAI fires employee for using confidential info on prediction markets",
      "neutral_headline": "OpenAI fires employee for using confidential info on prediction markets",
      "bullet_summary": [
        "The company said such trades violates its internal company policies about using confidential information for personal gain",
        "Reported by TechCrunch, Wired Tech"
      ],
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/02/27/openai-fires-employee-for-using-confidential-info-on-prediction-markets/",
          "published_at": "Fri, 27 Feb 2026 23:00:54 +0000",
          "title": "OpenAI fires employee for using confidential info on prediction markets",
          "standfirst": "The company said such trades violates its internal company policies about using confidential information for personal gain.",
          "content": "The company said such trades violates its internal company policies about using confidential information for personal gain.",
          "feed_position": 1
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/openai-fires-employee-insider-trading-polymarket-kalshi/",
          "published_at": "Fri, 27 Feb 2026 19:07:28 +0000",
          "title": "OpenAI Fires an Employee for Prediction Market Insider Trading",
          "standfirst": "Prediction markets like Polymarket and Kalshi are big business, and some Big Tech employees are testing boundaries by making trades based on insider knowledge.",
          "content": "Prediction markets like Polymarket and Kalshi are big business, and some Big Tech employees are testing boundaries by making trades based on insider knowledge.",
          "feed_position": 6,
          "image_url": "https://media.wired.com/photos/69a0b01c157af8f83feddf9b/master/pass/OpenAI-Employee-Fired-Insider-Trading-Business-2210029299.jpg"
        }
      ],
      "featured_image": "https://media.wired.com/photos/69a0b01c157af8f83feddf9b/master/pass/OpenAI-Employee-Fired-Insider-Trading-Business-2210029299.jpg",
      "popularity_score": 2011.668958888889
    },
    {
      "id": "cluster_38",
      "coverage": 2,
      "updated_at": "2026-02-27T14:06:25-05:00",
      "title": "CISA is getting a new acting director after less than a year",
      "neutral_headline": "CISA is getting a new acting director after less than a year",
      "bullet_summary": [
        "Reported by The Verge, TechCrunch"
      ],
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/policy/886316/acting-cisa-director-replaced-madhu-gottumukkala",
          "published_at": "2026-02-27T14:06:25-05:00",
          "title": "CISA is getting a new acting director after less than a year",
          "standfirst": "The US Cybersecurity and Infrastructure Security Agency (CISA), which is part of the Department of Homeland Security, is getting a new acting director, as reported by ABC, less than a year after Madhu Gottumukkala took charge of the agency as deputy director and acting director in May 2025. CISA's executive assistant director for cybersecurity, Nick [&#8230;]",
          "content": "The US Cybersecurity and Infrastructure Security Agency (CISA), which is part of the Department of Homeland Security, is getting a new acting director, as reported by ABC, less than a year after Madhu Gottumukkala took charge of the agency as deputy director and acting director in May 2025. CISA's executive assistant director for cybersecurity, Nick Andersen, will become the agency's new acting director while Gottumkkala will now be serving as director of strategic implementation at DHS. The leadership change comes just a month after reports that Gottumkkala uploaded sensitive documents to ChatGPT. Gottumkkala had requested special permiss … Read the full story at The Verge.",
          "feed_position": 6
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/02/27/cisa-replaces-acting-director-gottumukkala-after-a-bumbling-year-on-the-job/",
          "published_at": "Fri, 27 Feb 2026 15:57:02 +0000",
          "title": "CISA replaces acting director after a bumbling year on the job",
          "standfirst": "The U.S. cybersecurity agency's acting director Madhu Gottumukkala will be replaced, after a year of cuts, layoffs, and staff reassignments, and allegations of security lapses and claims he struggled to lead the agency.",
          "content": "The U.S. cybersecurity agency's acting director Madhu Gottumukkala will be replaced, after a year of cuts, layoffs, and staff reassignments, and allegations of security lapses and claims he struggled to lead the agency.",
          "feed_position": 11
        }
      ],
      "popularity_score": 2007.7609033333333
    },
    {
      "id": "cluster_11",
      "coverage": 1,
      "updated_at": "Sat, 28 Feb 2026 01:26:41 +0000",
      "title": "Google quantum-proofs HTTPS by squeezing 2.5kB of data into 64-byte space",
      "neutral_headline": "Google quantum-proofs HTTPS by squeezing 2.5kB of data into 64-byte space",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2026/02/google-is-using-clever-math-to-quantum-proof-https-certificates/",
          "published_at": "Sat, 28 Feb 2026 01:26:41 +0000",
          "title": "Google quantum-proofs HTTPS by squeezing 2.5kB of data into 64-byte space",
          "standfirst": "Merkle Tree Certificate support is already in Chrome. Soon, it will be everywhere.",
          "content": "Google on Friday unveiled its plan for its Chrome browser to secure HTTPS certificates against quantum computer attacks without breaking the Internet. The objective is a tall order. The quantum-resistant cryptographic data needed to transparently publish TLS certificates is roughly 40 times bigger than the classical cryptographic material used today. Today’s X.509 certificates are about 64 bytes in size, and comprise six elliptic curve signatures and two EC public keys. This material can be cracked through the quantum-enabled Shor’s algorithm. Certificates containing the equivalent quantum-resistant cryptographic material are roughly 2.5 kilobytes. All this data must be transmitted when a browser connects to a site. The bigger they come, the slower they move “The bigger you make the certificate, the slower the handshake and the more people you leave behind,” said Bas Westerbaan, principal research engineer at Cloudflare, which is partnering with Google on the transition. “Our problem is we don’t want to leave people behind in this transition.” Speaking to Ars, he said that people will likely disable the new encryption if it slows their browsing. He added that the massive size increase can also degrade “middle boxes,” which sit between browsers and the final site.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/https-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/https-1152x648.jpg",
      "popularity_score": 367.0986811111111
    },
    {
      "id": "cluster_13",
      "coverage": 1,
      "updated_at": "Sat, 28 Feb 2026 00:32:24 +0000",
      "title": "The Air Force's new ICBM is nearly ready to fly, but there’s nowhere to put it",
      "neutral_headline": "The Air Force's new ICBM is nearly ready to fly, but there’s nowhere to put it",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/02/the-air-forces-new-icbm-is-nearly-ready-to-fly-but-theres-nowhere-to-put-them/",
          "published_at": "Sat, 28 Feb 2026 00:32:24 +0000",
          "title": "The Air Force's new ICBM is nearly ready to fly, but there’s nowhere to put it",
          "standfirst": "\"There were assumptions that were made in the strategy that obviously didn’t come to fruition.\"",
          "content": "DENVER—The US Air Force's new Sentinel intercontinental ballistic missile is on track for its first test flight next year, military officials reaffirmed this week. But no one is ready to say when hundreds of new missile silos, dug from the windswept Great Plains, will be finished, how much they cost, or, for that matter, how many nuclear warheads each Sentinel missile could actually carry. The LGM-35A Sentinel will replace the Air Force's Minuteman III fleet, in service since 1970, with the first of the new missiles due to become operational in the early 2030s. But it will take longer than that to build and activate the full complement of Sentinel missiles and the 450 hardened underground silos to house them.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/6404099-1152x648-1772236795.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/6404099-1152x648-1772236795.jpg",
      "popularity_score": 351.1939588888889
    },
    {
      "id": "cluster_23",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 22:39:00 +0000",
      "title": "Under a Paramount-WBD merger, two struggling media giants would unite",
      "neutral_headline": "Under a Paramount-WBD merger, two struggling media giants would unite",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/under-a-paramount-wbd-merger-two-struggling-media-giants-would-unite/",
          "published_at": "Fri, 27 Feb 2026 22:39:00 +0000",
          "title": "Under a Paramount-WBD merger, two struggling media giants would unite",
          "standfirst": "Can two declining companies form a profitable one?",
          "content": "Netflix has dropped out of the bidding war for Warner Bros. Discovery (WBD), making Paramount Skydance the expected owner of WBD. A Paramount-WBD merger remains subject to regulatory approval, but it’s likely that we will see a Paramount-Skydance-Warner-Bros.-Discovery media giant. Such a conglomerate would unite two legacy media companies that have struggled with profitability for years and have strongly invested in streaming and cable. With Paramount inching closer to WBD ownership, let’s look at what the union implies for streaming and cable.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-79075226-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-79075226-1152x648.jpg",
      "popularity_score": 324.30395888888887
    },
    {
      "id": "cluster_29",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 21:27:33 +0000",
      "title": "Photons that aren't actually there influence superconductivity",
      "neutral_headline": "Photons that aren't actually there influence superconductivity",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/02/photons-that-arent-actually-there-influence-superconductivity/",
          "published_at": "Fri, 27 Feb 2026 21:27:33 +0000",
          "title": "Photons that aren't actually there influence superconductivity",
          "standfirst": "Interactions between neighboring materials is mediated by virtual photons.",
          "content": "Despite the headline, this isn't really a story about superconductivity—at least not the superconductivity that people care about, the stuff that doesn't require exotic refrigeration to work. Instead, it's a story about how superconductivity can be used as a test of some of the weirder consequences of quantum mechanics, one that involves non-existent particles of light that still act as if they exist. Researchers have found a way to get these virtual photons to influence the behavior of a superconductor, ultimately making it worse. That may, in the end, tell us something useful about superconductivity, but it'll probably take a little while. Virtual reality The story starts with quantum field theory, which is incredibly complex, but the simplified version is that even empty space is filled with fields that could govern the interactions of any quantum objects in or near that space. You can think of different particles as energetic excitements of these fields—so a photon is simply an energetic state of the quantum field.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/niac_2011_thibeault-1041x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/niac_2011_thibeault-1041x648.jpg",
      "popularity_score": 313.11312555555554
    },
    {
      "id": "cluster_39",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 19:04:23 +0000",
      "title": "The AI apocalypse is nigh in Good Luck, Have Fun, Don't Die",
      "neutral_headline": "The AI apocalypse is nigh in Good Luck, Have Fun, Don't Die",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/02/the-ai-apocalypse-is-nigh-in-good-luck-have-fun-dont-die/",
          "published_at": "Fri, 27 Feb 2026 19:04:23 +0000",
          "title": "The AI apocalypse is nigh in Good Luck, Have Fun, Don't Die",
          "standfirst": "Director Gore Verbinksi and screenwriter Matthew Robinson on the making of this darkly satirical sci-fi film.",
          "content": "We haven't had a new film from Gore Verbinski for nine years. But the director who brought us the first three Pirates of the Caribbean movies, the nightmare-inducing horror of The Ring (2002), and the Oscar-winning hijinks of Rango (2011) is back in peak form with Good Luck, Have Fun, Don't Die. It's a darkly satirical, inventive, and hugely entertaining time-loop adventure that also serves as a cautionary tale about our widespread online technology addiction. (Some spoilers below but no major reveals.) Sam Rockwell stars as an otherwise unnamed man who shows up at a Norms diner in Los Angeles looking like a homeless person but claiming to be a time traveler from an apocalyptic future. He’s there to recruit the locals into his war against a rogue AI, although the diner patrons are understandably dubious about his sanity. (“I come from a nightmare apocalypse,” he assures the crowd about his grubby appearance. “This is the height of f*@ing fashion!”)Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GLHFDD-TOP-1152x617.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GLHFDD-TOP-1152x617.jpg",
      "popularity_score": 305.7270144444444
    },
    {
      "id": "cluster_36",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 19:14:51 +0000",
      "title": "Whoops: US military laser strike takes down CBP drone near Mexican border",
      "neutral_headline": "Whoops: US military laser strike takes down CBP drone near Mexican border",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/whoops-us-military-laser-strike-takes-down-cbp-drone-near-mexican-border/",
          "published_at": "Fri, 27 Feb 2026 19:14:51 +0000",
          "title": "Whoops: US military laser strike takes down CBP drone near Mexican border",
          "standfirst": "Trump admin \"incompetence continues to cause chaos in our skies,\" Duckworth says.",
          "content": "The US military mistakenly shot down a Customs and Border Protection (CBP) drone near the Mexican border in a strike that reportedly used a laser-based anti-drone system. The CBP uses drones to track people crossing the border. \"Congressional aides told Reuters the Pentagon used the high-energy laser system to shoot down a Customs and Border Protection drone near the Mexican border, in an area that often has incursions from Mexican drones used by drug cartels,\" Reuters reported last night. The FAA closed some airspace along the border with Mexico in Fort Hancock, Texas, on Thursday with a notice announcing temporary flight restrictions for special security reasons. The restrictions are in place until June 24 but could be lifted earlier. There are conflicting reports on which day the strike happened, with The New York Times reporting that the strike occurred Thursday and Bloomberg writing that the Federal Aviation Administration (FAA) “was notified Wednesday after the event occurred.”Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/cbp-drone-1152x648-1772218578.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/cbp-drone-1152x648-1772218578.jpg",
      "popularity_score": 300.9014588888889
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 17:21:21 +0000",
      "title": "How strong is New York's \"illegal gambling\" case against Valve's loot boxes?",
      "neutral_headline": "How strong is New York's \"illegal gambling\" case against Valve's loot boxes",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/02/how-strong-is-new-yorks-illegal-gambling-case-against-valves-loot-boxes/",
          "published_at": "Fri, 27 Feb 2026 17:21:21 +0000",
          "title": "How strong is New York's \"illegal gambling\" case against Valve's loot boxes?",
          "standfirst": "Lawyers tell Ars the state has a tough road ahead, even as Valve is uniquely vulnerable.",
          "content": "For years now, Valve fans have been making jokes about the company's slow transition from game maker to glorified digital hat and knife paint marketplace. This week, though, a lawsuit brought by the state of New York argues that Valve's in-game loot box sales amount to an illegal gambling outfit worth tens of billions of dollars. Lawyers who have looked into the particulars of the case tell Ars that the state faces an uphill battle in convincing courts that this portion of Valve's business legally constitutes gambling. That said, there are a few elements of the case that might make Valve legally vulnerable to the state's arguments. What is gambling, anyway? For a game to legally be counted as \"gambling\" in most jurisdictions, it has to pass a three-part test: a player has to pay money (1) for an outcome that's materially determined by chance (2) in the hopes of receiving something of value (3). While buying a key to a loot box in a Valve game easily passes those first two tests, New York's legal case will likely hinge on whether the random cosmetic items players get from those loot boxes constitute \"something of value\" for statutory purposes.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/csgogun-1-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/csgogun-1-1152x648.jpeg",
      "popularity_score": 284.00979222222225
    },
    {
      "id": "cluster_42",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 18:36:04 +0000",
      "title": "Hyperion author Dan Simmons dies from stroke at 77",
      "neutral_headline": "Hyperion author Dan Simmons dies from stroke at 77",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/02/hyperion-author-dan-simmons-dies-from-stroke-at-77/",
          "published_at": "Fri, 27 Feb 2026 18:36:04 +0000",
          "title": "Hyperion author Dan Simmons dies from stroke at 77",
          "standfirst": "I went into Hyperion blind, decades ago, knowing almost nothing about it. I was never the same.",
          "content": "Dan Simmons, the author of more than three dozen books, including the famed Hyperion Cantos, has died from a stroke. He was 77. Simmons, who worked in elementary education before becoming an author in the 1980s, produced a broad portfolio of writing that spanned several genres, including horror fiction, historical fiction, and science fiction. Often, his books included elements of all of these. This obituary will focus on what is generally considered his greatest work, and what I believe is possibly the greatest science fiction novel of all time, Hyperion. Published in 1989, Hyperion is set in a far-flung future in which human settlement spans hundreds of planets. The novel feels both familiar, in that its structure follows Chaucer's Canterbury Tales, and utterly unfamiliar in its strange, far-flung setting.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/06/Hyperion-1152x648-1772217993.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2015/06/Hyperion-1152x648-1772217993.jpg",
      "popularity_score": 277.25507
    },
    {
      "id": "cluster_58",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 16:13:51 +0000",
      "title": "And the award for the most improved EV goes to... the 2026 Toyota bZ",
      "neutral_headline": "And the award for the most improved EV goes to... the 2026 Toyota bZ",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/02/and-the-award-for-the-most-improved-ev-goes-to-the-2026-toyota-bz/",
          "published_at": "Fri, 27 Feb 2026 16:13:51 +0000",
          "title": "And the award for the most improved EV goes to... the 2026 Toyota bZ",
          "standfirst": "Toyota's small electric SUV is much-revised, much more efficient, and much better.",
          "content": "The world's largest automaker has had a somewhat difficult relationship with battery-electric vehicles. Toyota was an early pioneer of hybrid powertrains, and it remains a fan today, often saying that given limited battery supply, it makes sense to build more hybrids than fewer EVs. Its first full BEV had a rocky start, suffering a recall due to improperly attached wheels just as the cars were hitting showrooms. Reviews for the awkwardly named bZ4x were mixed; the car did little to stand out among the competition. Toyota didn't get to be the world's largest automaker by being completely blind to feedback, and last year, it gave its EV platform (called e-TNGA and shared with Lexus and Subaru) a bit of a spiff-up. To start, it simplified the name—the small electric SUV is now just called the bZ. It uses a new 74.7 kWh battery pack, available with either front- or all-wheel-drive powertrains that now use silicon carbide power electronics. And for the North American market, instead of a CCS1 port just behind the front passenger wheel, you'll now see a Tesla-style NACS socket. Our test bZ was the $37,900 XLE FWD Plus, which has the most range of any bZ at 314 miles (505 km), according to the EPA test cycle. When you realize that the pre-facelift version managed just 252 miles (405 km) with 71.4 kWh onboard, the scale of the improvement becomes clear.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/2026-Toyota-bZ-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/2026-Toyota-bZ-1-1152x648.jpg",
      "popularity_score": 263.88479222222225
    },
    {
      "id": "cluster_63",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 15:13:48 +0000",
      "title": "Netflix cedes Warner Bros. Discovery to Paramount: “No longer financially attractive”",
      "neutral_headline": "Netflix cedes Warner Bros. Discovery to Paramount: “No longer financially attractive”",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/netflix-cedes-warner-bros-discovery-to-paramount-no-longer-financially-attractive/",
          "published_at": "Fri, 27 Feb 2026 15:13:48 +0000",
          "title": "Netflix cedes Warner Bros. Discovery to Paramount: “No longer financially attractive”",
          "standfirst": "Netflix shares jumped following the announcement.",
          "content": "Netflix backed out of its deal to acquire Warner Bros. Discovery’s (WBD’s) streaming and movie studios businesses on Thursday night. After increasing its bid for all of WBD by $1 per share on Tuesday, Paramount Skydance is poised to become the new owner of WBD, including Game of Thrones, DC Comics, and other IP, as well as the HBO Max streaming service and cable channels CNN and TBS. Netflix and WBD announced merger intentions on December 5. Netflix was going to pay an equity value of $72 billion, or an approximate total enterprise value of $82.7 billion, for part of WBD. At the time, NBC News reported that WBD’s total market value was $60 billion. But Paramount has reportedly been eyeing WBD for years and followed December's merger announcement with an aggressive hostile takeover bid. On Tuesday, in addition to raising its offer to buy all of WBD, Paramount also agreed to pay a $7 billion regulatory termination fee should a Paramount-WBD merger fail to close due to antitrust regulation, as well as a $0.25 per share ticking fee for every quarter that the deal doesn’t close, starting on September 30.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2258061457-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2258061457-1152x648.jpg",
      "popularity_score": 246.88395888888888
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Thu, 26 Feb 2026 17:12:10 +0000",
      "title": "Google reveals Nano Banana 2 AI image model, coming to Gemini today",
      "neutral_headline": "Google reveals Nano Banana 2 AI image model, coming to Gemini today",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/google-releases-nano-banana-2-ai-image-generator-promises-pro-results-with-flash-speed/",
          "published_at": "Thu, 26 Feb 2026 17:12:10 +0000",
          "title": "Google reveals Nano Banana 2 AI image model, coming to Gemini today",
          "standfirst": "Google's new image model replaces the previous versions immediately.",
          "content": "The last year has been big for Google's AI efforts. Its rapid-fire model releases have brought it to parity with the likes of OpenAI and Anthropic and, in some cases, pushed it into the lead. The Nano Banana image generator was emblematic of that trend when it debuted last year, and subsequent updates only made it better. Now, Google has announced yet another update to its image model with Nano Banana 2, which is available starting today. Nano Banana 2 is more accurately known as Gemini 3.1 Flash Image—the previous Nano Banana models were based on the 3.0 branch. According to Google, the new release can deliver results similar to Nano Banana Pro but with the speed of the non-pro Flash variant. Google promises the new image generator will have more advanced world knowledge pulled from the Internet by the Gemini 3.1 LLM. This apparently gives it the necessary information to render objects with greater fidelity and create more accurate infographics. The days of squiggly AI text were already ending, but Google says Nano Banana 2 has Pro-like text accuracy in image outputs.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/NB2_Hero.width-2200.format-webp-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/NB2_Hero.width-2200.format-webp-1152x648.png",
      "popularity_score": 166
    },
    {
      "id": "cluster_71",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 14:19:00 +0000",
      "title": "Block lays off 40% of workforce as it goes all-in on AI tools",
      "neutral_headline": "Block lays off 40% of workforce as it goes all-in on AI tools",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/block-lays-off-40-of-workforce-as-it-goes-all-in-on-ai-tools/",
          "published_at": "Fri, 27 Feb 2026 14:19:00 +0000",
          "title": "Block lays off 40% of workforce as it goes all-in on AI tools",
          "standfirst": "CEO says \"most companies are late\" to realize how much technology will affect employment.",
          "content": "Block, the fintech group headed by Twitter cofounder Jack Dorsey, will cut its workforce by “nearly half” in one of the clearest signs of the sweeping changes AI tools are having on employment. Shares in the payment company soared more than 25 percent in after-hours trading on Thursday as it announced it would shed more than 4,000 jobs from its 10,000-strong workforce. “Intelligence tools have changed what it means to build and run a company. We’re already seeing it internally,” Dorsey wrote in a letter to shareholders.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/jack-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/jack-1152x648.jpg",
      "popularity_score": 150.97062555555556
    },
    {
      "id": "cluster_112",
      "coverage": 1,
      "updated_at": "Thu, 26 Feb 2026 22:53:18 +0000",
      "title": "Perplexity announces \"Computer,\" an AI agent that assigns work to other AI agents",
      "neutral_headline": "Perplexity announces \"Computer,\" an AI agent that assigns work to other AI agents",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/perplexity-announces-computer-an-ai-agent-that-assigns-work-to-other-ai-agents/",
          "published_at": "Thu, 26 Feb 2026 22:53:18 +0000",
          "title": "Perplexity announces \"Computer,\" an AI agent that assigns work to other AI agents",
          "standfirst": "It's also a buttoned-down, ostensibly safer take on the OpenClaw concept.",
          "content": "Perplexity has introduced \"Computer,\" a new tool that allows users to assign tasks and see them carried out by a system that coordinates multiple agents running various models. The company claims that Computer, currently available to Perplexity Max subscribers, is \"a system that creates and executes entire workflows\" and \"capable of running for hours or even months.\" The idea is that the user describes a specific outcome—something like \"plan and execute a local digital marketing campaign for my restaurant\" or \"build me an Android app that helps me do a specific kind of research for my job.\" Computer then ideates subtasks and assigns them to multiple agents as needed, running the models Perplexity deems best for those tasks.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/Perplexity-Computer-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/Perplexity-Computer-1152x648.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_114",
      "coverage": 1,
      "updated_at": "Thu, 26 Feb 2026 22:19:39 +0000",
      "title": "xAI spent $7M building wall that barely muffles annoying power plant noise",
      "neutral_headline": "XAI spent $7M building wall that barely muffles annoying power plant noise",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/pops-whines-and-roars-xai-accused-of-torturing-neighbors-of-noisy-power-plant/",
          "published_at": "Thu, 26 Feb 2026 22:19:39 +0000",
          "title": "xAI spent $7M building wall that barely muffles annoying power plant noise",
          "standfirst": "“Temu sound wall” not enough to quell fury over xAI’s power plant.",
          "content": "For miles around xAI's makeshift power plant in Southaven, Mississippi, neighbors have endured months of constant roaring, erupting pops, and bursts of high-pitched whining from 27 temporary gas turbines installed without consulting the community. In a report on Thursday, NBC News interviewed residents fighting to shut down xAI's turbines. They confirmed that xAI operates the turbines day and night, allegedly tormenting residents in order to power xAI founder Elon Musk's unbridled AI ambitions. Eventually, 41 permanent gas turbines—that supposedly won't be as noisy—will be installed, if xAI can secure the permitting. In the meantime, xAI has erected a $7 million \"sound barrier\" that's supposed to mitigate some of the noise.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1768218692.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1768218692.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_75",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 13:37:42 +0000",
      "title": "Apple says it has \"a big week ahead.\" Here's what we expect to see.",
      "neutral_headline": "Apple says it has \"a big week ahead.\" Here's what we expect to see.",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/what-new-hardware-to-expect-from-apple-next-week/",
          "published_at": "Fri, 27 Feb 2026 13:37:42 +0000",
          "title": "Apple says it has \"a big week ahead.\" Here's what we expect to see.",
          "standfirst": "Apple is taking an \"ain't broke/don't fix\" approach to most of its gadgets.",
          "content": "Excepting the AirTag 2, so far it's been a quiet year for Apple hardware. But that's poised to change next week, as the company is hosting a \"special experience\" on March 4. The use of the word experience, rather than event or presentation, implies that Apple’s typical presentation format won't apply here. And CEO Tim Cook more or less confirmed this when he posted that the company had \"a big week ahead,\" starting on Monday. Apple is most likely planning multiple days of product launches announced via press release on its Newsroom site, with the “experience” on Wednesday serving as a capper and a hands-on session for the media. Apple has used a similar strategy before, spacing out relatively low-key refreshes over several days to generate sustained interest rather than dropping everything in a single 30- to 60-minute string of pre-recorded videos.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/DSC_5638-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/DSC_5638-1152x648.jpg",
      "popularity_score": 147.28229222222222
    },
    {
      "id": "cluster_64",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 15:08:08 +0000",
      "title": "NASA shakes up its Artemis program to speed up lunar return",
      "neutral_headline": "NASA shakes up its Artemis program to speed up lunar return",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/staff/2026/02/nasa-shakes-up-its-artemis-program-to-speed-up-lunar-return/",
          "published_at": "Fri, 27 Feb 2026 15:08:08 +0000",
          "title": "NASA shakes up its Artemis program to speed up lunar return",
          "standfirst": "\"Launching SLS every three and a half years or so is not a recipe for success.\"",
          "content": "NASA Administrator Jared Isaacman announced sweeping changes to the Artemis program on Friday morning, including an increased cadence of missions and cancellation of an expensive rocket stage. The upheaval comes as NASA has struggled to fuel the massive Space Launch System rocket for the upcoming Artemis II lunar mission, and Isaacman has sought to revitalize an agency that has moved at a glacial pace on its deep space programs. There is ever-increasing concern that, absent a shake-up, China's rising space program will land humans on the Moon before NASA can return there this decade with Artemis. \"NASA must standardize its approach, increase flight rate safely, and execute on the president’s national space policy,\" Isaacman said. \"With credible competition from our greatest geopolitical adversary increasing by the day, we need to move faster, eliminate delays, and achieve our objectives.\"Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/eus_art-1152x648-1753395940.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/eus_art-1152x648-1753395940.jpg",
      "popularity_score": 136.78951444444445
    },
    {
      "id": "cluster_86",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 12:00:13 +0000",
      "title": "Rocket Report: Vulcan \"many months\" from flying; Falcon 9 extends reuse milestone",
      "neutral_headline": "Rocket Report: Vulcan \"many months\" from flying; Falcon 9 extends reuse milestone",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/02/rocket-report-neutron-launch-date-is-delayed-again-vector-launch-is-back-sort-of/",
          "published_at": "Fri, 27 Feb 2026 12:00:13 +0000",
          "title": "Rocket Report: Vulcan \"many months\" from flying; Falcon 9 extends reuse milestone",
          "standfirst": "\"As the original architect of Vector’s vision, it’s deeply meaningful to bring these assets home.\"",
          "content": "Welcome to Edition 8.31 of the Rocket Report! We have some late-breaking news this week with an update Thursday afternoon from Rocket Lab on the timing of its much-anticipated Neutron rocket. Following the failure of a first stage tank during testing, the company is pushing the medium-lift rocket's debut into the fourth quarter of this year. Effectively that probably means 2027 for the booster, which is disappointing because we all very much want to see another reusable rocket take flight. As always, we welcome reader submissions, and if you don't want to miss an issue, please subscribe using the box below (the form will not appear on AMP-enabled versions of the site). Each report will include information on small-, medium-, and heavy-lift rockets as well as a quick look ahead at the next three launches on the calendar. The ghost of Vector lives on. Tucson, Arizona-based satellite and rocket developer Phantom Space, co-founded by Jim Cantrell in 2019, has acquired the remnants of Vector Launch, Space News reports. The announcement is notable because Cantrell left Vector as its finances deteriorated in 2019. Cantrell said some of the assets, comprising flight-proven design elements, engineering data, and other technology originally developed for Vector, will be immediately integrated into Phantom’s Daytona vehicle architecture to reduce development risk.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Neutron-Hungry-Hippo-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Neutron-Hungry-Hippo-1152x648.jpg",
      "popularity_score": 133.65757
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Fri, 27 Feb 2026 14:34:45 +0000",
      "title": "How to downgrade from macOS 26 Tahoe on a new Mac",
      "neutral_headline": "How to downgrade from macOS 26 Tahoe on a new Mac",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/how-to-downgrade-from-macos-26-tahoe-on-a-new-mac/",
          "published_at": "Fri, 27 Feb 2026 14:34:45 +0000",
          "title": "How to downgrade from macOS 26 Tahoe on a new Mac",
          "standfirst": "Most new Macs can still be downgraded with few downsides. Here's what to know.",
          "content": "An Ars Technica colleague recently bought a new M4 MacBook Air. I have essentially nothing bad to say about this hardware, except to point out that even in our current memory shortage apocalypse, Apple is still charging higher-than-market-rates for RAM and SSD upgrades. Still, most people buying this laptop will have a perfectly nice time with it. But for this colleague, it was also their first interaction with macOS 26 Tahoe and the Liquid Glass redesign, the Mac's first major software design update since the Apple Silicon era began with macOS 11 Big Sur in 2020. Negative consumer reaction to Liquid Glass has been overstated by some members of the Apple enthusiast media ecosystem, and Apple's data shows that iOS 26 adoption rates are roughly in line with those of the last few years. But the Mac's foray into Liquid Glass has drawn particular ire from longtime users (developers Jeff Johnson and Norbert Heger have been tracking persistently weird Finder and window resizing behavior, to pick two concrete examples, and Daring Fireball's John Gruber has encouraged users not to upgrade).Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/tahoe-goodbye-imac-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/tahoe-goodbye-imac-1152x648.jpg",
      "popularity_score": 133.23312555555555
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Thu, 26 Feb 2026 23:16:46 +0000",
      "title": "Neanderthals seemed to have a thing for modern human women",
      "neutral_headline": "Neanderthals seemed to have a thing for modern human women",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/02/genomes-chart-the-history-of-neanderthal-modern-human-interactions/",
          "published_at": "Thu, 26 Feb 2026 23:16:46 +0000",
          "title": "Neanderthals seemed to have a thing for modern human women",
          "standfirst": "\"Neanderthal deserts\" in our genomes suggest a strong pattern in matings.",
          "content": "By now, it's firmly established that modern humans and their Neanderthal relatives met and mated as our ancestors expanded out of Africa, resulting in a substantial amount of Neanderthal DNA scattered throughout our genome. Less widely recognized is that some of the Neanderthal genomes we've seen have pieces of modern human DNA as well. Not every modern human has the same set of Neanderthal DNA, however; different people will, by chance, have inherited different fragments. But there are also some areas, termed \"Neanderthal deserts,\" where none of the Neanderthal DNA seems to have persisted. Notably, the largest Neanderthal desert is the entire X chromosome, raising questions about whether this reflects the evolutionary fitness of genes there or mating preferences. Now, three researchers at the University of Pennsylvania, Alexander Platt, Daniel N. Harris, and Sarah Tishkoff, have done the converse analysis: examining the X chromosomes of the handful of completed Neanderthal genomes we have. It turns out there's also a strong bias toward modern human sequences there, as well, and the authors interpret that as selective mating, with Neanderthal males showing a strong preference for modern human females and their descendants.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1243699616-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1243699616-1024x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Thu, 26 Feb 2026 21:48:37 +0000",
      "title": "The physics of squeaking sneakers",
      "neutral_headline": "The physics of squeaking sneakers",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/02/the-physics-of-squeaking-sneakers/",
          "published_at": "Thu, 26 Feb 2026 21:48:37 +0000",
          "title": "The physics of squeaking sneakers",
          "standfirst": "Geometry of tread patterns determines frequency, so blocks were designed to play Star Wars music.",
          "content": "We're all familiar with the high-pitched squeak of basketball shoes on the court during games, or tires squealing on pavement. Scientists conducted several experiments and discovered that the geometry of the sneakers' tread patterns determines the squeak's frequency, enabling the team to make rubber blocks set to specific frequencies and slide them across glass surfaces to play Star Wars' \"Imperial March.\" \"Tuning frictional behavior on the fly has been a long-standing engineering dream,\" said co-author Katia Bertoldi of Harvard University. \"This new insight into how surface geometry governs slip pulses paves the way for tunable frictional metamaterials that can transition from low-friction to high-grip states on demand.” In addition, the dynamics revealed by these results are similar to those of tectonic faults and thus give scientists a new model for the mechanics of earthquakes, according to their new paper published in the journal Nature. Leonardo da Vinci is usually credited with conducting the first systematic study of friction in the late 15th century, a subfield now known as tribology that deals with the dynamics of interacting surfaces in relative motion. Da Vinci's notebooks depict how he pulled rows of blocks using weights and pulleys, an approach that is still used in frictional studies today, as well as examining the friction produced in screw threads, wheels, and axles. The authors of this latest paper used an experimental setup similar to da Vinci's.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/squeak3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/squeak3-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}