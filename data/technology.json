{
  "updated_at": "2025-12-09T19:11:14.088Z",
  "clusters": [
    {
      "id": "cluster_13",
      "coverage": 3,
      "updated_at": "Tue, 09 Dec 2025 13:15:01 -0500",
      "title": "Google Photos launches new video editing tools, including specialized templates with preset music and text overlays, as well as a redesigned video editor (Lauren Forristal/TechCrunch)",
      "neutral_headline": "Google Photos&#8217; new video editor makes it more like Capcut",
      "bullet_summary": [
        "Reported by TechMeme, The Verge, TechCrunch"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p34#a251209p34",
          "published_at": "Tue, 09 Dec 2025 13:15:01 -0500",
          "title": "Google Photos launches new video editing tools, including specialized templates with preset music and text overlays, as well as a redesigned video editor (Lauren Forristal/TechCrunch)",
          "standfirst": "Lauren Forristal / TechCrunch: Google Photos launches new video editing tools, including specialized templates with preset music and text overlays, as well as a redesigned video editor &mdash; Google Photos announced on Tuesday the launch of new features aimed at making video editing and highlight reel creation easier.",
          "content": "Lauren Forristal / TechCrunch: Google Photos launches new video editing tools, including specialized templates with preset music and text overlays, as well as a redesigned video editor &mdash; Google Photos announced on Tuesday the launch of new features aimed at making video editing and highlight reel creation easier.",
          "feed_position": 3,
          "image_url": "http://www.techmeme.com/251209/i34.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/840690/google-photos-video-editing-highlight-reel-updates",
          "published_at": "2025-12-09T13:01:00-05:00",
          "title": "Google Photos&#8217; new video editor makes it more like Capcut",
          "standfirst": "The Google Photos app on Android and iOS is getting a video editing overhaul with a redesigned editor, templates for highlight reels, and options for adding custom text overlays and music to individual videos. Highlight reels, which automatically cut together a video featuring selected photos and videos, were already part of Google Photos, but now [&#8230;]",
          "content": "The Google Photos app on Android and iOS is getting a video editing overhaul with a redesigned editor, templates for highlight reels, and options for adding custom text overlays and music to individual videos. Highlight reels, which automatically cut together a video featuring selected photos and videos, were already part of Google Photos, but now you'll have more options for creating them. The previous version of the highlight reel feature had fairly limited editing and style options, and simply compiled selected photos and videos into a basic mashup video. The new themed templates have built-in music and text overlays that your photos an … Read the full story at The Verge.",
          "feed_position": 6
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/09/google-photos-launches-new-video-editing-tools/",
          "published_at": "Tue, 09 Dec 2025 18:00:00 +0000",
          "title": "Google Photos launches new video editing tools",
          "standfirst": "Google Photos' new update helps make editing video reels in the app quicker, such as new templates that already add music and text to clips.",
          "content": "Google Photos' new update helps make editing video reels in the app quicker, such as new templates that already add music and text to clips.",
          "feed_position": 3
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i34.jpg",
      "popularity_score": 3019.0630311111113
    },
    {
      "id": "cluster_25",
      "coverage": 3,
      "updated_at": "Tue, 09 Dec 2025 17:28:36 +0000",
      "title": "OpenAI, Anthropic, and Block join new Linux Foundation effort to standardize the AI agent era",
      "neutral_headline": "OpenAI, Anthropic, and Block Are Teaming Up to Make AI Agents Play Nice",
      "bullet_summary": [
        "Anthropic, Block, and OpenAI are backing the Linux Foundation’s new Agentic AI Foundation, donating MCP, Goose, and AGENTS",
        "Reported by TechCrunch, TechMeme, Wired Tech"
      ],
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/09/openai-anthropic-and-block-join-new-linux-foundation-effort-to-standardize-the-ai-agent-era/",
          "published_at": "Tue, 09 Dec 2025 17:28:36 +0000",
          "title": "OpenAI, Anthropic, and Block join new Linux Foundation effort to standardize the AI agent era",
          "standfirst": "Anthropic, Block, and OpenAI are backing the Linux Foundation’s new Agentic AI Foundation, donating MCP, Goose, and AGENTS.md to standardize AI agents, boost interoperability, and curb proprietary fragmentation.",
          "content": "Anthropic, Block, and OpenAI are backing the Linux Foundation’s new Agentic AI Foundation, donating MCP, Goose, and AGENTS.md to standardize AI agents, boost interoperability, and curb proprietary fragmentation.",
          "feed_position": 5
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p30#a251209p30",
          "published_at": "Tue, 09 Dec 2025 12:20:00 -0500",
          "title": "Anthropic, OpenAI, Block, Google, AWS, Microsoft, and others launch the Agentic AI Foundation to build open-source agent standards under the Linux Foundation (Will Knight/Wired)",
          "standfirst": "Will Knight / Wired: Anthropic, OpenAI, Block, Google, AWS, Microsoft, and others launch the Agentic AI Foundation to build open-source agent standards under the Linux Foundation &mdash; American AI giants are backing a new effort to establish open standards for building agentic software and tools.",
          "content": "Will Knight / Wired: Anthropic, OpenAI, Block, Google, AWS, Microsoft, and others launch the Agentic AI Foundation to build open-source agent standards under the Linux Foundation &mdash; American AI giants are backing a new effort to establish open standards for building agentic software and tools.",
          "feed_position": 7,
          "image_url": "http://www.techmeme.com/251209/i30.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/openai-anthropic-and-block-are-teaming-up-on-ai-agent-standards/",
          "published_at": "Tue, 09 Dec 2025 17:06:58 +0000",
          "title": "OpenAI, Anthropic, and Block Are Teaming Up to Make AI Agents Play Nice",
          "standfirst": "American AI giants are backing a new effort to establish open standards for building agentic software and tools.",
          "content": "American AI giants are backing a new effort to establish open standards for building agentic software and tools.",
          "feed_position": 3,
          "image_url": "https://media.wired.com/photos/693854c57eeef7f25b0423c2/master/pass/ai-agents-play-nice-1.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i30.jpg",
      "popularity_score": 3018.28942
    },
    {
      "id": "cluster_8",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 13:40:01 -0500",
      "title": "Denise Dresser, Slack CEO since 2023, is stepping down to join OpenAI as the company's chief revenue officer (Wired)",
      "neutral_headline": "OpenAI Hires Slack CEO as New Chief Revenue Officer",
      "bullet_summary": [
        "Reported by TechMeme, Wired Tech"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p36#a251209p36",
          "published_at": "Tue, 09 Dec 2025 13:40:01 -0500",
          "title": "Denise Dresser, Slack CEO since 2023, is stepping down to join OpenAI as the company's chief revenue officer (Wired)",
          "standfirst": "Wired: Denise Dresser, Slack CEO since 2023, is stepping down to join OpenAI as the company's chief revenue officer &mdash; A memo obtained by WIRED confirms Denise Dresser's departure from Slack. She is now headed to OpenAI. &mdash; Slack CEO Denise Dresser is leaving the company and joining OpenAI &hellip;",
          "content": "Wired: Denise Dresser, Slack CEO since 2023, is stepping down to join OpenAI as the company's chief revenue officer &mdash; A memo obtained by WIRED confirms Denise Dresser's departure from Slack. She is now headed to OpenAI. &mdash; Slack CEO Denise Dresser is leaving the company and joining OpenAI &hellip;",
          "feed_position": 1,
          "image_url": "http://www.techmeme.com/251209/i36.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/slack-ceo-denise-dresser-joins-openai-chief-revenue-officer/",
          "published_at": "Tue, 09 Dec 2025 18:35:01 +0000",
          "title": "OpenAI Hires Slack CEO as New Chief Revenue Officer",
          "standfirst": "A memo obtained by WIRED confirms Denise Dresser's departure from Slack. She is now headed to OpenAI.",
          "content": "A memo obtained by WIRED confirms Denise Dresser's departure from Slack. She is now headed to OpenAI.",
          "feed_position": 0,
          "image_url": "https://media.wired.com/photos/693859e8da23a81834d81ac9/master/pass/OpenAI-Hires-Slack-CEO-Denise-Dresser-as-Chief-Revenue-Officer-Business-2181104969.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i36.jpg",
      "popularity_score": 2019.4796977777778
    },
    {
      "id": "cluster_17",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 12:52:05 -0500",
      "title": "Sources: OpenAI has become more guarded about publishing research on AI's economic harms, prompting at least two economic research staffers to leave (Maxwell Zeff/Wired)",
      "neutral_headline": "OpenAI Staffer Quits, Alleging Company’s Economic Research Is...",
      "bullet_summary": [
        "Reported by TechMeme, Wired Tech"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p33#a251209p33",
          "published_at": "Tue, 09 Dec 2025 12:52:05 -0500",
          "title": "Sources: OpenAI has become more guarded about publishing research on AI's economic harms, prompting at least two economic research staffers to leave (Maxwell Zeff/Wired)",
          "standfirst": "Maxwell Zeff / Wired: Sources: OpenAI has become more guarded about publishing research on AI's economic harms, prompting at least two economic research staffers to leave &mdash; Four sources close to the situation claim OpenAI has become hesitant to publish research on the negative impact of AI.",
          "content": "Maxwell Zeff / Wired: Sources: OpenAI has become more guarded about publishing research on AI's economic harms, prompting at least two economic research staffers to leave &mdash; Four sources close to the situation claim OpenAI has become hesitant to publish research on the negative impact of AI.",
          "feed_position": 4,
          "image_url": "http://www.techmeme.com/251209/i33.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/openai-economic-research-team-ai-jobs/",
          "published_at": "Tue, 09 Dec 2025 17:30:00 +0000",
          "title": "OpenAI Staffer Quits, Alleging Company’s Economic Research Is Drifting Into AI Advocacy",
          "standfirst": "Four sources close to the situation claim OpenAI has become hesitant to publish research on the negative impact of AI. The company says it has only expanded the economic research team’s scope.",
          "content": "Four sources close to the situation claim OpenAI has become hesitant to publish research on the negative impact of AI. The company says it has only expanded the economic research team’s scope.",
          "feed_position": 2,
          "image_url": "https://media.wired.com/photos/69377ab2ceb33c37ef2a4915/master/pass/business_openai_economic_impact.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i33.jpg",
      "popularity_score": 2018.6808088888888
    },
    {
      "id": "cluster_48",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 16:00:00 GMT",
      "title": "Databricks' OfficeQA uncovers disconnect: AI agents ace abstract tests but stall at 45% on enterprise docs",
      "neutral_headline": "Brand-context AI: The missing requirement for marketing AI",
      "bullet_summary": [
        "\"We come from a pretty heavy science or engineering background, and sometimes we create evals that reflect that,\" Elsen said",
        "Elsen said the benchmark provides \"a really strong feedback signal\" for developers working on search solutions",
        "\"At least in my imagination, the goal of releasing this is more as an eval and not as a source of raw training data,\" he said",
        "Technically, the ring is said to support roughly 12 to 14 hours of recording"
      ],
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/databricks-officeqa-uncovers-disconnect-ai-agents-ace-abstract-tests-but",
          "published_at": "Tue, 09 Dec 2025 16:00:00 GMT",
          "title": "Databricks' OfficeQA uncovers disconnect: AI agents ace abstract tests but stall at 45% on enterprise docs",
          "standfirst": "There is no shortage of AI benchmarks in the market today, with popular options like Humanity&#x27;s Last Exam (HLE), ARC-AGI-2 and GDPval, among numerous others.AI agents excel at solving abstract math problems and passing PhD-level exams that most benchmarks are based on, but Databricks has a question for the enterprise: Can they actually handle the document-heavy work most enterprises need them to do?The answer, according to new research from the data and AI platform company, is sobering. Even the best-performing AI agents achieve less than 45% accuracy on tasks that mirror real enterprise workloads, exposing a critical gap between academic benchmarks and business reality.\"If we focus our research efforts on getting better at [existing benchmarks], then we&#x27;re probably not solving the right problems to make Databricks a better platform,\" Erich Elsen, principal research scientist at Databricks, explained to VentureBeat. \"So that&#x27;s why we were looking around. How do we create a benchmark that, if we get better at it, we&#x27;re actually getting better at solving the problems that our customers have?\"The result is OfficeQA, a benchmark designed to test AI agents on grounded reasoning: Answering questions based on complex proprietary datasets containing unstructured documents and tabular data. Unlike existing benchmarks that focus on abstract capabilities, OfficeQA proxies for the economically valuable tasks enterprises actually perform.Why academic benchmarks miss the enterprise markThere are numerous shortcomings of popular AI benchmarks from an enterprise perspective, according to Elsen. HLE features questions requiring PhD-level expertise across diverse fields. ARC-AGI evaluates abstract reasoning through visual manipulation of colored grids. Both push the frontiers of AI capabilities, but don&#x27;t reflect daily enterprise work. Even GDPval, which was specifically created to evaluate economically useful tasks, misses the target.\"We come from a pretty heavy science or engineering background, and sometimes we create evals that reflect that,\" Elsen said. \" So they&#x27;re either extremely math-heavy, which is a great, useful task, but advancing the frontiers of human mathematics is not what customers are trying to do with Databricks.\"While AI is commonly used for customer support and coding apps, Databricks&#x27; customer base has a broader set of requirements. Elsen noted that answering questions about documents or corpora of documents is a common enterprise task. These require parsing complex tables with nested headers, retrieving information across dozens or hundreds of documents and performing calculations where a single-digit error can cascade into organizations making incorrect business decisions.Building a benchmark that mirrors enterprise document complexityTo create a meaningful test of grounded reasoning capabilities, Databricks needed a dataset that approximates the messy reality of proprietary enterprise document corpora, while remaining freely available for research. The team landed on U.S. Treasury Bulletins, published monthly for five decades beginning in 1939 and quarterly thereafter.The Treasury Bulletins check every box for enterprise document complexity. Each bulletin runs 100 to 200 pages and consists of prose, complex tables, charts and figures describing Treasury operations: Where federal money came from, where it went and how it financed government operations. The corpus spans approximately 89,000 pages across eight decades. Until 1996, the bulletins were scans of physical documents; afterwards, they were digitally produced PDFs. USAFacts, an organization whose mission is \"to make government data easier to access and understand,\" partnered with Databricks to develop the benchmark, identifying Treasury Bulletins as ideal and ensuring questions reflected realistic use cases.The 246 questions require agents to handle messy, real-world document challenges: Scanned images, hierarchical table structures, temporal data spanning multiple reports and the need for external knowledge like inflation adjustments. Questions range from simple value lookups to multi-step analysis requiring statistical calculations and cross-year comparisons.To ensure the benchmark requires actual document-grounded retrieval, Databricks filtered out questions that LLMs could answer using parametric knowledge or web search alone. This removed simpler questions and some surprisingly complex ones where models leveraged historical financial records memorized during pre-training.Every question has a validated ground truth answer (typically a number, sometimes dates or small lists), enabling automated evaluation without human judging. This design choice matters: It allows reinforcement learning (RL) approaches that require verifiable rewards, similar to how models train on coding problems.Current performance exposes fundamental gapsDatabricks tested Claude Opus 4.5 Agent (using Claude&#x27;s SDK) and GPT-5.1 Agent (using OpenAI&#x27;s File Search API). The results should give pause to any enterprise betting heavily on current agent capabilities.When provided with raw PDF documents: Claude Opus 4.5 Agent (with default thinking=high) achieved 37.4% accuracy. GPT-5.1 Agent (with reasoning_effort=high) achieved 43.5% accuracy. However, performance improved noticeably when provided with pre-parsed versions of pages using Databricks&#x27; ai_parse_document, indicating that the poor raw PDF performance stems from LLM APIs struggling with parsing rather than reasoning. Even with parsed documents, the experiments show room for improvement.When provided with documents parsed using Databricks&#x27; ai_parse_document:Claude Opus 4.5 Agent achieved 67.8% accuracy (a +30.4 percentage point improvement)GPT-5.1 Agent achieved a 52.8% accuracy (a +9.3 percentage point improvement)Three findings that matter for enterprise deploymentsThe testing identified critical insights for practitioners:Parsing remains the fundamental blocker: Complex tables with nested headers, merged cells and unusual formatting frequently produce misaligned values. Even when given exact oracle pages, agents struggled primarily due to parsing errors, although performance roughly doubled with pre-parsed documents.Document versioning creates ambiguity: Financial and regulatory documents get revised and reissued, meaning multiple valid answers exist depending on the publication date. Agents often stop searching once they find a plausible answer, missing more authoritative sources.Visual reasoning is a gap: About 3% of questions require chart or graph interpretation, where current agents consistently fail. For enterprises where data visualizations communicate critical insights, this represents a meaningful capability limitation.How enterprises can use OfficeQAThe benchmark&#x27;s design enables specific improvement paths beyond simple scoring. \"Since you&#x27;re able to look at the right answer, it&#x27;s easy to tell if the error is coming from parsing,\" Elsen explained. This automated evaluation enables rapid iteration on parsing pipelines. The verified ground truth answers also enable RL training similar to coding benchmarks, since there&#x27;s no human judgment required.Elsen said the benchmark provides \"a really strong feedback signal\" for developers working on search solutions. However, he cautioned against treating it as training data.\"At least in my imagination, the goal of releasing this is more as an eval and not as a source of raw training data,\" he said. \"If you tune too specifically into this environment, then it&#x27;s not clear how generalizable your agent results would be.\"What this means for enterprise AI deploymentsFor enterprises currently deploying or planning document-heavy AI agent systems, OfficeQA provides a sobering reality check. Even the latest frontier models achieve only 43% accuracy on unprocessed PDFs and fall short of 70% accuracy even with optimal document parsing. Performance on the hardest questions plateaus at 40%, indicating substantial room for improvement.Three immediate implications:Evaluate your document complexity: If your documents resemble the complexity profile of Treasury Bulletins (scanned images, nested table structures, cross-document references), expect accuracy well below vendor marketing claims. Test on your actual documents before production deployment.Plan for the parsing bottleneck: The test results indicate that parsing remains a fundamental blocker. Budget time and resources for custom parsing solutions rather than assuming off-the-shelf OCR will suffice. Plan for hard question failure modes: Even with optimal parsing, agents plateau at 40% on complex multi-step questions. For mission-critical document workflows that require multi-document analysis, statistical calculations or visual reasoning, current agent capabilities may not be ready without significant human oversight.For enterprises looking to lead in AI-powered document intelligence, this benchmark provides a concrete evaluation framework and identifies specific capability gaps that need solving.",
          "content": "There is no shortage of AI benchmarks in the market today, with popular options like Humanity&#x27;s Last Exam (HLE), ARC-AGI-2 and GDPval, among numerous others.AI agents excel at solving abstract math problems and passing PhD-level exams that most benchmarks are based on, but Databricks has a question for the enterprise: Can they actually handle the document-heavy work most enterprises need them to do?The answer, according to new research from the data and AI platform company, is sobering. Even the best-performing AI agents achieve less than 45% accuracy on tasks that mirror real enterprise workloads, exposing a critical gap between academic benchmarks and business reality.\"If we focus our research efforts on getting better at [existing benchmarks], then we&#x27;re probably not solving the right problems to make Databricks a better platform,\" Erich Elsen, principal research scientist at Databricks, explained to VentureBeat. \"So that&#x27;s why we were looking around. How do we create a benchmark that, if we get better at it, we&#x27;re actually getting better at solving the problems that our customers have?\"The result is OfficeQA, a benchmark designed to test AI agents on grounded reasoning: Answering questions based on complex proprietary datasets containing unstructured documents and tabular data. Unlike existing benchmarks that focus on abstract capabilities, OfficeQA proxies for the economically valuable tasks enterprises actually perform.Why academic benchmarks miss the enterprise markThere are numerous shortcomings of popular AI benchmarks from an enterprise perspective, according to Elsen. HLE features questions requiring PhD-level expertise across diverse fields. ARC-AGI evaluates abstract reasoning through visual manipulation of colored grids. Both push the frontiers of AI capabilities, but don&#x27;t reflect daily enterprise work. Even GDPval, which was specifically created to evaluate economically useful tasks, misses the target.\"We come from a pretty heavy science or engineering background, and sometimes we create evals that reflect that,\" Elsen said. \" So they&#x27;re either extremely math-heavy, which is a great, useful task, but advancing the frontiers of human mathematics is not what customers are trying to do with Databricks.\"While AI is commonly used for customer support and coding apps, Databricks&#x27; customer base has a broader set of requirements. Elsen noted that answering questions about documents or corpora of documents is a common enterprise task. These require parsing complex tables with nested headers, retrieving information across dozens or hundreds of documents and performing calculations where a single-digit error can cascade into organizations making incorrect business decisions.Building a benchmark that mirrors enterprise document complexityTo create a meaningful test of grounded reasoning capabilities, Databricks needed a dataset that approximates the messy reality of proprietary enterprise document corpora, while remaining freely available for research. The team landed on U.S. Treasury Bulletins, published monthly for five decades beginning in 1939 and quarterly thereafter.The Treasury Bulletins check every box for enterprise document complexity. Each bulletin runs 100 to 200 pages and consists of prose, complex tables, charts and figures describing Treasury operations: Where federal money came from, where it went and how it financed government operations. The corpus spans approximately 89,000 pages across eight decades. Until 1996, the bulletins were scans of physical documents; afterwards, they were digitally produced PDFs. USAFacts, an organization whose mission is \"to make government data easier to access and understand,\" partnered with Databricks to develop the benchmark, identifying Treasury Bulletins as ideal and ensuring questions reflected realistic use cases.The 246 questions require agents to handle messy, real-world document challenges: Scanned images, hierarchical table structures, temporal data spanning multiple reports and the need for external knowledge like inflation adjustments. Questions range from simple value lookups to multi-step analysis requiring statistical calculations and cross-year comparisons.To ensure the benchmark requires actual document-grounded retrieval, Databricks filtered out questions that LLMs could answer using parametric knowledge or web search alone. This removed simpler questions and some surprisingly complex ones where models leveraged historical financial records memorized during pre-training.Every question has a validated ground truth answer (typically a number, sometimes dates or small lists), enabling automated evaluation without human judging. This design choice matters: It allows reinforcement learning (RL) approaches that require verifiable rewards, similar to how models train on coding problems.Current performance exposes fundamental gapsDatabricks tested Claude Opus 4.5 Agent (using Claude&#x27;s SDK) and GPT-5.1 Agent (using OpenAI&#x27;s File Search API). The results should give pause to any enterprise betting heavily on current agent capabilities.When provided with raw PDF documents: Claude Opus 4.5 Agent (with default thinking=high) achieved 37.4% accuracy. GPT-5.1 Agent (with reasoning_effort=high) achieved 43.5% accuracy. However, performance improved noticeably when provided with pre-parsed versions of pages using Databricks&#x27; ai_parse_document, indicating that the poor raw PDF performance stems from LLM APIs struggling with parsing rather than reasoning. Even with parsed documents, the experiments show room for improvement.When provided with documents parsed using Databricks&#x27; ai_parse_document:Claude Opus 4.5 Agent achieved 67.8% accuracy (a +30.4 percentage point improvement)GPT-5.1 Agent achieved a 52.8% accuracy (a +9.3 percentage point improvement)Three findings that matter for enterprise deploymentsThe testing identified critical insights for practitioners:Parsing remains the fundamental blocker: Complex tables with nested headers, merged cells and unusual formatting frequently produce misaligned values. Even when given exact oracle pages, agents struggled primarily due to parsing errors, although performance roughly doubled with pre-parsed documents.Document versioning creates ambiguity: Financial and regulatory documents get revised and reissued, meaning multiple valid answers exist depending on the publication date. Agents often stop searching once they find a plausible answer, missing more authoritative sources.Visual reasoning is a gap: About 3% of questions require chart or graph interpretation, where current agents consistently fail. For enterprises where data visualizations communicate critical insights, this represents a meaningful capability limitation.How enterprises can use OfficeQAThe benchmark&#x27;s design enables specific improvement paths beyond simple scoring. \"Since you&#x27;re able to look at the right answer, it&#x27;s easy to tell if the error is coming from parsing,\" Elsen explained. This automated evaluation enables rapid iteration on parsing pipelines. The verified ground truth answers also enable RL training similar to coding benchmarks, since there&#x27;s no human judgment required.Elsen said the benchmark provides \"a really strong feedback signal\" for developers working on search solutions. However, he cautioned against treating it as training data.\"At least in my imagination, the goal of releasing this is more as an eval and not as a source of raw training data,\" he said. \"If you tune too specifically into this environment, then it&#x27;s not clear how generalizable your agent results would be.\"What this means for enterprise AI deploymentsFor enterprises currently deploying or planning document-heavy AI agent systems, OfficeQA provides a sobering reality check. Even the latest frontier models achieve only 43% accuracy on unprocessed PDFs and fall short of 70% accuracy even with optimal document parsing. Performance on the hardest questions plateaus at 40%, indicating substantial room for improvement.Three immediate implications:Evaluate your document complexity: If your documents resemble the complexity profile of Treasury Bulletins (scanned images, nested table structures, cross-document references), expect accuracy well below vendor marketing claims. Test on your actual documents before production deployment.Plan for the parsing bottleneck: The test results indicate that parsing remains a fundamental blocker. Budget time and resources for custom parsing solutions rather than assuming off-the-shelf OCR will suffice. Plan for hard question failure modes: Even with optimal parsing, agents plateau at 40% on complex multi-step questions. For mission-critical document workflows that require multi-document analysis, statistical calculations or visual reasoning, current agent capabilities may not be ready without significant human oversight.For enterprises looking to lead in AI-powered document intelligence, this benchmark provides a concrete evaluation framework and identifies specific capability gaps that need solving.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/11CUQal9q3dPlFL3fRIjP3/a601024e0be680f9645daaf6198bf0f4/OfficeQA-image-smk.jpg?w=300&q=30"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/09/pebbles-founder-introduces-a-75-ai-smart-ring-for-recording-brief-notes-with-a-press-of-a-button/",
          "published_at": "Tue, 09 Dec 2025 15:00:00 +0000",
          "title": "Pebble&#8217;s founder introduces a $75 AI smart ring for recording brief notes with a press of a button",
          "standfirst": "Named for the finger where the ring is meant to be worn, the new $75 Index 01 ring is not meant to be a competitor to the always-on, always-listening AI devices, like the AI pendant Friend, but instead offers a way to record quick notes and reminders with a press of a button on the ring's side. AI only comes into play via the open source, speech-to-text, and AI models that run locally on your smartphone, via the open source Pebble mobile app. That is, if the Ring's button is not being pressed, it's not recording. (And this is a press-and-hold gesture, too, which means you can't start the ring's recording and then let go to surreptitiously record a conversation.) You can wear the stainless steel ring while in the shower, washing hands, doing dishes, or in the rain, but you would have to take it off for other water-related activities, like swimming. At launch, it's water-resistant to 1 meter. The ring is also not a fitness tracker or sleep monitor. It doesn't record details about your heart rate or health. And it's not there to be your AI friend. \"I'm not trying to build some AI assistant thing,\" Migicovsky told TechCrunch in an interview. \"I build things that solve one main problem, and they solve it really well,\" he explains. \"I think of [the ring] as external memory for my brain...that's what this is. It's always with you.\" Plus, the ring has been designed to be highly reliable and privacy-preserving, he says, as all your thoughts are stored on your phone, not in the cloud. There is no subscription. Migicovsky has been wearing the ring for three months now and says he cannot imagine going back to a world where he doesn't always have a memory device with him. \"The problem is that, during the day, I get ideas or I remember something, and if I don't write it down that second, I forget it,\" he says. The ring solves this problem, he adds, without becoming another device you need to charge. \"The battery lasts for years,\" Migicovsky claims. Technically, the ring is said to support roughly 12 to 14 hours of recording. On average, the founder says he uses it 10-20 times per day to record 3-6 second thoughts. At that rate, he'll get about two years of usage. When the ring's battery dies, you can ship it back to the company for recycling. When using Index, you can record up to five minutes of audio, which can be saved to the ring and synced to your phone later. This makes sense for recording briefer, personal thoughts and notes, even when you don't have your phone handy, but it wouldn't work for recording a longer chat, like a presentation, meeting, or in-person interview of some kind. The ring also supports 99+ languages and has a bit of on-device memory, in case you're not in Bluetooth range of your device, where the recording is ultimately saved and transcribed. (The raw audio is retained, too, in case the speech-to-text is garbled due to loud background noise). If you own a Pebble smartwatch or one from another brand, your recorded thought can even appear on the watch's screen so you can verify it's correct. The ring works with Pebble's mobile app, which offers notes and reminders, but can optionally integrate with your phone's calendaring system, too, or other apps, like Notion. And the ring's software is open source, which makes it hackable by the community, the founder points out. Because of its open nature, the ring's button is already programmable. In addition to the press-and-hold gesture, you can program the ring to do other things with a single or double press, like play or pause your music or control the shutter on your phone's camera. You could use it to send a message through the universal chat app Beeper, which Migicovsky also created, or you could add your own voice actions via MCP. A new approach to hardware Migicovsky acknowledges that hardware can be difficult to get right -- as the previous exit of Pebble to Fitbit showed. (Fitbit, too, was later acquired by Google in 2021). \"I didn't earn any money during Pebble -- we exited, but it was not a great exit,\" Migicovsky admits. This year, however, he decided to reboot the Pebble project after Google open sourced the PebbleOS, which opened up the door to new hardware. With his new company, Core Devices, Migicovsky plans to do things differently. Still, the founder doesn't regret his previous choices, he clarifies. \"I wouldn't have gone back and changed anything. I loved what we built. I loved what we did. I love the company that we built, but it's not the only way to build a company,\" he told TechCrunch. \" And, speaking as an ex-YC partner, there are -- there's a time and a place for building a venture-backed startup. Some companies are phenomenal when they raise money and build a big team, and I tried that...I think what I'm doing now is trying an alternative path, which is [to] start from profitability,\" he says. The new company is a small team of five, self-funded, and focused on sustainability. So far, Core Devices has shipped the Pebble 2 Duo smartwatch with a black-and-white display. Its first run sold out, and the company is now preparing to ship the upgraded version, the Pebble Time 2. The newer device, which has seen 25,000 pre-orders, is a stainless steel watch with a larger, color e-ink screen. As for the Index 01, the ring's pre-order offer ends in March 2026. After that, the price increases to $99. It currently comes in silver, polished gold, and matte black and works with iOS and Android devices. Customers can select from eight ring sizes and three colors.",
          "content": "Named for the finger where the ring is meant to be worn, the new $75 Index 01 ring is not meant to be a competitor to the always-on, always-listening AI devices, like the AI pendant Friend, but instead offers a way to record quick notes and reminders with a press of a button on the ring's side. AI only comes into play via the open source, speech-to-text, and AI models that run locally on your smartphone, via the open source Pebble mobile app. That is, if the Ring's button is not being pressed, it's not recording. (And this is a press-and-hold gesture, too, which means you can't start the ring's recording and then let go to surreptitiously record a conversation.) You can wear the stainless steel ring while in the shower, washing hands, doing dishes, or in the rain, but you would have to take it off for other water-related activities, like swimming. At launch, it's water-resistant to 1 meter. The ring is also not a fitness tracker or sleep monitor. It doesn't record details about your heart rate or health. And it's not there to be your AI friend. \"I'm not trying to build some AI assistant thing,\" Migicovsky told TechCrunch in an interview. \"I build things that solve one main problem, and they solve it really well,\" he explains. \"I think of [the ring] as external memory for my brain...that's what this is. It's always with you.\" Plus, the ring has been designed to be highly reliable and privacy-preserving, he says, as all your thoughts are stored on your phone, not in the cloud. There is no subscription. Migicovsky has been wearing the ring for three months now and says he cannot imagine going back to a world where he doesn't always have a memory device with him. \"The problem is that, during the day, I get ideas or I remember something, and if I don't write it down that second, I forget it,\" he says. The ring solves this problem, he adds, without becoming another device you need to charge. \"The battery lasts for years,\" Migicovsky claims. Technically, the ring is said to support roughly 12 to 14 hours of recording. On average, the founder says he uses it 10-20 times per day to record 3-6 second thoughts. At that rate, he'll get about two years of usage. When the ring's battery dies, you can ship it back to the company for recycling. When using Index, you can record up to five minutes of audio, which can be saved to the ring and synced to your phone later. This makes sense for recording briefer, personal thoughts and notes, even when you don't have your phone handy, but it wouldn't work for recording a longer chat, like a presentation, meeting, or in-person interview of some kind. The ring also supports 99+ languages and has a bit of on-device memory, in case you're not in Bluetooth range of your device, where the recording is ultimately saved and transcribed. (The raw audio is retained, too, in case the speech-to-text is garbled due to loud background noise). If you own a Pebble smartwatch or one from another brand, your recorded thought can even appear on the watch's screen so you can verify it's correct. The ring works with Pebble's mobile app, which offers notes and reminders, but can optionally integrate with your phone's calendaring system, too, or other apps, like Notion. And the ring's software is open source, which makes it hackable by the community, the founder points out. Because of its open nature, the ring's button is already programmable. In addition to the press-and-hold gesture, you can program the ring to do other things with a single or double press, like play or pause your music or control the shutter on your phone's camera. You could use it to send a message through the universal chat app Beeper, which Migicovsky also created, or you could add your own voice actions via MCP. A new approach to hardware Migicovsky acknowledges that hardware can be difficult to get right -- as the previous exit of Pebble to Fitbit showed. (Fitbit, too, was later acquired by Google in 2021). \"I didn't earn any money during Pebble -- we exited, but it was not a great exit,\" Migicovsky admits. This year, however, he decided to reboot the Pebble project after Google open sourced the PebbleOS, which opened up the door to new hardware. With his new company, Core Devices, Migicovsky plans to do things differently. Still, the founder doesn't regret his previous choices, he clarifies. \"I wouldn't have gone back and changed anything. I loved what we built. I loved what we did. I love the company that we built, but it's not the only way to build a company,\" he told TechCrunch. \" And, speaking as an ex-YC partner, there are -- there's a time and a place for building a venture-backed startup. Some companies are phenomenal when they raise money and build a big team, and I tried that...I think what I'm doing now is trying an alternative path, which is [to] start from profitability,\" he says. The new company is a small team of five, self-funded, and focused on sustainability. So far, Core Devices has shipped the Pebble 2 Duo smartwatch with a black-and-white display. Its first run sold out, and the company is now preparing to ship the upgraded version, the Pebble Time 2. The newer device, which has seen 25,000 pre-orders, is a stainless steel watch with a larger, color e-ink screen. As for the Index 01, the ring's pre-order offer ends in March 2026. After that, the price increases to $99. It currently comes in silver, polished gold, and matte black and works with iOS and Android devices. Customers can select from eight ring sizes and three colors.",
          "feed_position": 15
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/brand-context-ai-the-missing-requirement-for-marketing-ai",
          "published_at": "Tue, 09 Dec 2025 08:00:00 GMT",
          "title": "Brand-context AI: The missing requirement for marketing AI",
          "standfirst": "Presented by BlueOceanAI has become a central part of how marketing teams work, but the results often fall short. Models can generate content at scale and summarize information in seconds, yet the outputs are not always aligned with the brand, the audience, or the company’s strategic goals. The problem is not capability. The problem is the absence of context.The bottleneck is no longer computational power. It is contextual intelligence.Generative AI is powerful, but it doesn’t understand the nuances of the business it supports. It doesn’t have the context for why customers choose one brand over another or what creates competitive advantage. Without that grounding, AI operates as a fast executor rather than a strategic partner. It produces more, but it does not always help teams make better decisions.This becomes even more visible inside complex marketing organizations where insights live in different corners of the business and rarely come together in a unified way.As Grant McDougall, CEO of BlueOcean, explains, “Inside large marketing organizations, the data is vertical. Digital has theirs, loyalty has theirs, content has theirs, media has theirs. But CMOs think horizontally. They need to combine customer insight, competitive movement, creative performance, and sales signals into one coherent view. Connecting that data fundamentally changes how decisions get made.”This shift from vertical data to horizontal intelligence reflects a new phase in AI adoption. The emphasis is shifting from output volume to decision quality. Marketers are recognizing that the future of AI is intelligence that understands who you are as a company and why you matter to your customers.In BlueOcean’s work with global brands across technology, healthcare, and consumer industries, including Amazon, Cisco, SAP, and Intel, the same pattern appears. Teams move faster and make better decisions when AI is grounded in structured brand and competitive context.Why context is becoming the critical ingredientLarge language models excel at producing language. They do not inherently understand brand, meaning, or intention. This is why generic prompts often lead to generic outputs. The model executes based on statistical prediction, not strategic nuance.Context changes that. When AI systems are supplied with structured inputs about brand strategy, audience insight, and creative intent, the output becomes sharper and more reliable. Recommendations become more specific. Creative stays on brief. The AI begins to act less like a content generator and more like a partner that understands the boundaries and goals of the business.This shift mirrors a key theme from BlueOcean’s recent report, Building Marketing Intelligence: The CMO Blueprint for Context-Aware AI. The report explains that AI is most effective when it is grounded in a clear frame of reference. CMOs who design these context-aware workflows see better performance, stronger creative, and more reliable decision-making.For a deeper exploration of these principles, the full report is available here.The industry’s pivot: From execution to understandingMany teams remain in an experimentation phase with AI. They test tools, run pilots, and explore new workflows. This creates productivity gains but not intelligence. Without shared context, every team uses AI differently, and the result is fragmentation.The companies making the clearest progress treat context as a shared layer across workflows. When teams pull from the same brand strategy, insights, and creative guidance, AI becomes more predictable and more valuable. It supports decisions rather than contradicting them. This becomes especially effective when the context includes external signals such as shifts in sentiment, competitor movement, content performance, and broader category trends.Brand-context AI connects brand identity, customer sentiment, competitive movement, and creative performance in a single environment. It strengthens workflows in practical ways: briefs become more strategic, content reviews more accurate, and insights faster because the system synthesizes patterns teams once assembled manually.Across enterprise teams supported by BlueOcean, this shift consistently unlocks clarity. AI becomes a contributor to strategic understanding rather than a generator of disconnected output. With shared context in place, teams make more confident, coherent, and aligned decisions.Structured context: What it actually includesStructured context is the intelligence marketers already curate to understand how their brand shows up in the world. It brings together the narrative elements that shape the brand’s voice, the customer motivations that influence messaging, the competitive signals unfolding in the market, and the creative patterns that have historically performed. It also includes the external brand signals teams monitor every day: sentiment shifts, content dynamics, press and social movement, and how competitors position themselves across channels.When this information is organized into a coherent frame, AI can interpret direction and creative choices with the same clarity strategists use. The value does not come from giving AI more data; it comes from giving it structure so it can reason through decisions the way marketers already do.The new division of labor between humans and AIThe strongest AI-enabled marketing teams have one thing in common. They are clear about what humans own and what AI owns. Humans define purpose, strategy, and creative judgment. They understand emotion, cultural nuance, competitive meaning, and brand intent.AI delivers speed, scale, and precision. It excels at synthesizing information, producing iterations, and following structured instruction.“AI works best when it is given clear boundaries and clear intent,” says McDougall. “Humans set the direction led by creativity and imagination. AI executes with precision. That partnership is where the real value emerges.”The systems that perform best are the ones guided by human-defined boundaries and human-led strategy. AI provides scale, but people provide meaning.CMOs are recognizing that governing context is becoming a leadership responsibility. They already own brand, messaging, and customer insight. Extending this ownership into AI systems ensures the brand shows up consistently across every touchpoint, whether a human or a model produced the work.A practical example of context in actionConsider a team preparing a global campaign. Without context, an AI system might generate copy that sounds polished but generic. It may overlook claims the brand can make, reference benefits competitors own, or ignore differentiators that matter most. It may even amplify a competitor’s message simply because that language appears frequently in public data.With structured context, the experience changes. The model understands the audience, the brand tone, the competitive landscape, and the objective. It knows which competitors are gaining attention, which messages resonate in the market, and where the brand has permission to play. It can propose angles that strengthen positioning rather than dilute it. It can generate variations that stay on brief and avoid competitor-owned territory.BlueOcean has observed this shift inside enterprise teams including Amazon, Intel, and SAP, where structured brand and competitive context has improved alignment and reduced drift at scale.Creative, brand, and competitive signals are no longer separate inputs. When they are connected and contextualized, AI begins supporting decision-making in a meaningful way. The technology stops producing output for its own sake and starts helping marketers understand where the brand stands and what actions will grow it.What comes nextA new phase of AI is beginning. AI agents are evolving from task assistants to systems that collaborate across tools and workflows. As these systems become more capable, context will determine whether they behave unpredictably or perform as trusted extensions of the team.Brand-context AI provides a path forward. It gives AI systems the structure they need to operate consistently. It supports the teams responsible for protecting brand integrity. In practice, these agents can already assemble context-aware creative briefs, review content for competitive and brand alignment, monitor shifts in category messaging, and synthesize insights across products or markets. It creates intelligence that adapts rather than overwhelms.In the coming years, success will not come from producing more content, but from producing content anchored in brand context, the kind that sharpens decisions, strengthens positioning, and drives long-term growth.The companies that build on context today will define the generative enterprise of tomorrow. BlueOcean is helping leading enterprises shape the next generation of context-aware AI systems.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by BlueOceanAI has become a central part of how marketing teams work, but the results often fall short. Models can generate content at scale and summarize information in seconds, yet the outputs are not always aligned with the brand, the audience, or the company’s strategic goals. The problem is not capability. The problem is the absence of context.The bottleneck is no longer computational power. It is contextual intelligence.Generative AI is powerful, but it doesn’t understand the nuances of the business it supports. It doesn’t have the context for why customers choose one brand over another or what creates competitive advantage. Without that grounding, AI operates as a fast executor rather than a strategic partner. It produces more, but it does not always help teams make better decisions.This becomes even more visible inside complex marketing organizations where insights live in different corners of the business and rarely come together in a unified way.As Grant McDougall, CEO of BlueOcean, explains, “Inside large marketing organizations, the data is vertical. Digital has theirs, loyalty has theirs, content has theirs, media has theirs. But CMOs think horizontally. They need to combine customer insight, competitive movement, creative performance, and sales signals into one coherent view. Connecting that data fundamentally changes how decisions get made.”This shift from vertical data to horizontal intelligence reflects a new phase in AI adoption. The emphasis is shifting from output volume to decision quality. Marketers are recognizing that the future of AI is intelligence that understands who you are as a company and why you matter to your customers.In BlueOcean’s work with global brands across technology, healthcare, and consumer industries, including Amazon, Cisco, SAP, and Intel, the same pattern appears. Teams move faster and make better decisions when AI is grounded in structured brand and competitive context.Why context is becoming the critical ingredientLarge language models excel at producing language. They do not inherently understand brand, meaning, or intention. This is why generic prompts often lead to generic outputs. The model executes based on statistical prediction, not strategic nuance.Context changes that. When AI systems are supplied with structured inputs about brand strategy, audience insight, and creative intent, the output becomes sharper and more reliable. Recommendations become more specific. Creative stays on brief. The AI begins to act less like a content generator and more like a partner that understands the boundaries and goals of the business.This shift mirrors a key theme from BlueOcean’s recent report, Building Marketing Intelligence: The CMO Blueprint for Context-Aware AI. The report explains that AI is most effective when it is grounded in a clear frame of reference. CMOs who design these context-aware workflows see better performance, stronger creative, and more reliable decision-making.For a deeper exploration of these principles, the full report is available here.The industry’s pivot: From execution to understandingMany teams remain in an experimentation phase with AI. They test tools, run pilots, and explore new workflows. This creates productivity gains but not intelligence. Without shared context, every team uses AI differently, and the result is fragmentation.The companies making the clearest progress treat context as a shared layer across workflows. When teams pull from the same brand strategy, insights, and creative guidance, AI becomes more predictable and more valuable. It supports decisions rather than contradicting them. This becomes especially effective when the context includes external signals such as shifts in sentiment, competitor movement, content performance, and broader category trends.Brand-context AI connects brand identity, customer sentiment, competitive movement, and creative performance in a single environment. It strengthens workflows in practical ways: briefs become more strategic, content reviews more accurate, and insights faster because the system synthesizes patterns teams once assembled manually.Across enterprise teams supported by BlueOcean, this shift consistently unlocks clarity. AI becomes a contributor to strategic understanding rather than a generator of disconnected output. With shared context in place, teams make more confident, coherent, and aligned decisions.Structured context: What it actually includesStructured context is the intelligence marketers already curate to understand how their brand shows up in the world. It brings together the narrative elements that shape the brand’s voice, the customer motivations that influence messaging, the competitive signals unfolding in the market, and the creative patterns that have historically performed. It also includes the external brand signals teams monitor every day: sentiment shifts, content dynamics, press and social movement, and how competitors position themselves across channels.When this information is organized into a coherent frame, AI can interpret direction and creative choices with the same clarity strategists use. The value does not come from giving AI more data; it comes from giving it structure so it can reason through decisions the way marketers already do.The new division of labor between humans and AIThe strongest AI-enabled marketing teams have one thing in common. They are clear about what humans own and what AI owns. Humans define purpose, strategy, and creative judgment. They understand emotion, cultural nuance, competitive meaning, and brand intent.AI delivers speed, scale, and precision. It excels at synthesizing information, producing iterations, and following structured instruction.“AI works best when it is given clear boundaries and clear intent,” says McDougall. “Humans set the direction led by creativity and imagination. AI executes with precision. That partnership is where the real value emerges.”The systems that perform best are the ones guided by human-defined boundaries and human-led strategy. AI provides scale, but people provide meaning.CMOs are recognizing that governing context is becoming a leadership responsibility. They already own brand, messaging, and customer insight. Extending this ownership into AI systems ensures the brand shows up consistently across every touchpoint, whether a human or a model produced the work.A practical example of context in actionConsider a team preparing a global campaign. Without context, an AI system might generate copy that sounds polished but generic. It may overlook claims the brand can make, reference benefits competitors own, or ignore differentiators that matter most. It may even amplify a competitor’s message simply because that language appears frequently in public data.With structured context, the experience changes. The model understands the audience, the brand tone, the competitive landscape, and the objective. It knows which competitors are gaining attention, which messages resonate in the market, and where the brand has permission to play. It can propose angles that strengthen positioning rather than dilute it. It can generate variations that stay on brief and avoid competitor-owned territory.BlueOcean has observed this shift inside enterprise teams including Amazon, Intel, and SAP, where structured brand and competitive context has improved alignment and reduced drift at scale.Creative, brand, and competitive signals are no longer separate inputs. When they are connected and contextualized, AI begins supporting decision-making in a meaningful way. The technology stops producing output for its own sake and starts helping marketers understand where the brand stands and what actions will grow it.What comes nextA new phase of AI is beginning. AI agents are evolving from task assistants to systems that collaborate across tools and workflows. As these systems become more capable, context will determine whether they behave unpredictably or perform as trusted extensions of the team.Brand-context AI provides a path forward. It gives AI systems the structure they need to operate consistently. It supports the teams responsible for protecting brand integrity. In practice, these agents can already assemble context-aware creative briefs, review content for competitive and brand alignment, monitor shifts in category messaging, and synthesize insights across products or markets. It creates intelligence that adapts rather than overwhelms.In the coming years, success will not come from producing more content, but from producing content anchored in brand context, the kind that sharpens decisions, strengthens positioning, and drives long-term growth.The companies that build on context today will define the generative enterprise of tomorrow. BlueOcean is helping leading enterprises shape the next generation of context-aware AI systems.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/14MrK6yiPQLN3SznWaBOl4/9e3ba1a69b409e0c085c7de1dcea47e2/AdobeStock_438714181.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/tracking-every-decision-dollar-and-delay-the-new-process-intelligence-engine",
          "published_at": "Tue, 09 Dec 2025 05:00:00 GMT",
          "title": "Tracking every decision, dollar and delay: The new process intelligence engine driving public-sector progress",
          "standfirst": "Presented by CelonisThe State of Oklahoma discovered its blind spots the hard way. In April 2023, a legislative report revealed its agencies had spent $3 billion without proper oversight. Janet Morrow, Director of Oklahoma&#x27;s Risk, Assessment and Compliance Division, set out to track thousands of monthly transactions across dozens of disconnected systems.The Sooner State became the first U.S. state to apply process intelligence (PI) technology for procurement oversight. The transformation, Morrow says, was immediate. Real-time monitoring replaced multi-year audit cycles. The platform from market-leader Celonis quickly identified more than $10 million of inappropriate spending. And the oversight team was able to redeploy staff from 13 to 5 members while dramatically increasing effectiveness.“Process for Progress”: A global movementOklahoma&#x27;s pioneering success using powerful new process technology spotlights an emerging global trend. Morrow was among more than 3,000 leaders gathered at Celosphere, Celonis’s recent annual conference, to explore how AI, powered with business context by PI, can deliver commercial returns as well as environmental and financial benefits worldwide. The vision: process intelligence as a foundation for public and social progress. The movement sees the combination of AI and PI like Oklahoma’s as a powerful way to help governments and other organizations deliver vital services more cost effectively, with improved decisions and better-informed policies. From procurement to juvenile justice to healthcare and environment, scores of organizations are now getting a first look at the famously byzantine, opaque way things get done.For veteran financial leader Aubrey Vaughan — now Vice President of Strategy for Public Sector at Celonis and formerly a top executive at a major financial software firm — the move toward real process improvement has been a long time coming. He recalls testifying proudly before Congress a few years ago about uncovering $10 billion in improper government payments at his previous company. Afterward, a senior government official pulled him aside and suggested he downplay the achievement.The reason, he was told: \"The next question they&#x27;re going to ask you is, ‘Why is that happening?’” says Vaughn. “Today we can answer not only why, but how we fix it.\" Across the U.S. and the globe, public agencies are tightening budgets. Desire to deploy AI to close the gap is colliding with a hard reality: you can&#x27;t automate what you don&#x27;t understand. Here are three real-world examples of organizations using PI and AI for better outcomes. Oklahoma: Real-time AI spending analysis boosts accountability Within just 60 days of implementation, Celonis reviewed $29.4 billion worth of purchase order lines, identifying $8.48 billion in statutory exempt purchases and flagging problematic transactions. The system now provides real-time feedback to buyers within 15 minutes of purchases, allowing immediate course correction.The system revealed agencies were purchasing from a vendor at prices 45% lower than the statewide contract, forcing renegotiation. \"Real-time AI analysis has increased accountability by providing key insights into spending patterns and streamlining contract utilization,\" Morrow explains. Last year, Oklahoma adopted Celonis&#x27;s Copilot feature, which uses conversational AI to let executives ask questions in plain language. Now, when the Governor or a cabinet member wonders about a contract, they get answers in seconds, not weeks, Morrow says. Her group is expanding the technology to other agencies. It’s also exploring how emerging AI agent capabilities can further automate compliance and spending analysis.In Texas, uncovering a startling hidden pattern in young offenders At Evident Change, a social research non-profit, Erin Espinosa&#x27;s work is about good stewardship — not of taxpayer money, but of young lives. Analyzing 400,000 data points from juvenile justice and public health systems in Texas, the former probation officer-turned Ph.D. made a startling discovery: the mental health treatment that young offenders received (or didn’t) was a stronger predictor of incarceration than the seriousness of the offense that brought them into the system. Espinosa told courts, legislatures, Congress. Nobody believed it.Frustrated, she partnered with Monica Chiarini Tremblay, a professor at William & Mary College. While traditional analysis showed correlation, Celonis process intelligence helped the pair show a clear, quantitative causation: A fragmented mental health system was actively pushing kids toward worse outcomes. Further machine learning analysis also demonstrated that doubling down on the same interventions increased likelihood of undesirable out-of-home placement for juvenile offenders.Recently accepted for academic publication, the real-world findings represent both indictment and opportunity. Espinosa and Tremblay are planning a larger 2026 pilot implementation of PI-based analysis, bringing together social services, juvenile justice, mental health providers, and education officials. \"This is a perfect intersection of business, social work, adolescent development, and community financial implications,\" Espinosa says. They’re now exploring how AI agent technologies could flag at-risk youth and trigger coordinated responses before patterns become entrenched.A $1-trillion defense budget — that has never passed a clean audit The U.S. Department of Defense faces financial challenges on an exponentially larger scale. As Acting Secretary of the Army, Robert M. Speer hired a big-three accounting firm to map the service’s financial processes. Three years later, the analysis was obsolete — processes had changed dramatically. So, when Speer first saw process intelligence, he was truly excited about what it revealed. \"I can see not only the data,” he explained, “but where it&#x27;s coming from, the business process delivering it.\"Tom Steffens, former Deputy Chief Financial Officer of Defense, agrees: \"There&#x27;s clearly a missing piece to the puzzle.\" Both recently joined Celonis&#x27;s Public Sector Advisory Board. They see potential for AI agents to automate compliance monitoring across DoD&#x27;s complex ecosystem.The stakes are unimaginably huge. The Department of Defense will receive more than a trillion dollars in funding in FY 2026. It’s also the only federal cabinet agency that&#x27;s never passed a clean audit. Beyond accounting, fast-changing geopolitics and modern warfare demands systems as dynamic as current battle environments. \"We&#x27;re talking about the ability to shift in real time,\" says Speer. \"We know that’s what happens on the battlefield, but we need something on the back end of those enabling processes and systems to ensure that happens correctly.\" The pair is working with defense leaders to show how process intelligence can create the foundation for transformation — enabling modeling and scenario planning that can support battlefield decisions with data-driven confidence rather than delayed, obsolete information.Efforts to modernize and optimize complex government systems and processes got a big boost recently. Working with partner Knox Systems, Celonis received FedRAMP authorization earlier this year, the security credential required for federal cloud services. \"Knox powers the most secure and longest-running managed federal cloud,\" notes CEO Irina Denisenko, supporting 15+ federal agencies. The authorization positions the technology \"as the backbone of compliance for the next generation of government SaaS.\"Where process meets purposeEarly public sector adopters are proving what&#x27;s possible with process intelligence — from identifying billions in potential savings to revealing why children enter the prison pipeline. The potential extends wherever public funds shape public good: climate response, education, infrastructure, emergency services.Advocates often speak of “process for progress” or \"process for empathy\" — using transparency to change minds and hearts, not just policies. Says Chiarini Tremblay, who worked on the Texas juvenile offenders’ system: \"We have to understand complex systems and make data-driven decisions, but the goal is always improving outcomes for people.\"It’s not just a U.S. movement. In the UK, for example, University Hospitals Coventry and Warwickshire NHS Trust have deployed PI with dramatic effect. Director Andy Hardy used Celonis to analyze 244,000 outpatient cases, revealing massive variation in care delivery.By optimizing appointment reminders from four to 14 days before visits, the trust enabled earlier cancellations and saw an additional 1,800 patients weekly. The waiting list was reduced by 5,300 patients in eight weeks. Concludes Hardy: \"Data understandable to clinicians is as important as scalpels.\" Technology continues to race ahead. At Celosphere 2025, Celonis unveiled a host of new offerings and platform updates for public and private sector organizations including the Orchestration Engine, which coordinates actions across workflows involving AI agents, human tasks, and legacy systems.All are built on the Celonis Process Intelligence Graph, which creates a \"living digital twin\" of a business or public agency’s processes. It’s system-agnostic, working across disconnected systems typical to government operations — integrating decades-old mainframes and cutting-edge cloud applications simultaneously.Agency heads and others note, however, that success demands more than software. For example, when Oklahoma reduced its oversight team from 13 to 5, resistance emerged. Morrow&#x27;s team invested heavily in training and change management. Process intelligence reveals improvement opportunities, but people implement solutions’ she explains. Ongoing, long-term education and cultural change are needed. “Continuous operational improvement is a lifestyle,” says Celonis’s Vaughn. “You need to have a culture that wants to build better processes, better systems, more efficient systems.”The tools are ready. The business case is proven. What remains is the will to change — and the courage to look clearly at the systems meant to serve the public good.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by CelonisThe State of Oklahoma discovered its blind spots the hard way. In April 2023, a legislative report revealed its agencies had spent $3 billion without proper oversight. Janet Morrow, Director of Oklahoma&#x27;s Risk, Assessment and Compliance Division, set out to track thousands of monthly transactions across dozens of disconnected systems.The Sooner State became the first U.S. state to apply process intelligence (PI) technology for procurement oversight. The transformation, Morrow says, was immediate. Real-time monitoring replaced multi-year audit cycles. The platform from market-leader Celonis quickly identified more than $10 million of inappropriate spending. And the oversight team was able to redeploy staff from 13 to 5 members while dramatically increasing effectiveness.“Process for Progress”: A global movementOklahoma&#x27;s pioneering success using powerful new process technology spotlights an emerging global trend. Morrow was among more than 3,000 leaders gathered at Celosphere, Celonis’s recent annual conference, to explore how AI, powered with business context by PI, can deliver commercial returns as well as environmental and financial benefits worldwide. The vision: process intelligence as a foundation for public and social progress. The movement sees the combination of AI and PI like Oklahoma’s as a powerful way to help governments and other organizations deliver vital services more cost effectively, with improved decisions and better-informed policies. From procurement to juvenile justice to healthcare and environment, scores of organizations are now getting a first look at the famously byzantine, opaque way things get done.For veteran financial leader Aubrey Vaughan — now Vice President of Strategy for Public Sector at Celonis and formerly a top executive at a major financial software firm — the move toward real process improvement has been a long time coming. He recalls testifying proudly before Congress a few years ago about uncovering $10 billion in improper government payments at his previous company. Afterward, a senior government official pulled him aside and suggested he downplay the achievement.The reason, he was told: \"The next question they&#x27;re going to ask you is, ‘Why is that happening?’” says Vaughn. “Today we can answer not only why, but how we fix it.\" Across the U.S. and the globe, public agencies are tightening budgets. Desire to deploy AI to close the gap is colliding with a hard reality: you can&#x27;t automate what you don&#x27;t understand. Here are three real-world examples of organizations using PI and AI for better outcomes. Oklahoma: Real-time AI spending analysis boosts accountability Within just 60 days of implementation, Celonis reviewed $29.4 billion worth of purchase order lines, identifying $8.48 billion in statutory exempt purchases and flagging problematic transactions. The system now provides real-time feedback to buyers within 15 minutes of purchases, allowing immediate course correction.The system revealed agencies were purchasing from a vendor at prices 45% lower than the statewide contract, forcing renegotiation. \"Real-time AI analysis has increased accountability by providing key insights into spending patterns and streamlining contract utilization,\" Morrow explains. Last year, Oklahoma adopted Celonis&#x27;s Copilot feature, which uses conversational AI to let executives ask questions in plain language. Now, when the Governor or a cabinet member wonders about a contract, they get answers in seconds, not weeks, Morrow says. Her group is expanding the technology to other agencies. It’s also exploring how emerging AI agent capabilities can further automate compliance and spending analysis.In Texas, uncovering a startling hidden pattern in young offenders At Evident Change, a social research non-profit, Erin Espinosa&#x27;s work is about good stewardship — not of taxpayer money, but of young lives. Analyzing 400,000 data points from juvenile justice and public health systems in Texas, the former probation officer-turned Ph.D. made a startling discovery: the mental health treatment that young offenders received (or didn’t) was a stronger predictor of incarceration than the seriousness of the offense that brought them into the system. Espinosa told courts, legislatures, Congress. Nobody believed it.Frustrated, she partnered with Monica Chiarini Tremblay, a professor at William & Mary College. While traditional analysis showed correlation, Celonis process intelligence helped the pair show a clear, quantitative causation: A fragmented mental health system was actively pushing kids toward worse outcomes. Further machine learning analysis also demonstrated that doubling down on the same interventions increased likelihood of undesirable out-of-home placement for juvenile offenders.Recently accepted for academic publication, the real-world findings represent both indictment and opportunity. Espinosa and Tremblay are planning a larger 2026 pilot implementation of PI-based analysis, bringing together social services, juvenile justice, mental health providers, and education officials. \"This is a perfect intersection of business, social work, adolescent development, and community financial implications,\" Espinosa says. They’re now exploring how AI agent technologies could flag at-risk youth and trigger coordinated responses before patterns become entrenched.A $1-trillion defense budget — that has never passed a clean audit The U.S. Department of Defense faces financial challenges on an exponentially larger scale. As Acting Secretary of the Army, Robert M. Speer hired a big-three accounting firm to map the service’s financial processes. Three years later, the analysis was obsolete — processes had changed dramatically. So, when Speer first saw process intelligence, he was truly excited about what it revealed. \"I can see not only the data,” he explained, “but where it&#x27;s coming from, the business process delivering it.\"Tom Steffens, former Deputy Chief Financial Officer of Defense, agrees: \"There&#x27;s clearly a missing piece to the puzzle.\" Both recently joined Celonis&#x27;s Public Sector Advisory Board. They see potential for AI agents to automate compliance monitoring across DoD&#x27;s complex ecosystem.The stakes are unimaginably huge. The Department of Defense will receive more than a trillion dollars in funding in FY 2026. It’s also the only federal cabinet agency that&#x27;s never passed a clean audit. Beyond accounting, fast-changing geopolitics and modern warfare demands systems as dynamic as current battle environments. \"We&#x27;re talking about the ability to shift in real time,\" says Speer. \"We know that’s what happens on the battlefield, but we need something on the back end of those enabling processes and systems to ensure that happens correctly.\" The pair is working with defense leaders to show how process intelligence can create the foundation for transformation — enabling modeling and scenario planning that can support battlefield decisions with data-driven confidence rather than delayed, obsolete information.Efforts to modernize and optimize complex government systems and processes got a big boost recently. Working with partner Knox Systems, Celonis received FedRAMP authorization earlier this year, the security credential required for federal cloud services. \"Knox powers the most secure and longest-running managed federal cloud,\" notes CEO Irina Denisenko, supporting 15+ federal agencies. The authorization positions the technology \"as the backbone of compliance for the next generation of government SaaS.\"Where process meets purposeEarly public sector adopters are proving what&#x27;s possible with process intelligence — from identifying billions in potential savings to revealing why children enter the prison pipeline. The potential extends wherever public funds shape public good: climate response, education, infrastructure, emergency services.Advocates often speak of “process for progress” or \"process for empathy\" — using transparency to change minds and hearts, not just policies. Says Chiarini Tremblay, who worked on the Texas juvenile offenders’ system: \"We have to understand complex systems and make data-driven decisions, but the goal is always improving outcomes for people.\"It’s not just a U.S. movement. In the UK, for example, University Hospitals Coventry and Warwickshire NHS Trust have deployed PI with dramatic effect. Director Andy Hardy used Celonis to analyze 244,000 outpatient cases, revealing massive variation in care delivery.By optimizing appointment reminders from four to 14 days before visits, the trust enabled earlier cancellations and saw an additional 1,800 patients weekly. The waiting list was reduced by 5,300 patients in eight weeks. Concludes Hardy: \"Data understandable to clinicians is as important as scalpels.\" Technology continues to race ahead. At Celosphere 2025, Celonis unveiled a host of new offerings and platform updates for public and private sector organizations including the Orchestration Engine, which coordinates actions across workflows involving AI agents, human tasks, and legacy systems.All are built on the Celonis Process Intelligence Graph, which creates a \"living digital twin\" of a business or public agency’s processes. It’s system-agnostic, working across disconnected systems typical to government operations — integrating decades-old mainframes and cutting-edge cloud applications simultaneously.Agency heads and others note, however, that success demands more than software. For example, when Oklahoma reduced its oversight team from 13 to 5, resistance emerged. Morrow&#x27;s team invested heavily in training and change management. Process intelligence reveals improvement opportunities, but people implement solutions’ she explains. Ongoing, long-term education and cultural change are needed. “Continuous operational improvement is a lifestyle,” says Celonis’s Vaughn. “You need to have a culture that wants to build better processes, better systems, more efficient systems.”The tools are ready. The business case is proven. What remains is the will to change — and the courage to look clearly at the systems meant to serve the public good.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/23oNXRNgKBLbHuQlKDjPbP/736c1c4212c55997871975219bea29ae/AdobeStock_1070394941.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for",
          "published_at": "Tue, 09 Dec 2025 01:03:00 GMT",
          "title": "Z.ai debuts open source GLM-4.6V, a native tool-calling vision model for multimodal reasoning",
          "standfirst": "Chinese AI startup Zhipu AI aka Z.ai has released its GLM-4.6V series, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. The release includes two models in \"large\" and \"small\" sizes: GLM-4.6V (106B), a larger 106-billion parameter model aimed at cloud-scale inferenceGLM-4.6V-Flash (9B), a smaller model of only 9 billion parameters designed for low-latency, local applicationsRecall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.The defining innovation in this series is the introduction of native function calling in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. With a 128,000 token context length (equivalent to a 300-page novel&#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&#x27;s available in the following formats:API access via OpenAI-compatible interfaceTry the demo on Zhipu’s web interfaceDownload weights from Hugging FaceDesktop assistant app available on Hugging Face SpacesLicensing and Enterprise UseGLM‑4.6V and GLM‑4.6V‑Flash are distributed under the MIT license, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.Model weights and documentation are publicly hosted on Hugging Face, with supporting code and tooling available on GitHub. The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.Architecture and Technical CapabilitiesThe GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.Native Multimodal Tool UseGLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.The tool invocation mechanism works bi-directionally:Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.In practice, this means GLM-4.6V can complete tasks such as:Generating structured reports from mixed-format documentsPerforming visual audit of candidate imagesAutomatically cropping figures from papers during generationConducting visual web search and answering multimodal queriesHigh Performance Benchmarks Compared to Other Similar-Sized ModelsGLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. According to the benchmark chart released by Zhipu AI:GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.Example scores from the leaderboard include:MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.Frontend Automation and Long-Context WorkflowsZhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:Replicate pixel-accurate HTML/CSS/JS from UI screenshotsAccept natural language editing commands to modify layoutsIdentify and manipulate specific UI components visuallyThis capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:150 pages of text (input)200 slide decks1-hour videosZhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.Training and Reinforcement LearningThe model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progressMulti-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial groundingFunction-aware training: Uses structured tags (e.g., <think>, <answer>, <|begin_of_box|>) to align reasoning and answer formattingThe reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domainsPricing (API)Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokensGLM-4.6V-Flash: FreeCompared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:USD per 1M tokens — sorted lowest → highest total costModelInputOutputTotal CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudERNIE 4.5 Turbo$0.11$0.45$0.56QianfanGLM‑4.6V$0.30$0.90$1.20Z.AIGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanQwen-Max$1.60$6.40$8.00Alibaba CloudGPT-5.1$1.25$10.00$11.25OpenAIGemini 2.5 Pro (≤200K)$1.25$10.00$11.25GoogleGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGemini 2.5 Pro (>200K)$2.50$15.00$17.50GoogleGrok 4 (0709)$3.00$15.00$18.00xAIGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.1$15.00$75.00$90.00AnthropicPrevious Releases: GLM‑4.5 Series and Enterprise ApplicationsPrior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipelEcosystem ImplicationsThe GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:Integrated visual tool usageStructured multimodal generationAgent-oriented memory and decision logicZhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.Takeaway for Enterprise LeadersWith GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems.",
          "content": "Chinese AI startup Zhipu AI aka Z.ai has released its GLM-4.6V series, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. The release includes two models in \"large\" and \"small\" sizes: GLM-4.6V (106B), a larger 106-billion parameter model aimed at cloud-scale inferenceGLM-4.6V-Flash (9B), a smaller model of only 9 billion parameters designed for low-latency, local applicationsRecall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.The defining innovation in this series is the introduction of native function calling in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. With a 128,000 token context length (equivalent to a 300-page novel&#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&#x27;s available in the following formats:API access via OpenAI-compatible interfaceTry the demo on Zhipu’s web interfaceDownload weights from Hugging FaceDesktop assistant app available on Hugging Face SpacesLicensing and Enterprise UseGLM‑4.6V and GLM‑4.6V‑Flash are distributed under the MIT license, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.Model weights and documentation are publicly hosted on Hugging Face, with supporting code and tooling available on GitHub. The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.Architecture and Technical CapabilitiesThe GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.Native Multimodal Tool UseGLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.The tool invocation mechanism works bi-directionally:Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.In practice, this means GLM-4.6V can complete tasks such as:Generating structured reports from mixed-format documentsPerforming visual audit of candidate imagesAutomatically cropping figures from papers during generationConducting visual web search and answering multimodal queriesHigh Performance Benchmarks Compared to Other Similar-Sized ModelsGLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. According to the benchmark chart released by Zhipu AI:GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.Example scores from the leaderboard include:MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.Frontend Automation and Long-Context WorkflowsZhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:Replicate pixel-accurate HTML/CSS/JS from UI screenshotsAccept natural language editing commands to modify layoutsIdentify and manipulate specific UI components visuallyThis capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:150 pages of text (input)200 slide decks1-hour videosZhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.Training and Reinforcement LearningThe model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progressMulti-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial groundingFunction-aware training: Uses structured tags (e.g., <think>, <answer>, <|begin_of_box|>) to align reasoning and answer formattingThe reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domainsPricing (API)Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokensGLM-4.6V-Flash: FreeCompared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:USD per 1M tokens — sorted lowest → highest total costModelInputOutputTotal CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudERNIE 4.5 Turbo$0.11$0.45$0.56QianfanGLM‑4.6V$0.30$0.90$1.20Z.AIGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanQwen-Max$1.60$6.40$8.00Alibaba CloudGPT-5.1$1.25$10.00$11.25OpenAIGemini 2.5 Pro (≤200K)$1.25$10.00$11.25GoogleGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGemini 2.5 Pro (>200K)$2.50$15.00$17.50GoogleGrok 4 (0709)$3.00$15.00$18.00xAIGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.1$15.00$75.00$90.00AnthropicPrevious Releases: GLM‑4.5 Series and Enterprise ApplicationsPrior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipelEcosystem ImplicationsThe GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:Integrated visual tool usageStructured multimodal generationAgent-oriented memory and decision logicZhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.Takeaway for Enterprise LeadersWith GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/c8LjxKPtjJumRUwmgig3W/3ceb88204d1ec334f3ce3719b80793eb/6Ud5A3f_99gyh14m2ffZr.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for",
          "published_at": "Mon, 08 Dec 2025 19:00:00 GMT",
          "title": "Anthropic's Claude Code can now read your Slack messages and write code for you",
          "standfirst": "Anthropic has launched a beta integration that connects its fast-growing Claude Code programming agent directly into Slack, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.The release, which Anthropic describes as a \"research preview,\" is the company&#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating more than $1 billion in annualized revenue just six months after its public debut.\"The critical context around engineering work often lives in Slack, including bug reports, feature requests and engineering discussions,\" the company wrote in a blog post. \"When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.\"From bug report to pull request: How the new Slack integration actually worksThe mechanics are deceptively simple but address a persistent friction point in software development: The gap between where problems are discussed and where they are fixed.When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new Claude Code session. Users can also explicitly instruct Claude to treat requests as coding tasks.Claude gathers context from recent Slack channel and thread messages to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories that have been authenticated to Claude Code on the web.As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.The feature builds on Anthropic&#x27;s existing Claude for Slack integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix and post a pull request — all while updating the original Slack thread with its progress.Why Anthropic is betting big on enterprise workflow integrationsThe Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit $1 billion in revenue, six months after its public debut, according to a LinkedIn post from Anthropic&#x27;s CPO Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify and Salesforce.The velocity of that growth helps explain why Anthropic made its first-ever acquisition earlier this month, of developer tool startup Bun. Anthropic declined to comment on specific financial details. Bun is a breakthrough JavaScript runtime that claims to be dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler and test runner — it&#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.Since becoming generally available in May 2025, Claude Code has grown from its origins as an internal engineering experiment into a critical tool for many of the world&#x27;s category-leading enterprises, including Netflix, Spotify, KPMG, L&#x27;Oreal and Salesforce — and Bun has been key in helping scale its infrastructure throughout that evolution.The acquisition signals that Anthropic views Claude Code not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.According to an Anthropic spokesperson, companies including Rakuten, Novo Nordisk, Uber, Snowflake and Ramp now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 to 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.Claude Code&#x27;s rapid rise from internal experiment to billion-dollar productThe Slack launch is the latest in a rapid series of Claude Code expansions. In late November, Claude Code was added to Anthropic&#x27;s desktop apps, including the Mac version. Previously, Claude Code was limited to mobile apps and the web. The desktop version allows software engineers to code, research and update work with multiple local and remote sessions running at the same time.That release accompanied Anthropic&#x27;s unveiling of Claude Opus 4.5, its newest and most capable model. Claude Opus 4.5 is available today on the company&#x27;s apps, API and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams and enterprises.The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, Anthropic released three new beta features for tool use: Tool Search Tool, which allows Claude to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment, thus reducing the impact on the model&#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.The Model Context Protocol (MCP) is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent, and it unlocks an entire ecosystem of integrations.Inside Anthropic&#x27;s own AI transformation: What happens when engineers use Claude all dayAnthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic surveyed 132 engineers and researchers, conducted 53 in-depth qualitative interviews and studied internal Claude Code usage data to understand how AI use is changing work at the company.Employees self-reported using Claude in 60% of their work and achieved a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&#x27;t be cost-effective if done manually.The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without the need for human intervention, versus 9.8 tool calls from six months ago.The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now, compared to six months ago.But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. \"It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial, and I go and talk to them,\" one engineer explained. Several engineers said they \"bounce ideas off\" Claude, similar to interactions with human collaborators.Some appreciate the reduced social friction, but others resist the change or miss the older way of working: \"I like working with people, and it is sad that I need them less now.\"How Anthropic stacks up against OpenAI, Google and Microsoft in the enterprise AI raceAnthropic is not alone in racing to capture the enterprise coding market. OpenAI, Google and Microsoft (through GitHub Copilot) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely used enterprise communication platforms — Slack claims over 750,000 organizations use its software.The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show that Anthropic expects to break even by 2028 — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware and consumer products.The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&#x27;s biggest titans. Microsoft and Nvidia pledged up to $15 billion in fresh investment in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&#x27;s cloud. This is in addition to the $8 billion invested by Amazon and $3 billion by Google.The cross-investment from both Microsoft and Google — fierce competitors in the cloud and AI spaces — highlights Anthropic&#x27;s valuable enterprise positioning. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.What the Slack integration means for developers — and whether they can trust itFor engineering teams, the Slack integration promises to collapse the distance between problem identification and resolution. A bug report in a Slack channel can immediately trigger an investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can \"fully delegate\" only 0 to 20% of their work to it. Claude is a constant collaborator, but using it generally involves active supervision and validation, especially in high-stakes work, versus handing off tasks requiring no verification at all.Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — \"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.\"The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&#x27;s own research has documented.The future of coding may be conversational—and Anthropic is racing to prove itThe beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.Whether that vision becomes reality will depend on whether Claude Code can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: A billion dollars in revenue, a roster of Fortune 500 customers and a growing ecosystem of integrations suggest Anthropic is onto something real.But in one of Anthropic&#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: \"Nobody knows what&#x27;s going to happen… the important thing is to just be really adaptable.\"In the age of AI coding agents, that may be the only career advice that holds up.",
          "content": "Anthropic has launched a beta integration that connects its fast-growing Claude Code programming agent directly into Slack, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.The release, which Anthropic describes as a \"research preview,\" is the company&#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating more than $1 billion in annualized revenue just six months after its public debut.\"The critical context around engineering work often lives in Slack, including bug reports, feature requests and engineering discussions,\" the company wrote in a blog post. \"When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.\"From bug report to pull request: How the new Slack integration actually worksThe mechanics are deceptively simple but address a persistent friction point in software development: The gap between where problems are discussed and where they are fixed.When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new Claude Code session. Users can also explicitly instruct Claude to treat requests as coding tasks.Claude gathers context from recent Slack channel and thread messages to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories that have been authenticated to Claude Code on the web.As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.The feature builds on Anthropic&#x27;s existing Claude for Slack integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix and post a pull request — all while updating the original Slack thread with its progress.Why Anthropic is betting big on enterprise workflow integrationsThe Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit $1 billion in revenue, six months after its public debut, according to a LinkedIn post from Anthropic&#x27;s CPO Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify and Salesforce.The velocity of that growth helps explain why Anthropic made its first-ever acquisition earlier this month, of developer tool startup Bun. Anthropic declined to comment on specific financial details. Bun is a breakthrough JavaScript runtime that claims to be dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler and test runner — it&#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.Since becoming generally available in May 2025, Claude Code has grown from its origins as an internal engineering experiment into a critical tool for many of the world&#x27;s category-leading enterprises, including Netflix, Spotify, KPMG, L&#x27;Oreal and Salesforce — and Bun has been key in helping scale its infrastructure throughout that evolution.The acquisition signals that Anthropic views Claude Code not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.According to an Anthropic spokesperson, companies including Rakuten, Novo Nordisk, Uber, Snowflake and Ramp now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 to 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.Claude Code&#x27;s rapid rise from internal experiment to billion-dollar productThe Slack launch is the latest in a rapid series of Claude Code expansions. In late November, Claude Code was added to Anthropic&#x27;s desktop apps, including the Mac version. Previously, Claude Code was limited to mobile apps and the web. The desktop version allows software engineers to code, research and update work with multiple local and remote sessions running at the same time.That release accompanied Anthropic&#x27;s unveiling of Claude Opus 4.5, its newest and most capable model. Claude Opus 4.5 is available today on the company&#x27;s apps, API and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams and enterprises.The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, Anthropic released three new beta features for tool use: Tool Search Tool, which allows Claude to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment, thus reducing the impact on the model&#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.The Model Context Protocol (MCP) is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent, and it unlocks an entire ecosystem of integrations.Inside Anthropic&#x27;s own AI transformation: What happens when engineers use Claude all dayAnthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic surveyed 132 engineers and researchers, conducted 53 in-depth qualitative interviews and studied internal Claude Code usage data to understand how AI use is changing work at the company.Employees self-reported using Claude in 60% of their work and achieved a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&#x27;t be cost-effective if done manually.The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without the need for human intervention, versus 9.8 tool calls from six months ago.The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now, compared to six months ago.But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. \"It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial, and I go and talk to them,\" one engineer explained. Several engineers said they \"bounce ideas off\" Claude, similar to interactions with human collaborators.Some appreciate the reduced social friction, but others resist the change or miss the older way of working: \"I like working with people, and it is sad that I need them less now.\"How Anthropic stacks up against OpenAI, Google and Microsoft in the enterprise AI raceAnthropic is not alone in racing to capture the enterprise coding market. OpenAI, Google and Microsoft (through GitHub Copilot) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely used enterprise communication platforms — Slack claims over 750,000 organizations use its software.The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show that Anthropic expects to break even by 2028 — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware and consumer products.The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&#x27;s biggest titans. Microsoft and Nvidia pledged up to $15 billion in fresh investment in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&#x27;s cloud. This is in addition to the $8 billion invested by Amazon and $3 billion by Google.The cross-investment from both Microsoft and Google — fierce competitors in the cloud and AI spaces — highlights Anthropic&#x27;s valuable enterprise positioning. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.What the Slack integration means for developers — and whether they can trust itFor engineering teams, the Slack integration promises to collapse the distance between problem identification and resolution. A bug report in a Slack channel can immediately trigger an investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can \"fully delegate\" only 0 to 20% of their work to it. Claude is a constant collaborator, but using it generally involves active supervision and validation, especially in high-stakes work, versus handing off tasks requiring no verification at all.Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — \"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.\"The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&#x27;s own research has documented.The future of coding may be conversational—and Anthropic is racing to prove itThe beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.Whether that vision becomes reality will depend on whether Claude Code can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: A billion dollars in revenue, a roster of Fortune 500 customers and a growing ecosystem of integrations suggest Anthropic is onto something real.But in one of Anthropic&#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: \"Nobody knows what&#x27;s going to happen… the important thing is to just be really adaptable.\"In the age of AI coding agents, that may be the only career advice that holds up.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/667OUCJyzh5TAzDpX4UQDa/d1b772df47ef4e01e6450e1bb9979970/nuneybits_Vector_art_of_code-filled_speech_bubble_in_burnt_oran_78f6bff7-7863-4363-bcad-892c8f7cf2f7.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/booking-coms-agent-strategy-disciplined-modular-and-already-delivering-2",
          "published_at": "Mon, 08 Dec 2025 15:00:00 GMT",
          "title": "Booking.com’s agent strategy: Disciplined, modular and already delivering 2× accuracy",
          "standfirst": "When many enterprises weren’t even thinking about agentic behaviors or infrastructures, Booking.com had already “stumbled” into them with its homegrown conversational recommendation system. This early experimentation has allowed the company to take a step back and avoid getting swept up in the frantic AI agent hype. Instead, it is taking a disciplined, layered, modular approach to model development: small, travel-specific models for cheap, fast inference; larger large language models (LLMs) for reasoning and understanding; and domain-tuned evaluations built in-house when precision is critical. Listen and subscribe to Beyond the Pilot on Spotify, Apple or wherever you get your podcasts.With this hybrid strategy — and selective collaboration with OpenAI, Gemini and other key players in the AI space — Booking.com has seen accuracy double across key retrieval, ranking and customer-interaction tasks. As Pranav Pathak, Booking.com’s AI product development lead, posed to VentureBeat in a new podcast: “Do you build it very, very specialized and bespoke and then have an army of a hundred agents? Or do you keep it general enough and have five agents that are good at generalized tasks, but then you have to orchestrate a lot around them? That&#x27;s a balance that I think we&#x27;re still trying to figure out, as is the rest of the industry.”Moving from guessing to deep personalization without being ‘creepy’Recommendation systems are core to Booking.com’s customer-facing platforms; however, traditional recommendation tools have been less about recommendation and more about guessing, Pathak conceded. So, from the start, he and his team vowed to avoid generic tools: As he put it, the price and recommendation should be based on customer context. Booking.com’s initial pre-gen AI tooling for intent and topic detection was a small language model, what Pathak described as “the scale and size of BERT.” The model ingested the customer’s inputs around their problem to determine whether it could be solved through self-service or bumped to a human agent. “We started with an architecture of ‘you have to call a tool if this is the intent you detect and this is how you&#x27;ve parsed the structure,” Pathak explained. “That was very, very similar to the first few agentic architectures that came out in terms of reason and defining a tool call.” His team has since built out that architecture to include an LLM orchestrator that classifies queries, triggers retrieval-augmented generation (RAG) and calls APIs or smaller, specialized language models. “We&#x27;ve been able to scale that system quite well because it was so close in architecture that, with a few tweaks, we now have a full agentic stack,” said Pathak. As a result, Booking.com is seeing a notable increase in topic detection, with more topics, even complicated ones previously identified as ‘other’ and requiring escalation, are becoming automated. Ultimately, this supports more self-service, freeing human agents to focus on customers with uniquely-specific problems that the platform doesn’t have a dedicated tool flow for — say, a family that is unable to access its hotel room at 2 a.m. when the front desk is closed. That not only “really starts to compound,” but has a direct, long-term impact on customer retention, Pathak noted. “One of the things we&#x27;ve seen is, the better we are at customer service, the more loyal our customers are.” Another recent rollout is personalized filtering. Booking.com has between 200 and 250 search filters on its website — an unrealistic amount for any human to sift through, Pathak pointed out. So, his team introduced a free text box that users can type into to immediately receive tailored filters. “That becomes such an important cue for personalization in terms of what you&#x27;re looking for in your own words rather than a clickstream,” said Pathak. In turn, it cues Booking.com into what customers actually want. For instance, hot tubs — when filter personalization first rolled out, jacuzzi’s were one of the most popular requests. That wasn’t even a consideration previously; there wasn’t even a filter. Now that filter is live. “I had no idea,” Pathak noted. “I had never searched for a hot tub in my room honestly.” When it comes to personalization, though, there is a fine line; memory remains complicated, Pathak emphasized. While it’s important to have long-term memories and evolving threads with customers — retaining information like their typical budgets, preferred hotel star ratings or whether they need disability access — it must be on their terms and protective of their privacy. Booking.com is extremely mindful with memory, seeking consent so as to not be “creepy” when collecting customer information. “Managing memory is much harder than actually building memory,” said Pathak. “The tech is out there, we have the technical chops to build it. We want to make sure we don&#x27;t launch a memory object that doesn&#x27;t respect customer consent, that doesn&#x27;t feel very natural.”Finding a balance of build versus buy As agents mature, Booking.com is navigating a central question facing the entire industry: How narrow should agents become? Instead of committing to either a swarm of highly specialized agents or a few generalized ones, the company aims for reversible decisions and avoids “one-way doors” that lock its architecture into long-term, costly paths. Pathak’s strategy is: Generalize where possible, specialize where necessary and keep agent design flexible to help ensure resiliency. Pathak and his team are “very mindful” of use cases, evaluating where to build more generalized, reusable agents or more task-specific ones. They strive to use the smallest model possible, with the highest level of accuracy and output quality, for each use case. Whatever can be generalized is. Latency is another important consideration. When factual accuracy and avoiding hallucinations is paramount, his team will use a larger, much slower model; but with search and recommendations, user expectations set speed. (Pathak noted: “No one’s patient.”) “We would, for example, never use something as heavy as GPT-5 for just topic detection or for entity extraction,” he said. Booking.com takes a similarly elastic tack when it comes to monitoring and evaluations: If it&#x27;s general-purpose monitoring that someone else is better at building and has horizontal capability, they’ll buy it. But if it’s instances where brand guidelines must be enforced, they’ll build their own evals. Ultimately, Booking.com has leaned into being “super anticipatory,” agile and flexible. “At this point with everything that&#x27;s happening with AI, we are a little bit averse to walking through one way doors,” said Pathak. “We want as many of our decisions to be reversible as possible. We don&#x27;t want to get locked into a decision that we cannot reverse two years from now.”What other builders can learn from Booking.com’s AI journeyBooking.com’s AI journey can serve as an important blueprint for other enterprises. Looking back, Pathak acknowledged that they started out with a “pretty complicated” tech stack. They’re now in a good place with that, “but we probably could have started something much simpler and seen how customers interacted with it.” Given that, he offered this valuable advice: If you’re just starting out with LLMs or agents, out-of-the-box APIs will do just fine. “There&#x27;s enough customization with APIs that you can already get a lot of leverage before you decide you want to go do more.” On the other hand, if a use case requires customization not available through a standard API call, that makes a case for in-house tools. Still, he emphasized: Don&#x27;t start with the complicated stuff. Tackle the “simplest, most painful problem you can find and the simplest, most obvious solution to that.” Identify the product market fit, then investigate the ecosystems, he advised — but don’t just rip out old infrastructures because a new use case demands something specific (like moving an entire cloud strategy from AWS to Azure just to use the OpenAI endpoint). Ultimately: “Don&#x27;t lock yourself in too early,” Pathak noted. “Don&#x27;t make decisions that are one-way doors until you are very confident that that&#x27;s the solution that you want to go with.”",
          "content": "When many enterprises weren’t even thinking about agentic behaviors or infrastructures, Booking.com had already “stumbled” into them with its homegrown conversational recommendation system. This early experimentation has allowed the company to take a step back and avoid getting swept up in the frantic AI agent hype. Instead, it is taking a disciplined, layered, modular approach to model development: small, travel-specific models for cheap, fast inference; larger large language models (LLMs) for reasoning and understanding; and domain-tuned evaluations built in-house when precision is critical. Listen and subscribe to Beyond the Pilot on Spotify, Apple or wherever you get your podcasts.With this hybrid strategy — and selective collaboration with OpenAI, Gemini and other key players in the AI space — Booking.com has seen accuracy double across key retrieval, ranking and customer-interaction tasks. As Pranav Pathak, Booking.com’s AI product development lead, posed to VentureBeat in a new podcast: “Do you build it very, very specialized and bespoke and then have an army of a hundred agents? Or do you keep it general enough and have five agents that are good at generalized tasks, but then you have to orchestrate a lot around them? That&#x27;s a balance that I think we&#x27;re still trying to figure out, as is the rest of the industry.”Moving from guessing to deep personalization without being ‘creepy’Recommendation systems are core to Booking.com’s customer-facing platforms; however, traditional recommendation tools have been less about recommendation and more about guessing, Pathak conceded. So, from the start, he and his team vowed to avoid generic tools: As he put it, the price and recommendation should be based on customer context. Booking.com’s initial pre-gen AI tooling for intent and topic detection was a small language model, what Pathak described as “the scale and size of BERT.” The model ingested the customer’s inputs around their problem to determine whether it could be solved through self-service or bumped to a human agent. “We started with an architecture of ‘you have to call a tool if this is the intent you detect and this is how you&#x27;ve parsed the structure,” Pathak explained. “That was very, very similar to the first few agentic architectures that came out in terms of reason and defining a tool call.” His team has since built out that architecture to include an LLM orchestrator that classifies queries, triggers retrieval-augmented generation (RAG) and calls APIs or smaller, specialized language models. “We&#x27;ve been able to scale that system quite well because it was so close in architecture that, with a few tweaks, we now have a full agentic stack,” said Pathak. As a result, Booking.com is seeing a notable increase in topic detection, with more topics, even complicated ones previously identified as ‘other’ and requiring escalation, are becoming automated. Ultimately, this supports more self-service, freeing human agents to focus on customers with uniquely-specific problems that the platform doesn’t have a dedicated tool flow for — say, a family that is unable to access its hotel room at 2 a.m. when the front desk is closed. That not only “really starts to compound,” but has a direct, long-term impact on customer retention, Pathak noted. “One of the things we&#x27;ve seen is, the better we are at customer service, the more loyal our customers are.” Another recent rollout is personalized filtering. Booking.com has between 200 and 250 search filters on its website — an unrealistic amount for any human to sift through, Pathak pointed out. So, his team introduced a free text box that users can type into to immediately receive tailored filters. “That becomes such an important cue for personalization in terms of what you&#x27;re looking for in your own words rather than a clickstream,” said Pathak. In turn, it cues Booking.com into what customers actually want. For instance, hot tubs — when filter personalization first rolled out, jacuzzi’s were one of the most popular requests. That wasn’t even a consideration previously; there wasn’t even a filter. Now that filter is live. “I had no idea,” Pathak noted. “I had never searched for a hot tub in my room honestly.” When it comes to personalization, though, there is a fine line; memory remains complicated, Pathak emphasized. While it’s important to have long-term memories and evolving threads with customers — retaining information like their typical budgets, preferred hotel star ratings or whether they need disability access — it must be on their terms and protective of their privacy. Booking.com is extremely mindful with memory, seeking consent so as to not be “creepy” when collecting customer information. “Managing memory is much harder than actually building memory,” said Pathak. “The tech is out there, we have the technical chops to build it. We want to make sure we don&#x27;t launch a memory object that doesn&#x27;t respect customer consent, that doesn&#x27;t feel very natural.”Finding a balance of build versus buy As agents mature, Booking.com is navigating a central question facing the entire industry: How narrow should agents become? Instead of committing to either a swarm of highly specialized agents or a few generalized ones, the company aims for reversible decisions and avoids “one-way doors” that lock its architecture into long-term, costly paths. Pathak’s strategy is: Generalize where possible, specialize where necessary and keep agent design flexible to help ensure resiliency. Pathak and his team are “very mindful” of use cases, evaluating where to build more generalized, reusable agents or more task-specific ones. They strive to use the smallest model possible, with the highest level of accuracy and output quality, for each use case. Whatever can be generalized is. Latency is another important consideration. When factual accuracy and avoiding hallucinations is paramount, his team will use a larger, much slower model; but with search and recommendations, user expectations set speed. (Pathak noted: “No one’s patient.”) “We would, for example, never use something as heavy as GPT-5 for just topic detection or for entity extraction,” he said. Booking.com takes a similarly elastic tack when it comes to monitoring and evaluations: If it&#x27;s general-purpose monitoring that someone else is better at building and has horizontal capability, they’ll buy it. But if it’s instances where brand guidelines must be enforced, they’ll build their own evals. Ultimately, Booking.com has leaned into being “super anticipatory,” agile and flexible. “At this point with everything that&#x27;s happening with AI, we are a little bit averse to walking through one way doors,” said Pathak. “We want as many of our decisions to be reversible as possible. We don&#x27;t want to get locked into a decision that we cannot reverse two years from now.”What other builders can learn from Booking.com’s AI journeyBooking.com’s AI journey can serve as an important blueprint for other enterprises. Looking back, Pathak acknowledged that they started out with a “pretty complicated” tech stack. They’re now in a good place with that, “but we probably could have started something much simpler and seen how customers interacted with it.” Given that, he offered this valuable advice: If you’re just starting out with LLMs or agents, out-of-the-box APIs will do just fine. “There&#x27;s enough customization with APIs that you can already get a lot of leverage before you decide you want to go do more.” On the other hand, if a use case requires customization not available through a standard API call, that makes a case for in-house tools. Still, he emphasized: Don&#x27;t start with the complicated stuff. Tackle the “simplest, most painful problem you can find and the simplest, most obvious solution to that.” Identify the product market fit, then investigate the ecosystems, he advised — but don’t just rip out old infrastructures because a new use case demands something specific (like moving an entire cloud strategy from AWS to Azure just to use the OpenAI endpoint). Ultimately: “Don&#x27;t lock yourself in too early,” Pathak noted. “Don&#x27;t make decisions that are one-way doors until you are very confident that that&#x27;s the solution that you want to go with.”",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2t04hE5kMFifED9FQ12PcA/86ec4e3895472b25edb77273488d1c7a/Booking.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/design-in-the-age-of-ai-how-small-businesses-are-building-big-brands-faster",
          "published_at": "Mon, 08 Dec 2025 08:00:00 GMT",
          "title": "Design in the age of AI: How small businesses are building big brands faster",
          "standfirst": "Presented by Design.comFor most of history, design was the last step in starting a business — something entrepreneurs invested in once the idea was proven. Today, it’s one of the first. The rise of generative AI has shifted how small businesses imagine, launch, and grow — turning what used to be a months-long creative process into something interactive, iterative, and accessible from day one.Search data tells the story. Since 2022, global interest in “AI business name generator” has surged more than 700%. Searches for “AI logo generator” are up 1,200%, and “AI website generator” 1,600%. Small businesses aren’t waiting for enterprise AI trickle-down. They’re adopting these tools en masse to move faster from concept to brand identity.“The appetite for AI-powered design has been extraordinary,” says Alec Lynch, founder and CEO of Design.com. “Entrepreneurs are realizing they can bring their ideas to life immediately — they don’t have to wait for funding, agencies, or a full creative team. They can start now.”The democratization of design powerFor decades, small businesses were boxed out of high-end design. Building a brand required deep pockets and specialized talent. AI has redrawn that map.Large language models and image generators now act as collaborative partners — sparking ideas, testing directions, and handling tedious layout and copy work. For founders, that means fewer barriers and faster iteration.Instead of hiring separate agencies for naming, logo design, and web development, small businesses are turning to unified AI platforms that handle the full early-stage design stack. Tools like Design.com merge naming, logo creation, and website generation into a single workflow — turning an entrepreneur’s first sketch into a polished brand system within minutes.“AI isn’t replacing creativity,” Lynch adds. “It’s giving people the confidence to express it.”The five frontiers of AI-powered entrepreneurshipToday’s AI tools mirror the creative journey every founder takes — from naming a business to sharing it with the world. The five fastest-growing design categories on Google reflect each stage of that journey.1. Naming: From idea to identityAI naming tools do more than spit out clever words — they help founders discover their voice. A good generator blends tone, personality, and domain availability so the result feels like a fit, not a random suggestion.2. Logos: From visuals to meaningLogo creation is one of the most emotionally resonant steps in brand-building. AI has turned it into a playground for experimentation. Entrepreneurs can test dozens of looks and get instant feedback.3. Websites: From static pages to adaptive brandsThe surge in “AI website generator” searches signals a deeper shift. Websites are no longer static brochures; they’re dynamic brand environments. AI-driven builders now create layouts, headlines, and imagery that adapt to a company’s tone and focus — drastically reducing time to launch.4. Business cards and brand collateralEven in a digital age, tangible touchpoints matter. AI-generated business cards give founders an immediate sense of legitimacy while ensuring design consistency across brand assets.5. Presentations: From slides to storytellingFounders aren’t just designing assets; they’re designing narratives. Generative AI turns bullet points into persuasive visual stories — raising the quality of pitches, decks, and demos once out of reach for most small teams.Together, these five frontiers show that small businesses aren’t just using AI to look more polished — they’re using it to think more strategically about brand, story, and customer experience from the start.The new design ecosystemBehind the surge in AI design tools lies a broader ecosystem shift. Companies like Canva and Wix made design accessible; the current wave — led by AI-native platforms like Design.com — is more personal and adaptive.Unlike templated platforms, these tools understand context. A restaurant founder and a SaaS startup will get not just different visuals, but different copy tones, typography systems, and user flows — automatically.“What we’re seeing,” Lynch explains, “isn’t just growth in one product category. It’s a movement toward connected creativity — where every part of the brand experience learns from every other.”From AI tools to AI brand systemsThe next evolution of small-business design won’t be about single-purpose tools. It will be about connected systems that share data, context, and creative intent across every brand touchpoint.Imagine naming a company and watching an AI instantly generate a logo, color palette, and homepage layout that all reflect the same personality. As your audience grows, the same system helps you update your visual identity or tone to match new goals — while preserving your original DNA.That’s the future Design.com and others are building toward: intelligent brand ecosystems that evolve alongside their founders.“AI design tools are giving small businesses superpowers,” Lynch says. “They’re removing friction from creativity.”And that frictionless design process is quietly rewriting what entrepreneurship looks like. The ability to create, iterate, and launch in hours instead of months is changing the tempo of business itself — and redefining what it means to be a designer in the age of AI.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by Design.comFor most of history, design was the last step in starting a business — something entrepreneurs invested in once the idea was proven. Today, it’s one of the first. The rise of generative AI has shifted how small businesses imagine, launch, and grow — turning what used to be a months-long creative process into something interactive, iterative, and accessible from day one.Search data tells the story. Since 2022, global interest in “AI business name generator” has surged more than 700%. Searches for “AI logo generator” are up 1,200%, and “AI website generator” 1,600%. Small businesses aren’t waiting for enterprise AI trickle-down. They’re adopting these tools en masse to move faster from concept to brand identity.“The appetite for AI-powered design has been extraordinary,” says Alec Lynch, founder and CEO of Design.com. “Entrepreneurs are realizing they can bring their ideas to life immediately — they don’t have to wait for funding, agencies, or a full creative team. They can start now.”The democratization of design powerFor decades, small businesses were boxed out of high-end design. Building a brand required deep pockets and specialized talent. AI has redrawn that map.Large language models and image generators now act as collaborative partners — sparking ideas, testing directions, and handling tedious layout and copy work. For founders, that means fewer barriers and faster iteration.Instead of hiring separate agencies for naming, logo design, and web development, small businesses are turning to unified AI platforms that handle the full early-stage design stack. Tools like Design.com merge naming, logo creation, and website generation into a single workflow — turning an entrepreneur’s first sketch into a polished brand system within minutes.“AI isn’t replacing creativity,” Lynch adds. “It’s giving people the confidence to express it.”The five frontiers of AI-powered entrepreneurshipToday’s AI tools mirror the creative journey every founder takes — from naming a business to sharing it with the world. The five fastest-growing design categories on Google reflect each stage of that journey.1. Naming: From idea to identityAI naming tools do more than spit out clever words — they help founders discover their voice. A good generator blends tone, personality, and domain availability so the result feels like a fit, not a random suggestion.2. Logos: From visuals to meaningLogo creation is one of the most emotionally resonant steps in brand-building. AI has turned it into a playground for experimentation. Entrepreneurs can test dozens of looks and get instant feedback.3. Websites: From static pages to adaptive brandsThe surge in “AI website generator” searches signals a deeper shift. Websites are no longer static brochures; they’re dynamic brand environments. AI-driven builders now create layouts, headlines, and imagery that adapt to a company’s tone and focus — drastically reducing time to launch.4. Business cards and brand collateralEven in a digital age, tangible touchpoints matter. AI-generated business cards give founders an immediate sense of legitimacy while ensuring design consistency across brand assets.5. Presentations: From slides to storytellingFounders aren’t just designing assets; they’re designing narratives. Generative AI turns bullet points into persuasive visual stories — raising the quality of pitches, decks, and demos once out of reach for most small teams.Together, these five frontiers show that small businesses aren’t just using AI to look more polished — they’re using it to think more strategically about brand, story, and customer experience from the start.The new design ecosystemBehind the surge in AI design tools lies a broader ecosystem shift. Companies like Canva and Wix made design accessible; the current wave — led by AI-native platforms like Design.com — is more personal and adaptive.Unlike templated platforms, these tools understand context. A restaurant founder and a SaaS startup will get not just different visuals, but different copy tones, typography systems, and user flows — automatically.“What we’re seeing,” Lynch explains, “isn’t just growth in one product category. It’s a movement toward connected creativity — where every part of the brand experience learns from every other.”From AI tools to AI brand systemsThe next evolution of small-business design won’t be about single-purpose tools. It will be about connected systems that share data, context, and creative intent across every brand touchpoint.Imagine naming a company and watching an AI instantly generate a logo, color palette, and homepage layout that all reflect the same personality. As your audience grows, the same system helps you update your visual identity or tone to match new goals — while preserving your original DNA.That’s the future Design.com and others are building toward: intelligent brand ecosystems that evolve alongside their founders.“AI design tools are giving small businesses superpowers,” Lynch says. “They’re removing friction from creativity.”And that frictionless design process is quietly rewriting what entrepreneurship looks like. The ability to create, iterate, and launch in hours instead of months is changing the tempo of business itself — and redefining what it means to be a designer in the age of AI.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5vOHZEQlNQsMRQBrdSXMLg/f701adc6486654954fb4b6b456792ccc/AdobeStock_1562823709.jpeg?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/11CUQal9q3dPlFL3fRIjP3/a601024e0be680f9645daaf6198bf0f4/OfficeQA-image-smk.jpg?w=300&q=30",
      "popularity_score": 2016.8127533333334
    },
    {
      "id": "cluster_49",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 10:55:03 -0500",
      "title": "Mistral launches Devstral 2, an AI coding model with 123B parameters requiring at least four H100 GPUs, and Devstral Small, a 24B-parameter model for local use (Anna Heim/TechCrunch)",
      "neutral_headline": "Mistral AI surfs vibe-coding tailwinds with new coding models",
      "bullet_summary": [
        "Reported by TechMeme, TechCrunch"
      ],
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p24#a251209p24",
          "published_at": "Tue, 09 Dec 2025 10:55:03 -0500",
          "title": "Mistral launches Devstral 2, an AI coding model with 123B parameters requiring at least four H100 GPUs, and Devstral Small, a 24B-parameter model for local use (Anna Heim/TechCrunch)",
          "standfirst": "Anna Heim / TechCrunch: Mistral launches Devstral 2, an AI coding model with 123B parameters requiring at least four H100 GPUs, and Devstral Small, a 24B-parameter model for local use &mdash; French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding &hellip;",
          "content": "Anna Heim / TechCrunch: Mistral launches Devstral 2, an AI coding model with 123B parameters requiring at least four H100 GPUs, and Devstral Small, a 24B-parameter model for local use &mdash; French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding &hellip;",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/251209/i24.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/09/mistral-ai-surfs-vibe-coding-tailwinds-with-new-coding-models/",
          "published_at": "Tue, 09 Dec 2025 14:45:00 +0000",
          "title": "Mistral AI surfs vibe-coding tailwinds with new coding models",
          "standfirst": "French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding, as the company seeks to catch up to bigger AI labs like Anthropic and other coding-focused LLMs.",
          "content": "French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding, as the company seeks to catch up to bigger AI labs like Anthropic and other coding-focused LLMs.",
          "feed_position": 17
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i24.jpg",
      "popularity_score": 2016.7302533333334
    },
    {
      "id": "cluster_19",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 17:47:01 +0000",
      "title": "Court: “Because Trump said to” may not be a legally valid defense",
      "neutral_headline": "Court: “Because Trump said to” may not be a legally valid defense",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/trumps-order-blocking-wind-development-thrown-out-by-court/",
          "published_at": "Tue, 09 Dec 2025 17:47:01 +0000",
          "title": "Court: “Because Trump said to” may not be a legally valid defense",
          "standfirst": "The \"arbitrary and capricious\" standard strikes down another administration action.",
          "content": "On Monday, US District Court Judge Patti Saris vacated a Trump executive order that brought a halt to all offshore wind power development, as well as some projects on land. That order had called for the suspension of all permitting for wind power on federal land and waters pending a review of current practices. This led states and an organization representing wind power companies to sue, claiming among other things that the suspension was arbitrary and capricious. Over 10 months since the relevant government agencies were ordered to start a re-evaluation of the permitting process, testimony revealed that they had barely begun to develop the concept of a review. As such, the only reason they could offer in defense of the suspension consisted of Trump’s executive order and a Department of the Interior memo implementing it. “Whatever level of explanation is required when deviating from longstanding agency practice,” Judge Saris wrote, “this is not it.” Lifting Trump’s suspension does not require the immediate approval of any wind projects. Instead, the relevant agencies are likely to continue following Trump’s wishes and slow-walking any leasing and licensing processes, which may force states and project owners to sue individually. But it does provide a legal backdrop for any suits that ultimately occur, one in which the government’s actions have little justification beyond Trump’s personal animosity toward wind power.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235990625-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235990625-1024x648.jpg",
      "popularity_score": 366.5963644444444
    },
    {
      "id": "cluster_30",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 17:00:28 +0000",
      "title": "Google is reviving wearable gesture controls, but only for the Pixel Watch 4",
      "neutral_headline": "Google is reviving wearable gesture controls, but only for the Pixel Watch 4",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/google-is-reviving-wearable-gesture-controls-but-only-for-the-pixel-watch-4/",
          "published_at": "Tue, 09 Dec 2025 17:00:28 +0000",
          "title": "Google is reviving wearable gesture controls, but only for the Pixel Watch 4",
          "standfirst": "Google will let you select and dismiss with a gesture, but only on the newest watch.",
          "content": "Long ago, Google’s Android-powered wearables had hands-free navigation gestures. Those fell by the wayside as Google shredded its wearable strategy over and over, but gestures are back, baby. The Pixel Watch 4 is getting an update that adds several gestures, one of which is straight out of the Apple playbook. When the update hits devices, the Pixel Watch 4 will gain a double pinch gesture like the Apple Watch has. By tapping your thumb and forefinger together, you can answer or end calls, pause timers, and more. The watch will also prompt you at times when you can use the tap gesture to control things. In previous incarnations of Google-powered watches, a quick wrist turn gesture would scroll through lists. In the new gesture system, that motion dismisses what’s on the screen. For example, you can clear a notification from the screen or dismiss an incoming call. Pixel Watch 4 owners will also enjoy this one when the update arrives.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Pixel-watch-4-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Pixel-watch-4-1-1152x648.jpg",
      "popularity_score": 358.8205311111111
    },
    {
      "id": "cluster_4",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 18:53:21 +0000",
      "title": "Supreme Court appears likely to approve Trump’s firing of FTC Democrat",
      "neutral_headline": "Supreme Court appears likely to approve Trump’s firing of FTC Democrat",
      "bullet_summary": [
        "Reported by Ars Technica Main"
      ],
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/tech-policy/2025/12/supreme-court-appears-likely-to-approve-trumps-firing-of-ftc-democrat/",
          "published_at": "Tue, 09 Dec 2025 18:53:21 +0000",
          "title": "Supreme Court appears likely to approve Trump’s firing of FTC Democrat",
          "standfirst": "Conservative justices seem ready to back Trump control of independent agencies.",
          "content": "The Supreme Court’s conservative justices appear ready to overturn a 90-year-old precedent that said the president cannot fire a Federal Trade Commission member without cause. A ruling for Trump would give him more power over the FTC and potentially other independent agencies such as the Federal Communications Commission. Former FTC Commissioner Rebecca Kelly Slaughter, a Democrat, sued Trump after he fired both Democrats from the commission in March. Slaughter’s case rests largely on the 1935 ruling in Humphrey’s Executor v. United States, in which the Supreme Court unanimously held that the president can only remove FTC commissioners for inefficiency, neglect of duty, or malfeasance in office. Chief Justice John Roberts said during yesterday’s oral arguments that Humphrey’s Executor is a “dried husk” despite being the “primary authority” that Slaughter’s legal team is relying on. Roberts said the court’s 2020 ruling in Seila Law made it “pretty clear… that Humphrey’s Executor is just a dried husk of whatever people used to think it was because, in the opinion itself, it described the powers of the agency it was talking about, and they’re vanishingly insignificant, have nothing to do with what the FTC looks like today.”Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg",
      "popularity_score": 352.70192
    },
    {
      "id": "cluster_14",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 18:09:05 +0000",
      "title": "NASA astronauts will have their own droid when they go back to the Moon",
      "neutral_headline": "NASA astronauts will have their own droid when they go back to the Moon",
      "bullet_summary": [
        "Reported by Ars Technica Main"
      ],
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/space/2025/12/lunar-outpost-rover-to-study-lunar-dust-alongside-artemis-astronauts-on-moon/",
          "published_at": "Tue, 09 Dec 2025 18:09:05 +0000",
          "title": "NASA astronauts will have their own droid when they go back to the Moon",
          "standfirst": "NASA crew will be the first astronauts to work with a robot on a celestial body other than Earth.",
          "content": "B-9 had Will Robinson. Twiki had Buck Rogers. And, of course, C-3PO and R2-D2 had Luke Skywalker. Now, in a scenario straight out of science fiction, MAPP will have whoever NASA names to the crew of the second Artemis mission to land on the moon. The space agency has selected Lunar Outpost’s Mobile Autonomous Prospecting Platform, or MAPP, to become the first robotic rover to operate on the moon alongside astronauts. Although its tasks will be far simpler than those of the robots seen on TV and in the movies, the autonomous four-wheeled MAPP will help scientists learn more about the crew’s surroundings. Science instruments on the rover will characterize the surface plasma and behavior of the dust in the lunar environment. “The Apollo era taught us that the further humanity is from Earth, the more dependent we are on science to protect and sustain human life on other planets,” said Nicky Fox, NASA’s associate administrator for science, in a statement. “By deploying these… science instruments on the lunar surface, our proving ground, NASA is leading the world in the creation of humanity’s interplanetary survival guide to ensure the health and safety of our spacecraft and human explorers as we begin our epic journey back to the Moon.”Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/news-120825a-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/news-120825a-lg-1152x648.jpg",
      "popularity_score": 341.9641422222222
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 16:10:45 +0000",
      "title": "Brazil weakens Amazon protections days after COP30",
      "neutral_headline": "Brazil weakens Amazon protections days after COP30",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/days-after-cop30-brazil-weakened-amazon-safeguards/",
          "published_at": "Tue, 09 Dec 2025 16:10:45 +0000",
          "title": "Brazil weakens Amazon protections days after COP30",
          "standfirst": "Backed by powerful corporations, nations are giving public false choices: Environmental protection or economic growth.",
          "content": "Despite claims of environmental leadership and promises to preserve the Amazon rainforest ahead of COP30, Brazil is stripping away protections for the region’s vital ecosystems faster than workers dismantled the tents that housed the recent global climate summit in Belém. On Nov. 27, less than a week after COP30 ended, a powerful political bloc in Brazil’s National Congress, representing agribusiness, and development interests, weakened safeguards for the Amazon’s rivers, forests, and Indigenous communities. The rollback centered on provisions in an environmental licensing bill passed by the government a few months before COP30. The law began to take shape well before, during the Jair Bolsonaro presidency from 2019 to 2023. It reflected the deregulatory agenda of the rural caucus, the Frente Parlamentar da Agropecuária, which wielded significant power during his term and remains influential today.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1249347312-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1249347312-1152x648.jpg",
      "popularity_score": 326.99192
    },
    {
      "id": "cluster_46",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 16:00:29 +0000",
      "title": "Pompeii construction site confirms recipe for Roman concrete",
      "neutral_headline": "Pompeii construction site confirms recipe for Roman concrete",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/study-confirms-romans-used-hot-mixing-to-make-concrete/",
          "published_at": "Tue, 09 Dec 2025 16:00:29 +0000",
          "title": "Pompeii construction site confirms recipe for Roman concrete",
          "standfirst": "Latest results from a recently discovered ancient Roman construction site confirm earlier findings.",
          "content": "Back in 2023, we reported on MIT scientists’ conclusion that the ancient Romans employed “hot mixing” with quicklime, among other strategies, to make their famous concrete, giving the material self-healing functionality. The only snag was that this didn’t match the recipe as described in historical texts. Now the same team is back with a fresh analysis of samples collected from a recently discovered site that confirms the Romans did indeed use hot mixing, according to a new paper published in the journal Nature Communications. As we’ve reported previously, like today’s Portland cement (a basic ingredient of modern concrete), ancient Roman concrete was basically a mix of a semi-liquid mortar and aggregate. Portland cement is typically made by heating limestone and clay (as well as sandstone, ash, chalk, and iron) in a kiln. The resulting clinker is then ground into a fine powder with just a touch of added gypsum to achieve a smooth, flat surface. But the aggregate used to make Roman concrete was made up of fist-sized pieces of stone or bricks. In his treatise De architectura (circa 30 CE), the Roman architect and engineer Vitruvius wrote about how to build concrete walls for funerary structures that could endure for a long time without falling into ruin. He recommended the walls be at least two feet thick, made of either “squared red stone or of brick or lava laid in courses.” The brick or volcanic rock aggregate should be bound with mortar composed of hydrated lime and porous fragments of glass and crystals from volcanic eruptions (known as volcanic tephra).Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/concrete1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/concrete1-1152x648.jpg",
      "popularity_score": 319.8208088888889
    },
    {
      "id": "cluster_47",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 16:00:25 +0000",
      "title": "In a major new report, scientists build rationale for sending astronauts to Mars",
      "neutral_headline": "In a major new report, scientists build rationale for sending astronauts to Mars",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/in-a-major-new-report-scientists-build-rationale-for-sending-astronauts-to-mars/",
          "published_at": "Tue, 09 Dec 2025 16:00:25 +0000",
          "title": "In a major new report, scientists build rationale for sending astronauts to Mars",
          "standfirst": "\"Everyone is inspired by this because it's becoming real.\"",
          "content": "Sending astronauts to the red planet will be a decades-long activity and cost many billions of dollars. So why should NASA undertake such a bold mission? A new report published Tuesday, titled “A Science Strategy for the Human Exploration of Mars,” represents the answer from leading scientists and engineers in the United States: finding whether life exists, or once did, beyond Earth. “We’re searching for life on Mars,” said Dava Newman, a professor in the Department of Aeronautics and Astronautics at Massachusetts Institute of Technology and co-chair of the committee that wrote the report, in an interview with Ars. “The answer to the question ‘are we alone‘ is always going to be ‘maybe,’ unless it becomes yes.”Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/file-20250327-56-dflaq1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/file-20250327-56-dflaq1-1152x648.jpg",
      "popularity_score": 317.8196977777778
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 15:35:39 +0000",
      "title": "Asked why we need Golden Dome, the man in charge points to a Hollywood film",
      "neutral_headline": "Asked why we need Golden Dome, the man in charge points to a Hollywood film",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/asked-why-we-need-golden-dome-the-man-in-charge-points-to-a-hollywood-film/",
          "published_at": "Tue, 09 Dec 2025 15:35:39 +0000",
          "title": "Asked why we need Golden Dome, the man in charge points to a Hollywood film",
          "standfirst": "\"If they see how prepared we are, no one starts a nuclear war.\"",
          "content": "Near the end of the film A House of Dynamite, a fictional American president portrayed by Idris Elba sums up the theory of nuclear deterrence. “Just being ready is the point, right?” Elba says. “It keeps people in check. Keeps the world straight. If they see how prepared we are, no one starts a nuclear war.” There’s a lot that goes wrong in the film, namely the collapse of deterrence itself. For more than 60 years, the US military has used its vast arsenal of nuclear weapons, constantly deployed on Navy submarines, at Air Force bomber bases, and in Minuteman missile fields, as a way of saying, “Don’t mess with us.” In the event of a first strike against the United States, an adversary would be assured of an overwhelming nuclear response, giving rise to the concept of mutual assured destruction.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/5208709-1152x648-1765274456.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/5208709-1152x648-1765274456.jpg",
      "popularity_score": 299.40692
    },
    {
      "id": "cluster_57",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 15:00:56 +0000",
      "title": "Pebble maker announces Index 01, a smart-ish ring for under $100",
      "neutral_headline": "Pebble maker announces Index 01, a smart-ish ring for under $100",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/",
          "published_at": "Tue, 09 Dec 2025 15:00:56 +0000",
          "title": "Pebble maker announces Index 01, a smart-ish ring for under $100",
          "standfirst": "The Pebble Index 01 isn't quite a smart ring, but it can do some smart things.",
          "content": "Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well. Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99. Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg",
      "popularity_score": 288.8283088888889
    },
    {
      "id": "cluster_92",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 21:54:58 +0000",
      "title": "ICEBlock lawsuit: Trump admin bragged about demanding App Store removal",
      "neutral_headline": "ICEBlock lawsuit: Trump admin bragged about demanding App Store removal",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/iceblock-lawsuit-trump-admin-bragged-about-demanding-app-store-removal/",
          "published_at": "Mon, 08 Dec 2025 21:54:58 +0000",
          "title": "ICEBlock lawsuit: Trump admin bragged about demanding App Store removal",
          "standfirst": "ICEBlock creator sues to protect apps that are crowd-sourcing ICE sightings.",
          "content": "In a lawsuit filed against top Trump administration officials on Monday, Apple was accused of caving to unconstitutional government demands by removing an Immigration and Customs Enforcement-spotting app from the App Store with more than a million users. In his complaint, Joshua Aaron, creator of ICEBlock, cited a Fox News interview in which Attorney General Pam Bondi “made plain that the United States government used its regulatory power to coerce a private platform to suppress First Amendment-protected expression.” Suing Bondi—along with Department of Homeland Security Secretary Kristi Noem, Acting Director of ICE Todd Lyons, White House “Border Czar” Thomas D. Homan, and unnamed others—Aaron further alleged that US officials made false statements and “unlawful threats” to criminally investigate and prosecute him for developing ICEBlock.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235825531-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235825531-1024x648.jpg",
      "popularity_score": 263
    },
    {
      "id": "cluster_96",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 18:36:16 +0000",
      "title": "Paramount tries to swipe Warner Bros. from Netflix with a hostile takeover",
      "neutral_headline": "Paramount tries to swipe Warner Bros. from Netflix with a hostile takeover",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/paramount-says-it-could-get-antitrust-approval-for-wbd-before-netflix/",
          "published_at": "Mon, 08 Dec 2025 18:36:16 +0000",
          "title": "Paramount tries to swipe Warner Bros. from Netflix with a hostile takeover",
          "standfirst": "Paramount has already proven it can get a controversial merger done.",
          "content": "Netflix won the bidding war for Warner Bros. Discovery’s (WBD’s) streaming and movie studio businesses last week. But Paramount Skydance isn’t relenting on its dreams of owning WBD and is pushing forward with a hostile takeover bid. On Friday, Netflix announced that it had agreed to pay an equity value of $72 billion, or an approximate total enterprise value of $82.7 billion, for WBD’s streaming and film businesses, as well as its film and TV libraries. The deal includes HBO and the HBO Max streaming service but not WBD’s cable channels, which are to be split off ahead of the acquisition into a separate company called Discovery Global. Netflix said WBD’s split should conclude in Q3 2026. Paramount has different plans, though.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250240969-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250240969-1024x648.jpg",
      "popularity_score": 253
    },
    {
      "id": "cluster_97",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 17:01:37 +0000",
      "title": "F1 in Abu Dhabi: And that’s the championship",
      "neutral_headline": "F1 in Abu Dhabi: And that’s the championship",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/f1-in-abu-dhabi-and-thats-the-championship/",
          "published_at": "Mon, 08 Dec 2025 17:01:37 +0000",
          "title": "F1 in Abu Dhabi: And that’s the championship",
          "standfirst": "A three-way fight down to the wire as the ground effect era comes to a close.",
          "content": "The 2025 Formula 1 World Championship drew to a close this past weekend in Abu Dhabi, and with it came the end of the current generation of cars. After a grueling 24 races, the title was decided in a three-way fight by the finest of margins; just two points, less than half a percent, separated the winning driver from second place when the checkered flag waved on Sunday. Coming into Abu Dhabi, McLaren’s Lando Norris was, if not a comfortable favorite, then at least the driver with the highest odds of prevailing. After a strong start to the season, the British driver’s form dipped at the Dutch Grand Prix. But he bounced back, retaking the championship lead from his Australian teammate Oscar Piastri in Mexico in October. For much of the season, it seemed to be a two-car race. McLaren had a clear car advantage and two strong drivers, suggesting a repeat of the years we saw Lewis Hamilton and Nico Rosberg duking it out to bring home titles for Mercedes. But that didn’t figure on Red Bull developing its car late in the season. New boss Laurent Mekies has revitalized the energy drinks squad, and four-time champion Max Verstappen was able to close inexorably toward the McLaren drivers in the points with a string of sublime performances.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250341495-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250341495-1152x648.jpg",
      "popularity_score": 240
    },
    {
      "id": "cluster_100",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 14:57:11 +0000",
      "title": "Meta offers EU users ad-light option in push to end investigation",
      "neutral_headline": "Meta offers EU users ad-light option in push to end investigation",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/12/meta-offers-eu-users-ad-light-option-in-push-to-end-investigation/",
          "published_at": "Mon, 08 Dec 2025 14:57:11 +0000",
          "title": "Meta offers EU users ad-light option in push to end investigation",
          "standfirst": "Facebook agrees to change \"pay or consent\" model after talks with European Commission.",
          "content": "Meta has agreed to make changes to its “pay or consent” business model in the EU, seeking to agree to a deal that avoids further regulatory fines at a time when the bloc’s digital rule book is drawing anger from US authorities. On Tuesday, the European Commission announced that the social media giant had offered users an alternative choice of Facebook and Instagram services that would show them fewer personalized advertisements. The offer follows an EU investigation into Meta’s policy of requiring users either to consent to data tracking or pay for an ad-free service. The Financial Times reported on optimism that an agreement could be reached between the parties in October.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1359152239-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1359152239-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_104",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 12:00:14 +0000",
      "title": "Please send help. I can’t stop playing these roguelikes.",
      "neutral_headline": "Please send help. I can’t stop playing these roguelikes.",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/12/please-send-help-i-cant-stop-playing-these-roguelikes/",
          "published_at": "Mon, 08 Dec 2025 12:00:14 +0000",
          "title": "Please send help. I can’t stop playing these roguelikes.",
          "standfirst": "2025 was a very good year for my favorite genre.",
          "content": "It’s time to admit, before God and the good readers of Ars Technica, that I have a problem. I love roguelikes. Reader, I can’t get enough of them. If there’s even a whisper of a hot new roguelike on Steam, I’m there. You may call them arcane, repetitive, or maddeningly difficult; I call them heaven. The second best part of video games is taking a puny little character and, over 100 hours, transforming that adventurer into a god of destruction. The best thing about video games is doing the same thing in under an hour. Beat a combat encounter, get an upgrade. Enter a new area, choose a new item. Put together a build and watch it sing. If you die—immediately ending your ascent and returning you to the beginning of the game—you’ll often make a pit stop at a home base to unlock new goodies to help you on your next run. (Some people distiguish between roguelikes and “roguelites,” with the latter including permanent, between-run upgrades. For simplicity’s sake, I’ll use “roguelike” as an umbrella term).Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/hades2_dec22_01-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/hades2_dec22_01-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_99",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 15:49:32 +0000",
      "title": "A big bike on a budget: Lectric’s XPress 750",
      "neutral_headline": "A big bike on a budget: Lectric’s XPress 750",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/lectric-xpress-750-a-full-sized-bike-for-the-budget-minded/",
          "published_at": "Mon, 08 Dec 2025 15:49:32 +0000",
          "title": "A big bike on a budget: Lectric’s XPress 750",
          "standfirst": "A budget e-bike that offers more than you might expect.",
          "content": "Almost every bit of bike testing I’ve done starts out the same way. After assembling the bike, I set the seatpost to its maximum recommended height, take it on a short test ride, and try to figure out new and creative phrasing to describe the same old problem: The frame isn’t quite big enough to accommodate my legs. While I’m on the tall side at a bit over 6 feet (~190 cm), I’m definitely not abnormally large. Yet very few e-bike manufacturers seem to be interested in giving people my height a comfortable ride. So imagine my surprise when, within two blocks of my first ride on the XPress 750, I had to pull off to the side of the street and lower the seat. This was especially notable given that the XPress is a budget bike (currently on sale for just under $1,000.00) that is only offered in a single frame size. So kudos to Lectric for giving me a comfortable and enjoyable ride, and doing so with a lot of features I wouldn’t expect at this price point. That said, hitting that price necessitated some significant compromises. We’ll discuss those in detail so you can get a sense of whether any of them will get in the way of your riding enjoyment.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/IMG_1802-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/IMG_1802-1152x648.jpeg",
      "popularity_score": 130
    },
    {
      "id": "cluster_101",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 14:03:30 +0000",
      "title": "The Boys gears up for a supe-ocalypse in S5 teaser",
      "neutral_headline": "The Boys gears up for a supe-ocalypse in S5 teaser",
      "bullet_summary": [
        "Reported by Ars Technica"
      ],
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/12/the-boys-gears-up-for-a-supe-ocalypse-in-s5-teaser/",
          "published_at": "Mon, 08 Dec 2025 14:03:30 +0000",
          "title": "The Boys gears up for a supe-ocalypse in S5 teaser",
          "standfirst": "\"So how about it, you lot? One last go?\"",
          "content": "Prime Video dropped an extended teaser for the fifth and final season of The Boys—based on the comic book series of the same name by Garth Ennis and Darick Robertson—during CCXP in Sao Paulo, Brazil. And it looks like we’re getting nothing less than a full-on Supe-ocalypse as an all-powerful Homelander seeks revenge on The Boys. (Spoilers for prior seasons of The Boys and S2 of Gen V below.) Things were not looking good for our antiheroes after the S4 finale. They managed to thwart the assassination of newly elected US President Robert Singer, but new Vought CEO/evil supe Sister Sage (Susan Heyward) essentially overthrew the election and installed Senator Steve Calhoun (David Andrews) as president. Calhoun declared martial law, and naturally, Homelander (Antony “Give Him an Emmy Already” Starr) swore loyalty as his chief enforcer. Butcher (Karl Urban) and Annie (Erin Moriarty) escaped, but the rest of The Boys were rounded up and placed in re-education—er, “Freedom”—camps.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/boys2-1152x648-1765118776.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/boys2-1152x648-1765118776.jpg",
      "popularity_score": 130
    }
  ]
}